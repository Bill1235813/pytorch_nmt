Opiates flow freely across borders into Iran, Pakistan, and other Central Asian countries.
The opium fields of wealthy landowners are untouched, because local officials are paid off.
Major traffickers never come to trial because judges are bribed or intimidated.
Senior government officials take their cut of opium revenues or bribes in return for keeping quiet.
Perversely, some provincial governors and government officials are themselves major players in the drug trade.
As a result, the Afghan state is at risk of takeover by a malign coalition of extremists, criminals, and opportunists.
Opium is choking Afghan society.
Within Afghanistan, drug addiction is rising.
Neighbors that used to be transit states for drugs are now major consumers, owing to similar dramatic increases in opium and heroin addiction.
Intravenous drug use is spreading HIV/AIDS in Iran, Central Asia, and the former Soviet Union.
In traditional Western European markets, health officials should brace for a rise in the number of deaths from drug overdoses, as this year’s bumper opium crop will lead to higher-purity doses of heroin.
What can be done?
First, the veil of corruption in Afghanistan must be lifted.
Afghans are fed up with arrogant and well-armed tycoons who live in mansions and drive top-of-the range Mercedes limousines – this in a country where barely 13% of the population have electricity and most people must survive on less than $200 a year.
It is time for the Afghan government to name, shame, and sack corrupt officials, arrest major drug traffickers and opium landlords, and seize their assets.
Donors have trained police and prosecutors and built courts and detention centers.
Now it is up to the government to use the judicial system to impose the rule of law.
It will be difficult, but not impossible, to re-establish confidence in the central government.
Putting major drug traffickers behind bars at the new maximum-security prison at Pul-i-Charki, near Kabul, would be a good start.
Of course, Afghanistan does not bear sole responsibility for its plight.
The heroin trade would not be booming if Western governments were serious about combating drug consumption.
It is a bitter irony that the countries whose soldiers’ lives are on the line in Afghanistan are also the biggest markets for Afghan heroin.
Furthermore, Afghanistan’s neighbors must do more to stop insurgents, weapons, money, and chemical precursors from flowing across their borders into the country.
Coalition forces should take a more robust approach to the drug problem.
Counter-insurgency and counter-narcotics are two sides of the same coin.
Improving security and the rule of law must include destroying the opium trade.
Allowing opium traffickers to operate with impunity gives them a free hand to raise money to pay for the arms and fighters battling the Afghan army and NATO forces.
The United Nations Security Council has authorized the International Security Assistance Force to take all necessary measures to fulfill its mandate.
NATO troops should be given the green light to help the Afghan army fight opium – destroy the heroin labs, disband the opium bazaars, attack the opium convoys, and bring the big traders to justice.
And they should be given the tools and manpower to do the job.
There is no point in trying to win the hearts and minds of major drug traffickers.
Farmers are a different story.
Forced eradication risks pushing farmers into the hands of extremists, and thus will not lead to the sustainable reduction of opium fields.
Indeed, as we have seen in some Andean countries, it can be counter-productive.
Therefore, security and development must go hand in hand.
To achieve this, Afghanistan needs more development assistance.
International support so far has been generous, but it is still well below per capita equivalents for other post-conflict situations – and the need is much greater.
Farmers will be weaned off opium over the long term only if they have sustainable livelihoods.
At the moment, Afghanistan’s drug lords are prospering, and rural communities are suffering.
That situation needs to be reversed.
We must punish the traffickers and reward the farmers.
We cannot afford to fail in Afghanistan.
Recent history has given us graphic evidence of what would happen if we do.
But any solution in Afghanistan depends on eliminating its opium.
Afghanistan’s Terrorized Women
KABUL – Recently, the Afghan Independent Human Rights Commission (AIHRC) office in Kudoz province reported the rescue of a young woman who had been imprisoned in her in-laws’ dungeon for seven months.
Fifteen-year-old Sahar Gul was forced to marry an older man who serves in the Afghan army.
She was then kept in the dungeon by her husband’s family and brutally tortured for months, because she refused to work as a prostitute.
Over the past ten years, the AIHRC has received more than 19,000 complaints related to violence against women.
Despite making some progress in investigating the complaints and referring them to the justice system, as well as in raising public awareness about the issue, the challenges remain huge.
Since 2002, many efforts have been made to improve women’s lives in Afghanistan.
The country has enacted several new laws and established a fairly advanced legal framework to end discrimination against women, including a new law that criminalizes any act that results in violence against women.
But laws and policies alone are not sufficient to protect women from horrific domestic abuse.
Indeed, the Gul case is hardly the only well-publicized case.
There was also Gulnaz, a young woman who was jailed for adultery after being raped by a relative (she was recently released after a presidential pardon, but may be forced to marry her attacker).
The husband of another young woman, Aisha, cut off her nose and ears when she ran away.
Violence against women in Afghanistan persists for many reasons.
First, the country has inherited a patriarchal tribal tradition that assumes women’s inferiority.
Women are therefore deprived of their basic rights and freedoms.
Second, there is a strong political incentive to deprive women of their rights.
Radical groups receive immense support from the large share of the population that opposes women’s rights.
The Taliban, for example, have consistently used an anti-women policy to appeal to tribal and rural people.
Third, family pride and honor are deemed more important than a woman’s individual well-being and safety.
For example, if family members beat or abuse a woman, she has few options.
Often, her only choice is to remain silent or risk disgracing the family.
If she does report the matter to the authorities, the case will almost certainly never be properly investigated, nor the perpetrators ever prosecuted.
Gul, for example, complained to the police about her abusive in-laws, but she was returned to the family when some of their influential contacts intervened.
Fourth, laws are often arbitrarily applied, and sharia (Islamic law) frequently takes precedence over civil legislation, resulting in widespread impunity for crimes of violence against women.
For example, in October 2010, the Afghan Supreme Court ruled that women who run away from home can be charged with prostitution, unless they go to the police or an immediate relative's home.&#160;It is this mindset that led to Gul’s victimization.
Finally, while the Taliban lost power ten years ago, discrimination and violence against women has occurred in Afghan society for centuries.
Thus, despite some progress, public and official sensitivity to violence against women is only slowly emerging.
The Afghan government must take several steps to protect women fully.
Above all, perpetrators of violence against women should be prosecuted and tried under due process of law.
This will require strengthening the rule of law and ending the prevailing culture of impunity.
That, in turn, requires educating the public further about human rights and women’s rights through school textbooks, continuing education courses, and a vigorous media campaign.
It also requires persuading representatives and policymakers to develop policies and allocate budget revenues to combat violence against women, and training police and judges to handle cases of violence against women without deferring to claims of family honor.
Perhaps most importantly, non-constitutional justice systems, such as sharia, must be monitored and checked, if not prohibited altogether.
As for Sahar Gul, her case must be thoroughly investigated, and the police and judiciary must commit to bringing her torturers to justice.
Furthermore, Gul’s case, and others like it, should be studied in order to understand the roots of such crimes.
Until Afghanistan’s leaders begin to address this problem seriously, our country will continue to bear the scar of violence against women on its face.
Africa, Climate Change, and the G-8 Summit
British Prime Minister Tony Blair has declared that the two issues at the center of the G-8 Summit this July will be African poverty and global climate change.
These may seem to be distinct issues.
In fact, they are linked.
A trip I took to a village in the Tigre region in northern Ethiopia shows why.
One morning, I was taken to a dry riverbed at the village’s edge.
Farmers were digging a pit in the riverbed, down to the water table approximately two meters below ground level.
They explained that until recently this was a perennial river – one that flows throughout the year – but now the river stops flowing during the dry season.
Only when the annual rains begin in the summer does water reappear in the river bed.
Until then, water-starved communities dig for water, if they can find it and if they can afford to pump it out.
In northern Ethiopia, as in much of Africa, the rain cycle has changed markedly in recent years.
Ethiopian village life has long depended on two crops, one during a short rain in March and April, and the main crop during the long rain in the summer months.
In recent years, the short rains have failed entirely, and long rains have been erratic.
Hunger is omnipresent.
Perhaps half of the children are severely underweight.
Much of arid sub-Saharan Africa, notably in the Sahel (the region just south of the Sahara desert), has experienced a pronounced drop in rainfall over the past quarter-century.
This decline coincided with a rise in the surface temperature of the neighboring Indian Ocean, a hint that the decline in rainfall is in fact part of the longer-term process of man-made global warming.
Failures of rainfall contribute not only to famines and chronic hunger, but also to the onset of violence when hungry people clash over scarce food and water.
When violence erupts in water-starved regions such as Darfur, Sudan, political leaders tend to view the problems in narrow political terms.
If they act at all, they mobilize peacekeepers, international sanctions, and humanitarian aid.
But Darfur, like Tigre, needs a development strategy to fight hunger and drought even more than it needs peacekeepers.
Soldiers cannot keep peace among desperately hungry people.
One course of action must be to help impoverished African regions to “adapt” to climate change and to escape the poverty trap.
Water-stressed regions like Ethiopia and Sudan can adapt, at least in part, through improved technologies such as “drip irrigation,” rainwater harvesting, improved water storage facilities, deep wells, and agro-forestry techniques that make best use of scarce rainfall.
Better land-management practices (the re-planting of degraded forests, for example) can recharge underground water aquifers.
Poor countries cannot afford these technologies on their own.
Nor should they have to.
Help for poor countries in Africa and elsewhere to adapt to climate change should not be described as charity or aid, but rather as compensation for damages being imposed on the poorest people on the planet.
Greater help for these countries to escape from extreme poverty has been promised for decades but has not been delivered.
In addition to adapting to climate change, the world must also reduce future risks to the planet by cutting back on emissions of greenhouse gases, which are the source of man-made climate change.
While adaptation to climate change is necessary – because it is already occurring – this is not enough.
If the world fails to mitigate future climate change, the effects of rising temperatures, increasing droughts, more numerous and severe tropical storms, rising sea levels, and a spread of tropical diseases will pose huge threats to the entire planet.
The famines in Ethiopia and the violence in Darfur suggest what can lie ahead.
The best way to reduce long-term climate change is to reduce carbon emissions.
There are at least three options: shift to non-carbon energy sources such as solar or nuclear energy; capture and dispose of the carbon dioxide emitted at carbon-based power plants; economize on energy use, for example by shifting to hybrid automobiles and trucks.
Most likely, all three of these methods will have to play a role.
The effort to reduce greenhouse gases will require decades of action, but, given the long lead times in overhauling the world’s energy systems, we must start now.
Rich countries need to lead the way.
It is ironic that the United States, which portrays itself as a friend of democracy and impoverished countries, gives the smallest share of its GNP in aid among the rich countries, and also refuses to participate in global efforts to reduce greenhouse gas emissions.
This is especially ironic because African countries like Ethiopia stand steadfastly and bravely with the US in the fight for freedom and against terrorism, even as they struggle with hunger, disease, and famine.
Moreover, countries like Ethiopia are making valiant, indeed remarkable, efforts to overcome their problems, despite the lack of adequate, and long-promised, help from the world’s richest countries.
Africans suffering from hunger and drought, and indeed poor people everywhere, have a right to ask much more of the US and other rich countries.
Tony Blair is right to call on his rich-country colleagues to follow through on their unfulfilled promises.
Africa at Risk
ADDIS ABABA – Climate change will hit Africa – a continent that has contributed virtually nothing to bring it about – first and hardest.
Aside from Antarctica, Africa is the only continent that has not industrialized.
Indeed, since 1980’s the industrialization that had taken place in Africa has by and large been reversed.
Africa has thus contributed nothing to the historical accumulation of greenhouse gases through carbon-based industrialization.
Moreover, its current contribution is also negligible, practically all of it coming from deforestation and degradation of forests and farmland.
Yet climate change will hit Africa hardest, because it will cripple the continent’s vulnerable agricultural sector, on which 70% of the population depends.
All estimates of the possible impact of global warming suggest that a large part of the continent will become drier, and that the continent as a whole will experience greater climatic variability.
We know what the impact of periodic droughts have been on the lives of tens of millions of Africans.
We can therefore imagine what the impact of a drier climate on agriculture is likely to be.
Conditions in this vital economic sector will become even more precarious than they currently are.
Africa will not only be hit hardest, but it will be hit first.
Indeed, the long dreaded impact of climate change is already upon us.
The current drought covering much of East Africa – far more severe than past droughts – has been directly associated with climate change.
The upcoming climate negotiations ought to address the specific problems of Africa and similarly vulnerable poor parts of the world.
This requires, first and most importantly, reducing global warming to the apparently inevitable increase of two degrees Celsius, beyond which lies an environmental catastrophe that could be unmanageable for poor and vulnerable countries.
Second, adequate resources should be made available to poor and vulnerable regions and countries to enable them to adapt to climate change.
Climate change, which was largely brought about by the activities of developed countries, has made it difficult for poor and vulnerable countries to fight poverty.
It has created a more hostile environment for development.
No amount of money will undo the damage done.
But adequate investment in mitigating the damage could partly resolve the problem.
Developed countries are thus morally obliged to pay partial compensation to poor and vulnerable countries and regions to cover part of the cost of the investments needed to adapt to climate change.
Various estimates have been made of the scale of investment required by those countries.
One conservative estimate – which has a reasonable chance of being accepted precisely because it is conservative – calls for $50 billion per year as of 2015, increasing to $100 billion by 2020 and beyond.
A transitional financing arrangement would be put in place for the period 2010-2015.
Some argue that developed countries cannot come up with such sums, particularly given their current economic challenges.
But no one has so far argued that the cost of damage caused to the development prospects of poor countries and regions is less than the amount of compensation being offered to cover adjustment costs.
The reason is obvious: the damage caused is many times higher than the compensation being requested.
Nonetheless, it is argued, whatever the real cost of the damage, developed countries currently cannot afford to provide that kind of money.
But we all know that these countries and their national banks were able to spend trillions of dollars in a few months to bail out their bankers, who earned super-profits when the going was good.
When the good times ended, taxpayers and governments were prepared to rescue them and to ensure that they continued to receive their extraordinary bonuses.
If the developed world is able to pay trillions of dollars to clean up its bankers’ mess, how is it possible that it cannot afford to pay billions of dollars to clean up a mess that it created, and that is threatening the survival of whole continents?
Clearly this is not about the availability of resources.
It is about the inappropriate priorities in how resources are allocated.
It is about moral values that make it appropriate to rescue bankers, who expect everyone but themselves to pay for the mess they created, and inappropriate to compensate the world’s poorest people, whose survival is threatened precisely because of the mess created by developed countries.
I cannot believe that people in developed counties, when informed about the issues, would support rescuing bankers and oppose partial compensation for poor countries and regions.
I cannot believe that they will let such an injustice occur.
If they are not expressing their outrage over the injustice of it all, it can only be because they are inadequately informed.
Africa\u0027s \
I visited Ghana recently and like many others left asking: how can a "developing" country be developed?
But there was something troubling about this formulation, in particular with the word "developing," which is often a euphemism for the absence of economic development.
Do countries stop developing because outsiders are so intent on developing them?
My hosts, the Kweku Hutchful Foundation, invited me with a different question in mind: How can Ghanaian leaders be developed?
Something troubled me about this formulation, too.
It was that word "development" again.
Do we really "develop" leaders or countries?
Do multinational companies, international non-governmental organizations, and multilateral lenders really understand local needs?
Just because some "best practice" works in New York, does that mean it will work in Accra, Ghana?
Imagine how American managers would react to consultants arriving from Ghana with their "best practice": "It worked in Accra, so it is bound to work in New York!"
Of course, there is a prominent example of just that: Kofi Annan, under whose stewardship the UN has undergone a remarkable improvement.
Annan spent most of his career outside of Ghana, and had some of his higher education in the US.
But, as one of Annan's advisors once put it, he "runs the UN like an old-fashioned African village, with long discussions among the elders, periods of reflection, and eventually a decision."
Of course, Annan can hardly control the UN by imposing great strategies on everyone else, as American corporate CEOs do.
But perhaps he knows better.
Unlike most American CEOs, Annan was not parachuted in from outside; he is the first career employee to lead the UN.
So he knew what was wrong and appreciated that it had to be fixed carefully and patiently, by engaging the staff rather than intimidating them.
His re-election to a second term as UN Secretary General was supported by nations all over the world, rich and poor, as well as by UN employees.
Was Kofi Annan "developed?"
Perhaps we don't develop leaders so much as foster the conditions that bring out leadership.
If so, then a key condition must be the self-respect that comes from working things out for ourselves, individually and collectively.
This self-respect is fostered by organizations and institutions that can likewise stand on their own feet, building on the best of their own cultural traditions.
Passive importation of techniques, controls, and beliefs, via agencies and experts that run around solving everyone else's problems, may be the biggest impediment to development--just another form of outside exploitation, of which Africa has had more than enough.
Is it not time for indigenous development, of countries and leaders alike?
One thing is clear.
Countries like Ghana do not lack enterprise.
Markets and personal initiative are pervasive.
At a red light in New York, a squeegee kid or two might approach your car; in Accra, a stopped car is immediately surrounded by a virtual supermarket.
What Ghana and most of Africa lacks is not enterprise, but
Instead, foreign corporations, with their funds, controls, and experts--and, just as importantly, their beliefs--dominate larger enterprise.
To be sure, foreign corporations can do good things: bring in fresh ideas, techniques, and processes, as well as capital and the scale required in some contemporary forms of manufacturing.
But nothing they do--aside from cosmetic modifications to consumer products and marketing tactics--responds to local conditions.
Development as now practiced often fails because it does not build on a country's unique strengths, respect its social traditions, or foster the autonomy necessary to develop indigenous leaders and enterprises.
All too often, it is
Pride, dignity, and self-confidence do not figure in economic theory: they cannot be measured.
But they figure prominently in just about every story of success, whether of countries or of leaders.
How people feel about themselves, personally and collectively, greatly influences the energy with which they develop themselves.
America, for example, did not develop by depending on an imposed ideology or outside experts.
It developed through the indigenous efforts of Americans acting in their own way--assisted by extensive state intervention, through land grants to farmers, railroads, and mining companies, military spending that stimulated the economy (and still does), and, of course, tariff barriers.
Likewise, indigenous development played a key role in Japan and Germany after WWII, in South Korea more recently, and the UK long before.
In fact, has
Globalization must not be allowed to blind us.
Ghana certainly needs to develop economically, because material wealth is required to improve health, provide education, and sustain democracy.
But the reverse is also true: a deep-rooted sense of democracy--precisely what globalization lacks--seems necessary to support economic efforts.
The (economically) developed West should consider importing that lesson.
Africa’s Avoidable AIDS Crisis
NEW YORK – At Uganda’s largest AIDS clinic, I recently witnessed a remarkable celebration of life.
The performers were a troupe of young African singers, drummers and dancers, ranging in age from roughly eight to 28.
Rarely have I been so profoundly moved.
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; “This is a land,” they sang,
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; “Where beautiful people
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; Laugh and dance in harmony.
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; Africa.
O Africa.”
And, indeed, these young people laughed and danced not only in harmony but with a joie de vivre that lit up their faces and filled us all with happiness.
Listening, it was hard to imagine that they could easily be dead – and would be, if not for this clinic.
Each of those splendid performers is living with HIV.
Some arrived at the clinic so ill that they could scarcely walk.
Others showed few symptoms but, having tested positive, came to be treated.
They were mothers and fathers, sisters and brothers, children and grandparents.
All were alive and healthy for one reason only: the Joint Clinical Research Center in Kampala, and the drugs that it provides them.
Uganda was the epicenter of the AIDS epidemic.
There the scourge began in earnest; there (as elsewhere in Africa) it exacts its highest toll.
Yet Uganda is also a success story.
A decade ago, fewer than 10,000 people were taking the new generation of antiretroviral drugs that suppress the disease and offer the promise of a normal life.
Today, that figure is 200,000, thanks in large measure to generous support from the United States (under its PEPFAR program) and the Global Fund in Geneva.
We have seen similarly encouraging progress elsewhere.
Botswana, among others, has invested heavily to offer universal treatment, and now is well on its way to ensuring that no baby is born with HIV – a reality in developed countries, but not so in Africa, where 400,000 children are born with the disease each year.
South Africa, with the largest number of people living with HIV, has spent nearly $1 billion over the past year in an ambitious counseling and testing campaign to roll back the epidemic.
But there is a new and growing danger that these advances might not be sustained.
Peter Mugyenyi, who runs the Joint Clinical Research Center, told me that part of the problem is the sheer weight of numbers.
In Uganda, only about half of those with HIV/AIDS are being treated.
Meanwhile, money for treatment is drying up.
Because of the global recession, some international donors are threatening to cap their financial support.
Countries such as Malawi, Zimbabwe, and Kenya, as well as Uganda, are requesting assistance for emergency drug supplies.
In Kampala, Dr. Mugyenyi has begun placing new patients on a waiting list.
As many as seven million Africans who should be getting treatment for HIV are not.
Worldwide, the number is about 10 million.
Compounding the problem: donors have also been shifting their focus from AIDS to other diseases, because there is a sense that more lives can be saved more cheaply.
At a time when we should be scaling up to meet the AIDS challenge, we are dialing back.
In our global war on AIDS, the international community is on the verge of snatching defeat from the jaws of victory.
Those who rallied to the fight are alarmed.
They fear that the impressive gains of the last decade will be lost. “We are sitting on a time bomb,” Dr. Mugyenyi told me.
Every day, he is forced into moral choices that no one should have to make.
How do you choose to treat a young girl but not her little brother?
How do you turn away a pregnant mother, sitting with her children, crying for help?
Surely we can do better.
In Kampala, I promised my young friends that I would do everything I could to help.
In Washington recently, the United Nations rolled out an action plan that should dramatically accelerate progress on maternal and child health, including HIV.
At the International AIDS Conference in Vienna, in July, I hope that the international community will rally around UNAIDS’ launch of Treatment 2.0 — the next generation of HIV treatment, which must be more affordable, more effective, and accessible to all.
As chair of this year’s replenishment of the Global Fund, I urge all donors to see to it that countries such as Uganda get the support they need, so that Dr. Mugyenyi and other front-line soldiers in the fight against AIDS need not make those difficult choices.
I left Uganda with a snatch of song that still echoes within my heart.
Its inherent truth would be obvious, had you been there to see:&#160;
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; We are still useful.
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; To our countries, to our families.
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; All we need is a way to live our days,
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; All we need is to survive in Africa.
Yes, times are hard.
That is all the more reason to act out of compassion and with generosity.
Africa\u0027s Debt Dilemma
The success that US President George W. Bush and his special envoy, former Secretary of State James Baker, had in getting Iraq's foreign debts canceled or rescheduled shows what can be done when a policy is backed by political will.
The contrast with Africa's debts could hardly be starker.
Just three years ago, Jubilee 2000 made news when civil society groups, rock stars, and a few finance ministers like Britain's Gordon Brown pushed for African debt cancellation.
President Bush mostly succeeded in his crusade; Jubilee 2000 succeeded mostly in getting empty promises.
Of course, the two campaigns confronted different obstacles and had different bases of support.
Baker's mission possessed the unlimited backing of a US faced with the gargantuan cost of reconstructing Iraq; Jubilee 2000 had only world opinion behind it.
Lucrative reconstruction contracts in Iraq gave America leverage to coerce its allies into submission; Jubilee 2000 had no such weapon of persuasion.
Finally, Baker was appealing to America's traditional allies in Europe and the Middle East, who need American friendship in many areas.
The campaign to forgive Africa's debt was, on the other hand, focused on the crushing debt load owed by African countries to the IMF and the World Bank, who only have money to worry about.
Still, the Jubilee campaign's street protests began a healthy debate about the lending arrangements of the IMF/World Bank.
But the time has come to abandon the Jubilee movement's humanitarian approach and focus instead on the legal aspect.
It is worth remembering that no sub-Saharan African country - with the exceptions of South Africa and Rhodesia (now Zimbabwe), then administered by white minority governments - could tap international capital markets at independence in the early 1960's.
Domestic capital markets were non-existent.
As a result, these countries were left with no choice but to subcontract their development to the IMF and World Bank.
These institutions identified, appraised, approved, and financed projects that ratcheted up African debt.
They monitored and sanctioned the recruitment of consultants that conducted the studies related to these projects, as well as the expatriate staff involved in their execution.
During the period of implementation, they carried out regular supervisory missions supplemented by quarterly progress reports and annual audits.
Still, despite all these costly measures paid for by Africans, all the post-evaluation reports on completed projects indicated that the overwhelming majority of projects were unable to generate the revenues needed to service the debts that financed them.
But the debtor countries are nonetheless obliged to service these debts by whatever means necessary, including more borrowing and further entanglement, leading to further impoverishment.
For example, per capita growth between 1965 and 1998
What went wrong?
The multilateral lenders advocated a development strategy based on the theory of comparative advantage and unbridled economic liberalization.
Africa had to open its markets while producing raw materials and basic products to generate revenues for investment in industry, education, health, and food production.
These assumptions turned out to be wrong, as export prices fell steadily over four decades.
But it should have been obvious from the outset that the IMF/World Bank strategy was doomed to fail.
All developed nations protected and subsidized their domestic production in the first phases of their development - and still do whenever it suits them.
Furthermore the US, the EU member states and the East Asian countries all regulated inward foreign investment and imposed capital controls.
Partnerships with foreign companies were designed to allow local businesses to benefit from technology transfer and training, while creating the most favorable conditions for local producers to add value to domestic production and exports.
In the face of these undeniable facts, the path chosen by the IMF and the World Bank was, at best, a mark of their incompetence and, at worst, a deliberate ploy to keep the sub-Saharan countries and their populations in bondage.
Whatever the case, both institutions certainly have much to answer for - and the loan agreements provide an arbitration clause to bring them to account.
These clauses have yet to be invoked, but for the sake of unity and to strengthen the solidarity of African countries, it would be advisable for the nascent African Union take the lead.
The AU should seek legal advice, have the issue included on the agenda of the UN General Assembly, advocate the freezing of international debt payments while arbitration proceedings are taking place and secure the support of the international community, especially NGOs that have a track record in raising awareness about Africa's debts.
Any compensation retrieved from such legal proceedings should be used to establish an African monetary fund whose objective would be to put Africa on a fast track to comprehensive regional integration, enabling the continent to capitalize on globalization's positive features.
Litigation can be costly and hazardous.
However, set against the alternative of relying on handouts punctuated by desperation and frequent episodes of collective self-destruction, Africa should choose the legal option to rally its people and rescue its development process.
Africa’s Dictator-Diplomat
BRUSSELS – The recent death in Brussels of Ethiopian Prime Minister Meles Zenawi finally brings to light what lay behind his mysterious two-month disappearance from public life.
Ethiopia’s government had strenuously denied rumors of serious ill health caused by liver cancer.
Now that the worst has, indeed, proven true, Ethiopia and all of East Africa will need to learn to live without the stabilizing influence of its great dictator-diplomat.
Meles was certainly both.
Ethiopia has undergone a remarkable transformation under his strongman rule since 1991, when his Tigrayan minority group from the country’s north came to power with the overthrow of the odious Communist Derg led by Mengistu Haile Mariam (still enjoying a comfortable retirement in Robert Mugabe’s Zimbabwe).
Initially serving as the president of the first post-Derg government, and then as Ethiopia’s prime minister from 1995 until his death, Meles (his nom de guerre in the revolution) oversaw 7.7% annual GDP growth in recent years.
Strong economic performance is somewhat surprising, given his party’s interventionist policy approach, but Meles showed himself to be a consummate pragmatist in attracting investment – particularly from China – to drive growth.
Meles’s own political provenance as the leader of the Tigrayan People’s Liberation Front was Marxist-Leninist.
But, when the Cold War ended, so, too, did his dogmatism.
To his credit, child mortality was reduced by 40% under his government; Ethiopia’s economy became more diversified, with new industries like car manufacturing, beverages, and floriculture; and major infrastructure projects, including Africa’s largest hydroelectric dam, were launched.
Once a basket-case associated in the world’s eyes only with famine and drought, Ethiopia has become one of Africa’s largest economies – and without the benefit of gold or oil.
Perhaps more important than Meles’s domestic achievements was his diplomatic record. He was an indispensable ally of the West in the fight against Islamist terrorism, culminating in Ethiopia’s military operation in neighboring Somalia in 2006.
More recently, Meles coordinated efforts with Kenya to stage limited strikes against the al-Shabaab militia, which has waged an unrelenting war to turn Somalia into a fundamentalist Islamic theocracy.
At the same time, Meles courted China as both an investor and as a hedge against the West’s criticism of his human-rights record.
And yet he controversially but rightly held out a hand of friendship to the breakaway region of Somaliland, before it became fashionable, and went as far as he could short of formal re-recognition of that ray of democratic hope in the Horn of Africa. Meles will be sorely missed in Hargeisa, as he planned to run a Chinese-financed gas pipeline through Somaliland territory from the Ogaden to the coast.
More important, Meles put Addis Ababa on the map as the home of the African Union, and as a capital where Africa’s worst problems could be discussed in a pragmatic manner, unburdened by colonial grudges.
Meles himself became a major diplomatic player, particularly over climate-change policy, and most recently was active in mediating border and natural-resource disputes between Sudan and the newly independent (and oil-rich) South Sudan.
He will be remembered for accepting the painful secession of Eritrea in 1993, rather than prolong the civil war, and for his efforts to reach an agreement with Egypt over the use of the Blue Nile waters.
The great stain on Meles’s record will always be his intolerance of dissent.
To be sure, his human-rights record was far better than the Derg’s.
For example, he allowed a private press to flourish, and in 2000 he became the first Ethiopian leader to hold multi-party parliamentary elections. Moreover, compared to neighboring Eritrea under President Isaias Afewerki or Omar al-Bashir’s Sudan, his regime was by no means the worst offender in the region.
Nor was there much evidence of personal enrichment or widespread corruption.
Nevertheless, following a violently contested parliamentary election in 2005, in which more than 30 parties participated, Meles demonstrated open contempt for democratic pluralism and press freedom, jailing several journalists in recent years. At the same time, he imposed increasingly strict central control on his ethnically and linguistically diverse country.
Although nominally governed by “ethnic federalism,” where this threatened secession, as in Oromia or the Ogaden, Meles was quick to ignore the constitutional set-up.
Although he strengthened religious freedom and peaceful coexistence between Muslims and Christians, the human-rights situation in Ethiopia remained poor.
For example, groups like Freedom House and Human Rights Watch have documented widespread official repression of the Oromo people.
And yet Meles is irreplaceable – unmatched intellectually as an African leader (he dropped out of medical school, but went on to teach himself impeccable English and obtain European university degrees by correspondence), and unmatched politically at home, with no obvious successor groomed to replace him.
In the Horn of Africa, there is no leader of his stature who could ensure the stability and strong governance that the region so desperately needs.
Hailemariam Desalegn, Meles’s foreign minister, will take over Ethiopia’s government.
But there will be considerable concern in the West about the danger of a power vacuum or struggle in a geopolitically vital but fractious country –&nbsp;and just when neighboring Somalia is supposed to be undergoing a transition to a new parliament and elected government.
For his admirers and critics alike, Meles leaves behind a potent political legacy.
He will be remembered as an African leader of major historical significance: visionary, despotic, and indispensable.
Africa’s Hard Black Gold
LAGOS - Few infrastructure services in the developed world may be as taken for granted as electric power.
To consumers in industrialized countries, uninterrupted power supply is a given.
Not so in much of Africa, which experiences some of the world’s greatest power deficits, and where only two in ten people have access to electricity.
According to the International Monetary Fund’s most recent Regional Economic Outlook for Sub-Saharan Africa , in 2007 alone, nearly two-thirds of the countries in the region experienced an acute energy crisis marked by frequent and extended electricity outages.
There is no shortage of hydropower plants for electricity generation in Africa.
However, many of these plants are unable to keep up with rapid population growth and attendant increases in demand.
Furthermore, they are prone to frequent drought, which reduces their output significantly, leaving many as little more than decorative infrastructure landmarks.
Increasingly burgeoning populations in countries like Nigeria and Ghana imply a greater extraction of water resources for power generation.
Rapid expansion of agricultural activity is requiring more and more water all across the continent.
Other resources like fuel oil, diesel, light crude, solar, and gas are also available as means of electricity generation, but their costs are all quite prohibitive.
These factors make a good argument for coal as a cheap alternative source of Africa's power.
Coal has historically played a crucial role as a source of energy worldwide, and has several important advantages over other fossil fuels.  First is its relative abundance.
The current level of proven coal reserves worldwide stands at roughly 850 billion tons.
Africa has about 50 billion tons.
Coal is also much more widely distributed geographically than any other fossil fuel.
Worldwide energy demand has increased by more than 50% since 1980, and is expected to grow annually by 1.6% between now and 2030.
More than 70% of this new demand will come from developing countries, with fossil fuels projected to account for about 80% of total energy demand by the end of this period.
Coal is the world’s fastest growing fossil fuel, with annual production increasing by 6.4% since 2004.
It is already the dominant source of power generation in some very important energy-consuming nations.
Much of the future increases in coal-fired electricity generation will come from strategically important developing countries like China and India.
In 2006 alone, China added about 93,000 megawatts of coal- fired electricity generating capacity, and this trend is expected to continue as the country tries to meet its huge energy needs.
Even in many developed countries, coal still accounts for a large share of power generation.
Coal plants currently provide more than half of America’s electricity supply.
Denmark, which houses some of the most efficient coal-fired power plants in the world, equally relies on coal for half of its electricity production.
The same is true for Germany, which is home to some of the most efficient pulverized coal combustion units in Europe.
Poland uses coal for 98% of its electricity production, and South Africa uses coal for about 50% of its electricity production.
Against this picture then, it is hard not to expect developing countries to exploit their abundant coal resources to generate power for their own development, especially given that modern technology can help produce coal cleanly.
Some argue that gas might be a better alternative to hydro or coal, but for countries that must import much of their gas the benefits of a stable and reliable source of cheap fuel in the form of coal present a very strong counter-argument to the capital costs of a gas plant.
Unlike prices for coal, which is abundant and dispersed geographically, gas prices are subject to significant volatility, and the long-term trend in the face of fossil fuel depletion is uncertain.
In contrast, coal prices are more stable, and may remain that way for a long time.
Apart from electric-power generation, coal also has wide application in a number of industries.
It is pivotal in both steel and cement production.
Moreover, the use of wood by Africa’s growing population is causing increasingly rapid deforestation in many countries.
There is significant potential domestic demand for coal briquettes to replace wood for cooking and domestic and industrial heating.
The demand outlook thus appears favorable for the coal industry, creating significant investment opportunities.
Clearly, there are environmental drawbacks from the use of coal as an energy resource, and these concerns are far too important to overlook.
The massive reserves notwithstanding, coal is still a finite resource.
It must be mined with greater efficiency and with a view to mitigating the environmental impact.
Fortunately, much greater attention is paid today to mine safety and the management of the by-products of coal use.
With acid rain and other public-health hazards linked to coal combustion, more technologies are emerging for reducing harmful emissions from power plants.
Fueled by research, the past few years have witnessed the development of increasingly cleaner and more energy-efficient coal-fired generation plants and the retirement of older technologies, especially in the developed world.
Developing countries have lagged behind in this process, but, with the common threat of global warming, there is now growing pressure to adopt conservation policies.
Africa's mineral-rich countries must exploit their abundant natural resources.
They must use coal to advance their economic development.
Failure to do so would be a missed opportunity at a time when African countries must avail themselves of all available resources for poverty reduction.
Africa’s Immunity
ACCRA – The United States suffers rising job losses.
Britain nationalizes its banks.
Once high-flying small economies like Ireland, Hungary, and Iceland break down.
Even robust China and India are experiencing slower growth, curtailed ambitions, and broken dreams.
Yet, in sub-Saharan Africa, there are few hints of the global financial crisis that is consuming the capitalist world.
In fashionable African cities, residential home prices remain stratospheric.
A typical Western-style house in Kampala or Accra, for example, now costs an astonishing two to three times the price of a comparable home in, say, Cleveland or other cities in the American heartland.
While home prices are crashing from Madrid to Dublin and Miami to Los Angeles, African prices remain near or at record-high levels.
African banks, meanwhile, are rock-solid compared to their debt-heavy counterparts in the US and Europe.
While international bankers went bust by making legions of bad loans, African bankers stuck to earning profits the old-fashioned way: paying very little to depositors, and earning a big “spread” by buying guaranteed government debt, which yielded healthy returns.
Even government deficit spending – long the bane of Africa – seems positively puny compared to the massive debts that the US and some European countries face.
The new Obama administration is proposing spending plans that would create a record US deficit of more than one trillion dollars – and this coming on top of the outgoing Bush administration’s record deficit.
And yet there are good reasons to believe that it is just a matter of time before Africa and its peoples experience the ill effects of the global crisis.
From Ghana to Kenya, governments are having increased difficulty in raising money for infrastructure projects and selling official debt.
Foreign investment in sub-Saharan Africa, which reached record levels in recent years, is retreating, which is evidence of investor caution, not any underlying lack of optimism about the region.
And exports of raw materials to China, India, Europe, and the US – a key factor in Africa’s recent growth surge – may suffer simply because the global slowdown means less consumption everywhere.
All of these factors suggest that an African financial bust is possible.
Popular equity investments, such as shares in Safaricom, are already trading at unexpectedly low levels.
If real estate prices were to fall dramatically, a chain reaction could occur, taking down big and small investors alike, and over time causing wide suffering to ordinary Africans.
Even assuming stability in real estate prices, the global crisis surely will cause a fall in remittances by Africans working good jobs in Europe, the US, Canada, Australia, and the Middle East.
Remittances are already believed to be falling, which makes sense: immigrants in rich countries are and will be disproportionately hurt by slowing economic activity.
Immigration itself may even slow dramatically, depending on the length and depth of the economic slowdown.
Fewer Africans working in rich countries will automatically translate into less money circulating in African countries.
The decline in remittances, however, cuts both ways.
Remittances have long spurred inflation in many parts of Africa.
A Ugandan doctor working in Norway, for instance, cares little about the cost of a beer in Kampala.
He is also willing – and able – to pay more than a local doctor for services and, of course, a home in Uganda.
Fewer remittances flowing into Uganda could mean less economic activity – or simply lower prices.
The financial meltdown in the US, which incubated the global crisis, is either coming under control or threatening to mutate into a new, more virulent form that could destroy not only America’s paper economy of trading and brokering, but also its real economy of goods and services.
President Barack Obama, acting as if the latter scenario remains likely, is betting on large-scale government spending to prop up the real economy.
If his administration succeeds, the chances that Africa will remain relatively unscathed will grow.
Even if Obama fails, however, Africans should escape the worst of the global crisis, for both good reasons and bad.
The good reasons have to do with African self-reliance and a growing awareness among scholars and policymakers that trade within the region – especially between urban and rural Africa – will ultimately deliver enormous benefits.
Another factor working in Africa’s favor is its private companies’ and consumers’ low dependence on borrowed money.
People tend to pay cash for goods and services, however costly.
In the US, loans for cars and homes – loans that now aren’t being paid back – are the major factor behind the financial crisis.
In Africa, very few people borrow money for such purchases.
Africa’s cash-based economy has in the past constrained development.
After all, by allowing people to spend more than they have, borrowed money can fuel growth.
But today, Africa’s pay-as-you-go practices are a powerful defense against financial contagion.
Another way of looking at Africa’s paradoxical economic position is to admit that the region’s historical marginalization within the international financial system – so costly in times of global plenty – is proving to be an unexpected benefit when the wealthiest of the world are sick unto death.
Africa’s Integration Imperative
Karl Marx predicted that states would wither away in anticipation of an idyllic communist society capable of auto-regulating economic imbalances and empowering the masses.
So he would have been flabbergasted to see his prophecy realized, not by communism, but by the globalization of Anglo-American economic liberalism.
Opening up markets to the free flow of capital, not the dictatorship of the proletariat, has rendered state power obsolete.
Today’s capital markets raise money for governments, corporate clients, and individual customers, manage pension funds’ investments, and bet on the level of interest rates or the stock market.
Trading in derivatives by investment banks, hedge funds, and other market participants, reaps huge profits for traders while depriving the real economy of productive investment and job creation.
No population in the world is spared from the harsh treatment of such a system.
Some 40% of the world’s 6.5 billion people live in poverty, and a sixth live in extreme poverty.
However, the world’s black populations are the prime victims.
In the United States, one-eighth of all black males between the ages of 25 and 34 are in jail, and three out of five black American households with children are headed by a single mother.
As for African countries, the politics and economics of globalization have stripped them of their assets and natural resources and left them with an unbearable debt burden.
As a result, the percentage of Africa’s population living in extreme poverty increased from 41.6% in 1981 to 46.9% in 2001.
On the other hand, in the era of globalization, regions in which internal trade exceeds external trade have better economic outlooks and stronger social cohesion.
This is the case for Europe, Asia, and, increasingly, Latin America, particularly among the member states of Mercosur (Argentina, Brazil, Chile, Uruguay, and Paraguay).
The opposite is true for regional groupings in Africa and in the Middle East where trade with the outside world is more important than intra-regional trade.
As a result, any country formulating strategies to counter the destructive forces of globalization should give overriding priority to a self-centred economic development strategy, preferably within a regional framework.
This is a prerequisite to defending against market fundamentalism and avoiding the iniquitous conditions of the international marketplace.
In this respect, the Association of Southeast Asian Nations (Brunei Darussalam, Cambodia, Indonesia, Laos, Malaysia, Myanmar, Philippines, Singapore, Thailand, and Vietnam) constitutes an edifying example.
The ASEAN economies adopted a united front on international economic issues and accorded priority to internal economic integration and expanding linkages with major trading partners.
Exports have remained the main driver of economic performance for the ASEAN countries, contributing to 5.8% regional GDP growth in 2006.
ASEAN foreign direct investment (FDI) flows reached US$38 billion in 2005, up by 48% from the previous year.
The outlook for 2006 was also bullish, with preliminary data for the first quarter indicating that FDI flows had already reached US$14 billion, up from US$7.4 billion in the year-earlier period.
ASEAN’s drive to establish a fully-fledged economic community has been underlined by implementation of its Priority Integration Sector Roadmaps.
By contrast, sub-Saharan Africa’s historical legacy of artificial and unmanageable colonial boundaries, ethnic antagonisms, its citizens’ deficit of self-respect, and an appalling record of leadership failures has hampered its quest for economic integration.
But a sector-by-sector approach could mitigate these handicaps, and, given the pressing need to address demand for energy and climate change, it might be strategically advisable to start with the energy sector.
Africa is a continent rich in energy, holding two-thirds of the world’s reserves of hydro-electric power – trillions of kilowatt-hours representing about half of total world resources.
The Congo River alone holds more than 600 billion kilowatt-hours of annual reserves. The Sanaga (Cameroon) and the Ogooué (Gabon) hold half as much.
Technological breakthroughs have made it feasible to transport electricity via high-voltage direct current (HVDC) over long distances without incurring great losses (only about 3% per 1,000 kilometers).
Carbon-free hydroelectric power is the right choice as sub-Saharan Africa’s principal source of energy.
Harnessing the hydroelectric power of the Congo Basin alone would be enough to meet all of Africa’s energy needs, or to light the entire continent of South America. Moreover, establishing an African grid would enable power from the Democratic Republic of Congo to be delivered to southern European countries such as Spain, Portugal, and Italy.
However, while 90% of world reserves of hydraulic energy are concentrated in underdeveloped regions like sub-Saharan Africa, HVDC technology remains the preserve of developed countries.
There is thus an imperative not only for regional integration in Africa, but also for a joint strategic vision and partnership to help build global energy and climate security.
Africa’s Misplaced Monetary Ambitions
DAKAR – Sub-Saharan African is in the grip of currency-union mania.
Regional groups of countries in eastern, southern, and western Africa are all giving priority to the idea of creating a monetary union.
But haven’t we heard this all before in Africa?
Indeed, today’s enthusiasm for currency unions ignores the poor track record of previous attempts on the continent to establish them through peaceful means.
A common currency requires unified and centrally agreed monetary and fiscal policies.
But this necessitates political integration, which, as the troubles of the euro this year have demonstrated, is a hard sell among nation-states.
Before the euro’s arrival on the international financial scene in 1999, the only examples of countries with common currencies were neo-colonial francophone Africa and nineteenth-century precedents like the Latin American and Scandinavian monetary unions.
The creation of the CFA franc, which gives France control of 65% of the CFA countries’ foreign-exchange reserves, combined currency convertibility with a grossly overvalued parity – pegged first to the French franc and now to the euro – as well as trade barriers.
This led only to structural deficits, vast capital flight, and, in 1994, a 100% devaluation.
Yet, despite the difficulties that have bedeviled the CFA (and the euro of late) – indeed, despite the absence of viable regional customs unions (except in the East African Community), let alone a single market – Africans retain a strong allegiance to the idea of a currency union.
That allegiance is misplaced.
At this stage of their economic development, with its focus on commodity exports, the priority for Africa’s countries should be long-term economic integration, not currency union.
Here, the model to follow is not the euro, but Latin America’s Southern Common Market (Mercosur) and the Association of Southeast Asian Nations (ASEAN).
Both regional groupings have, by lowering trade barriers, delivered a real catalyst to economic growth.
Mercosur’s member countries have adopted a strategy that prioritizes the creation of a free-trade area.
They steered clear of establishing a heavy budget-fueled bureaucracy, leaving respective ministries to handle administering the accord. In 2008, Mercosur intra-regional exports reached $41.6 billion, up by 28.4% from 2007.
The external trade of Paraguay, Argentina, and Brazil within the area amounted, respectively, to 65%, 33%, and 15% of their total exports.

The other major advantage of such regional groupings has been their ability to attract foreign direct investment. Since the early 1990’s, Mercosur has managed to mobilize 5.9% of world FDI inflows.
In 2008, it attracted a record $56 billion in FDI, an increase of 31.5 % from 2007. Moreover, cross-investment between Mercosur member countries has led to greater economic integration. Argentina is now Brazil’s second-largest trading partner, behind the United States, while Brazil is Argentina’s largest trading partner, ahead of the US.
The seven member countries of ASEAN decided to implement economic “roadmaps” defining integration priorities, a testimony to their determination to achieve a single economic community.
A currency union, however, is not high on the agenda.
In 2009, ASEAN regional trade represented 24.6% of its members’ total exports and 24.3% of total imports.
By contrast, trade among African countries accounts for only about 10-12% of the continent’s exports and imports.
But, partly because several African countries have escaped the hang-over from the global credit crisis, more investors are focusing on business opportunities there.
By 2050, the combined GDP of the largest 11 African economies should reach more than $13 trillion, surpassing Brazil and Russia (but not China or India).
For some analysts, the emergence of countries like Brazil as economic powerhouses stems partly from successful demutualization of their stock exchanges.
Twenty-three bourses are currently operating in Africa, and their combined market capitalization has soared from $245 billion in 2002 to $1 trillion (2% of the world total) at the end of 2009 – as high as the fifteenth-largest stock exchange in the world.
With a volume of $800 billion, the Johannesburg Stock Exchange alone accounts for 80% of the total and ranks 19th worldwide. Nigeria plans to demutualize its stock exchange in order to establish it among the prime destinations for frontier investors.
Africa stands to benefit from a massive economic turnaround, provided that the right environment for sustainable growth and rising productivity is created.
This requires consistent macroeconomic policies focused on economic integration, food self-sufficiency, low inflation, and reduced debt.
It also requires political stability, eradication of corruption, enhanced rule of law, improvement of basic levels of education, and greater use of mobile telephones and the Internet.
What it does not require is a currency union.
When it comes to exchange rates, the members of Africa’s economic groupings would be better off linking their currencies in regional monetary systems to prevent large fluctuations relative to one another.
Africa\u0027s Odious Debts
One side effect of the American/British occupation of Iraq is that it sparked public debate on a dark secret of international finance: the debts taken on by odious regimes.
As Iraq's new rulers debate what to do about the billions of dollars in foreign debts inherited from Saddam Hussein's regime, voices ranging from the charity Oxfam-International to US defence guru Richard Perle are calling for debt repudiation on the grounds that the debts Iraq now bears were contracted to sustain a corrupt, oppressive regime.
Iraq is not the only country burdened by such debts.
Across sub-Saharan Africa, many of the world's poorest people struggle with the crippling legacy of profligate lending to corrupt, oppressive rulers.
During his 32-year dictatorship, Congo's former president Joseph Mobutu accumulated a personal fortune estimated at $4 billion, while his government ran up a $12 billion foreign debt.
More of the same in Angola, where last year an IMF investigation revealed that $4 billion disappeared from Angola's treasury over the past five years.
It so happens that the Angolan government borrowed a similar sum from private banks in this period, mortgaging future oil revenues as security.
Much of Africa's ill-gotten wealth is now stashed abroad.
In a study of 30 sub-Saharan African countries, we estimate that total capital flight for the period 1970-1996 amounted to $187 billion.
Adding imputed interest earnings, the stock of Africa's capital flight stood at $274 billion - a sum equivalent to 145% of the debts owed by those countries.
In other words, sub-Saharan Africa is a net creditor to the rest of the world: its external assets exceed its external debts.
The difference is that the assets are private and the debts public.
Statistical analysis reveals that roughly 80 cents on every dollar borrowed by African countries flowed back as capital flight in the same year.
Foreign borrowing and capital flight were connected by a financial revolving door, as funds borrowed in the name of governments were captured by politically connected individuals and channeled overseas as their private wealth.
Moreover, every dollar added to a country's total debt generated 3 to 4 cents of extra capital flight per year in subsequent years, implying that capital flight was partly a response to the deteriorating economic environment associated with rising debt burdens.
In the last decade, sub-Saharan Africa recorded a "net transfer" (new borrowing minus debt service on past loans) of

But there is a remedy at hand.
The doctrine of "odious debt" dates from the end of the 19
Properly functioning financial markets require creditors to bear the consequences of imprudent lending.
The notion that lenders should always be repaid, regardless of how and to whom they lend, is indefensible.
The logic of sound banking tells us that current and future African governments should accept liability only for those portions of public debts that were incurred to finance
This policy poses two practical problems.
The first is to determine who should bear the burden of proof in identifying which debts are "odious."
Given the evidence of widespread capital flight fueled by external borrowing, African governments can rightly insist that creditors have the responsibility of establishing that their loans were used for
The second problem is that creditors may withhold new lending from governments that have the nerve to reject odious debts.
But today resources flow from Africa to creditors, rather than the reverse.
In the short run, African countries will save money by staunching this outflow.
In the long run, selective repudiation of odious debts will benefit both borrowers and creditors by promoting more responsible lending practices.
If Iraq's occupation gives impetus to legal challenges that free Africans from the burden of odious debts, then the war will have succeeded in dismantling at least one weapon of mass destruction.
Africa\u0027s Oil Rush
It takes a threat to oil supplies to get world leaders to pay attention to Africa.
Usually neglected by globetrotting statesmen, the continent recently saw visits from US President George W. Bush, Chinese President Hu Jintao, Brazil's Lula Da Silva, German Chancellor Gerhard Schroeder and many other world leaders.
Their public comments were typically devoted to development, ending Africa's many wars, and the fight against HIV/AIDS, but all of them had oil on their minds.
An oil rush is underway in the continent, because all developed countries' national security depends on a steady oil supply, and sub-Saharan Africa owns 8% of the world's known reserves.
In 2002 production was 2.1 million barrels a day in Nigeria, 900,000 in Angola, 283,000 in Congo Brazzaville, 265,000 in Equatorial Guinea, 247,000 in Gabon, 227,000 in Sudan, 75,000 in Cameroon, 28,000 in South Africa, 25,000 in the Democratic Republic of Congo, and 11,000 in Ivory Coast.
The US alone imports 1.5 million barrels a day from West Africa, the same amount it imports from Saudi Arabia.
According to the US Energy Department, within this decade American oil imports from Africa will reach 770 millions barrels annually, as exploration intensifies throughout the Gulf of Guinea, and as the US brokers peace in war-ravaged oil-producing countries, such as Sudan and Angola, and establishes strategic bases to safeguard output.
As a result, West African oil producers will be earning an estimated $200 billion over the next decade, more than 10 times the sum Western countries allocate each year to the "aid industry" in the region.
So why aren't Africans celebrating?
Because they are well aware of the "curse of oil" - corruption, conflict, ecological disaster, and anaesthetized entrepreneurial spirit.
In Nigeria, an estimated $300 billion in oil revenues entered the government's coffers over the past 25 years, but per capita income remains less than one dollar a day for the simple reason that much of the money ends up in Swiss bank accounts, like the one owned by the late dictator Sani Abacha.
In Angola, British Petroleum revealed that it had to pay a $111 million "signature bonus" to the government.
As scandalous as it might be, these sums are mere peanuts compared to the $4.5 billions in oil revenues supposedly siphoned away from African government coffers over the last decade.
But graft and theft are not the only problems.
The celebrated pipeline between Chad and Cameroon caused water pollution, devastated pygmies' hunting grounds, destroyed harvests, and spread AIDS - perhaps the inevitable result of ill-educated migrant workers toiling thousands of miles from home, with a horde of prostitutes following them. Projected annual revenues from this project for oil companies such as Chevron, Exxon, and Petronas, and lenders like the World Bank and the European Investment Bank, are estimated at $4.7 billion.
Chad will see a mere $62 million, and Cameroon just $18.6 million.
So petrodollars exacerbate poverty in sub-Saharan Africa, not cure it.
Only openness and accountability can reverse this trend.
This was the objective set by the Extractive Industries Transparency Initiative, which requested that the publication of multinational oil companies' payments to governments be made mandatory.
Unfortunately, because the companies and African politicians have every reason to avoid transparency, the EITI plans were rendered meaningless when they were not made compulsory.
What lessons can be learned from this sorry state of affairs?
One relates to the defenders of those suffering such obvious exploitation.
The people who rebelled against these injustices were not Africans, but well-wishers from western organizations such as EITI, the US aid agency Catholic Relief Services (CRS), and Global Witness.
African intellectuals, artists, and civil rights activists did not feel the need to express solidarity with fellow Africans and to defend the rights of the downtrodden.
African civil society seems trapped in indifference and inertia.
Another lesson concerns Africa's failed political leadership.
It is customary to link Africa's failures to slavery and colonization, and no one can deny such claims.
But this recognition does not exonerate Africa's "kinglets," who handed their subjects over to the slave traders.
Recently, a congregation of African church leaders convened in the Senegalese island of Gorée and urged their people to assess their share of responsibility for the slave trade.
The call went unheeded.
That is a shame, for here was a moment to expose the flaws of most African leaders throughout our tormented history.
The constant ability of Africa's leaders to betray their people is the root cause of Africa's current misery.
Their fecklessness and recklessness are what convince much of the world that Africans can do nothing but dance, slaughter each other, and beg.
Easy access to oil wealth only allows their cynical frivolity to continue.
If Africa's oil wealth is to be exploited - and it

Africa’s Press: Missing in Action
In much of Africa, the challenge for journalists, editors, and readers goes beyond freedom of the press, and involves its very survival.
Under Nigeria’s various dictatorships, for example, many journalists underwent a rite of passage that most prefer to forget: routine harassment, beatings, torture, frame-ups on spurious charges, and incongruously long prison sentences.
Among the numerous victims, perhaps the most bizarre case was that of a young journalist named Bagauda Kaltho.
His body was found in a hotel toilet in the city of Kaduna with the remains of a parcel bomb after an explosion that no one heard.
Yet there he lay, and with a copy of my book The Man Died beside him.
The implication, encouraged by the regime, was that Kaltho was a recruit of mine who blew himself up while preparing his next bomb in a campaign of terror aimed at Sanni Abacha’s dictatorship.
This unconscionable fabrication was fully exposed only after Abacha’s death and the spate of confessions that followed it by the police agents who actually committed the crime.
The press fought back tenaciously, despite casualties.
Journalists adopted tactics of underground publication, in the best tradition of East European samizdat.
When police raided one place, copies emerged from other secure depots, to be sold in the streets by kamikaze youths who darted in and out of traffic offering the subversive contraband.
It did not matter that these youthful hawkers, some no more than seven or eight years old, were often arrested, beaten, and locked up for weeks, occasionally months. When they emerged from prison, they returned to their dangerous work.
But Nigeria does not offer the premier example of the awesome power of the press. That honor belongs to a different history and region.
If benchmarks such as focus, mobilization, commitment, organization, and sheer impact are any guide, then the prize goes to the media’s baleful role in preparing the Rwandan massacre of 1994, and in directing, overseeing, and stoking the fervor of the génocidaires once the extermination of Tutsis began.
It remains a sobering lesson, one that presents the media in the role of aggressor and violator, in contrast to their normal position as victim.
Those events are too familiar to require re-hashing.
What matters now is the role that the rest of the African media should have played, and the questions that this raises about their capacity to function as a watchdog.
Not many Africans, even among those who are knowledgeable in world affairs, had ever heard of Radio Milles Collines, the most blatant instrument of the Rwandan genocide.
It is chastening that events primarily concerning Africans enter the public domain mainly owing to the intervention of the foreign media.
It was they who exposed the complicity of certain foreign powers in an ongoing crime against humanity.
And it was the foreign press that detailed the parallel failure of the United Nations, whose agents were on the ground but whose inability to call genocide by its proper name led to a comatose response.
Simply put, the African media failed to reach beyond immediate borders and serve as a voice for the continent in its encounters with the world.
The African media’s response to the massacres and rapes in Darfur has been equally muted.
Once again, African readers are being shortchanged, remaining dependent on foreign reportage in order to grasp the enormity of what is transpiring.
African civil society, whose mouthpiece is the press, cannot escape some measure of reproach for its failure to urge leaders to rescue fellow Africans.
From Liberia to the Congo, the predicament of the African continent today demands that the press act not only as a watchdog, but as a goad.
It is to the media that the continent must look for an example of solidarity.
Such solidarity should not be understood as something to be exercised only in times of convulsion.
The cheap recourse to dismissive invectives such as “outside interference,” “jaundiced reporting,” and “imperial mouthpiece” – so beloved by corrupt and/or repressive regimes – is recognized as self-serving cant even by those who routinely mouth them.
Africa’s media must respond with its own analyses, explanations, and narratives.
Unfortunately, in repressive conditions such as those in, say, Zimbabwe, Third-World journalists tend to take their cues from the conduct of their national leaders and close ranks around the continent’s rogue elephants.
This reflex has left Zimbabwe practically a journalism-free zone, with only the foreign press seeking to hold President Robert Mugabe to account.
Imitation appears to be a hallmark of tyrants in their exercise of power, so the absence of solidarity among Africa’s journalists and Africa’s peoples has created a dangerous vacuum.
Today it is Zimbabwe’s press that is under the gun.
Tomorrow?
We should all bear this in mind, for territorial ambition often goes hand in hand with the censor’s creed.
Africa\u0027s Quest for Power
Why does Africa remain poor?
Civil war, famine, disease, the legacy of colonialism-all have been advanced as plausible reasons for the continent's grinding poverty and economic backwardness.
But another factor-probably related in some ways to these others-plays a fundamental role in stifling development: a lack of modern energy sources.
Africa's enormous energy potential remains vastly under exploited. This is a key conclusion of the 2003/2004 African Economic Outlook, published recently by the OECD.
While almost half of Africa's 53 countries could profitably produce hydropower, only 7% of this potential is reached because of poor infrastructure and the high costs of initial investments.
Furthermore, despite its large geothermal and solar energy potential, Africa accounts for only 1.3% of the world's installed solar facilities, and only four countries have started exploiting underground heat sources.
Of the fossil energy sources - primarily oil - that African countries do exploit, only a quarter is consumed locally.
Limited energy development in Africa has resulted in one of the lowest uses of modern energy sources in the world.
More than three quarters of sub-Saharan Africans have no access to electricity, compared to fewer than 14% of Latin Americans and East Asians.
As a result, most Africans use biomass (animal and vegetable wastes and firewood) for lighting, cooking, and heating.
Families in rural and semi-rural areas often have no choice but to exploit what they perceive as a "free" energy source.
However, as populations grow and the need for energy increases, fragile ecosystems are threatened.
Replacing biomass sources by less destructive energy supplies has thus become increasingly urgent.
Clearly, African countries, assisted by their development partners, need to develop the continent's enormous energy potential as an integral part of their efforts to spur economic growth and reduce poverty.
Improvements in energy supplies have multiple beneficial effects.
Public and home lighting, refrigeration of food, medicine and vaccines, and heating and proper sanitation help improve people's living conditions and health.
More and higher-quality energy increases production through modernized communications, improved productivity, and a better business environment.
A secure energy supply, moreover, greatly extends learning possibilities and improves access to information.
More informed citizens, in turn, participate at a higher rate and to a greater degree in their country's decision-making processes.
Thus, institutions are rendered more democratic and governments become more transparent and responsible.
Some countries have sought to bridge the gap between their energy potential and their populations' lack of access to energy.
In a few countries, private-sector participation in electricity companies, coupled with new independent regulators, has resulted in greater and more efficient power generation and higher employment, while doubling the number of subscribers.
Electrification for the rural poor has improved in South Africa and Ghana through the creation of independent agencies in charge of implementing rural electrification plans.
Similar policies need to be adopted by a greater number of countries to enable them to address their energy challenges in a more effective way.
More attention should also be given to regional and inter-regional power initiatives, which can help smooth out the uneven distribution of energy resources across countries.
Such reforms have the potential of benefiting consumers by lowering costs and improving the reliability and quality of services.
An integrated, continent-wide energy strategy, linked to national policies for growth would, indeed, go a long way toward addressing this important need.
One vehicle for promoting such an approach is the New Partnership for Africa's Development (NEPAD).
The NEPAD Heads of States Implementing Committee has asked the African Development Bank to take the lead in regional infrastructure (including transport, energy, water, etc.) and banking and financial standards.
As part of the work on regional infrastructure, the Bank has developed a short-term action plan.
Several projects, including some in the energy sector, have been prepared or are under preparation, and four have already been approved for financing by the Bank.
Projects and programs identified in the short-term action plan are estimated to cost $7 billion.
In addition, work has also started in preparing a medium to long-term action plan in close collaboration with the regional economic communities and in cooperation with the World Bank and the European Union.
The
As Africa looks to the future, developing its enormous energy resources - through both national and regional efforts - must be given high priority.
Indeed, boosting its energy capacity will be critical to unleashing the continent's economic and human potential.
Africa’s Urban Farmers
NAIROBI – When I met Eunice Wangari at a Nairobi coffee shop recently, I was surprised to hear her on her mobile phone, insistently asking her mother about the progress of a corn field in her home village, hours away from the big city.
A nurse, Wangari counts on income from farming to raise money to buy more land – for more farming.
Even though Wangari lives in Kenya’s capital, she is able to reap hundreds of dollars a year in profit from cash crops grown with the help of relatives.
Her initial stake – drawn from her nursing wages of about $350 a month – has long since been recovered.
Wangari is one of thousands of urban workers in Kenya – and one of hundreds of thousands, even millions, across Africa – who are increasing their incomes through absentee agriculture.
With prices for basic foodstuffs at their highest levels in decades, many urbanites feel well rewarded by farming.
Absentee agriculture also bolsters national pride – and pride in traditional diets – by specializing in vegetables specific to the region. “For too long our country has been flooded with imported food and Westernized foods,” Wangari says.
“This is our time to fight back – and grow our own.”
Across Africa, political leaders, long dismissive of rural concerns, have awakened to the importance of agriculture and the role that educated people, even those living in major cities, can play in farming.
In Nigeria, former President Olusegun Obasanjo has a huge diversified farm and has pushed for policies to help absentee farmers prosper.
In Uganda, Vice President Gilbert Bukenya routinely travels the country, promoting higher-value farming, such as dairy production.
Perhaps the most visible political support for absentee agriculture is in Liberia, a small West African country where civil war destroyed agriculture, rendering the population dependent on food imports, even today.
President Johnson-Sirleaf, recognizing that educated people could contribute much to an agriculture revival, launched her “Back to the Soil” campaign in June 2008 in large part to encourage urban dwellers to farm.
To be sure, absentee farming by elites and educated urban workers can’t solve all of Africa’s urgent food needs.
Moreover, absentee farmers face unexpected problems.
Because they don’t visit their fields often, they rely heavily on relatives and friends.  When I decided to farm wheat for the first time this spring on leased land in my childhood village, my mother agreed to supervise plowing, planting, and harvesting.
Without her help, I might not have farmed at all.
Even with mother’s help, I have worries.
Although I grew up around wheat fields, my knowledge of farming is thin.
Fertilizer and spraying were both more expensive than I thought.
While my wheat stalks are sprouting on schedule, I now fear that at harvest time – in November – prices will fall and I won’t recoup my costs.
One key tool is the mobile phone.
My hopes for success are buoyed by my ability to call my mother inexpensively and discuss the farm.
We even decided over the phone what kind of pesticide to use and which tractor company to hire.
Because they know both the tastes of fellow city dwellers and rural conditions, many urban farmers are succeeding.
In fact, some city dwellers don’t even bother with acquiring land or gaining distant help.
Certain crops can be grown in their own homes.
James Memusi, an accountant, grows mushrooms in a spare bedroom, selling them to nearby hotels and supermarkets.
Nevertheless, most people living in Africa’s cities have access to land in the countryside, which is why Liberia’s government rightly highlights the potential for farm expansion.
In a new advertising campaign rolled out this summer, the authorities declared, “The soil is a bank; invest in it.”
In Liberia, the main push is to reduce imports of staples such as rice and tomatoes.
In more prosperous countries, African elites are motivated by a complex interplay of national pride, dietary concerns, and the pursuit of profit.
In Zambia, for example, Sylva Banda ignited a craze for authentic traditional meals two decades ago with a chain of popular restaurants.
Now, ordinary Lusakans want to cook similar meals in their own homes, driving demand for farmers who produce such delicacies as dried pumpkin, “black jack” leaves, and fresh Okra.
Similarly, in Nairobi, Miringo Kinyanjui, another woman entrepreneur, is supplying unrefined – and more nutritious – maize and wheat flour.
In another move to distinguish her ingredients from Western versions, Kinyanjui also sells through grocery stores flour flavored with Amarathan, a green vegetable that grows around Kenya.
The revival of traditional foods has attracted the attention of large multinational corporations.
Last year, Unilever’s Kenyan branch ran a “taste our culture” campaign in support of its line of traditional East African herbs and spices.
Such campaigns go hand-in-hand with expanded farming, because sellers of these foods prefer nearby growers – even if these growers increasingly live in the city.
African Misrule on Trial
THE HAGUE – As the world focuses on the inauguration of America’s first black president and celebrates an important milestone in the ongoing struggle for racial equality, recent developments across the Atlantic represent significant progress in a related global campaign to end impunity for mass crimes.
In the coming days, judges sitting on the International Criminal Court in The Hague will decide whether to issue a warrant for the arrest of Sudanese President Omar al-Bashir for the crime of genocide.
And on January 26, the ICC will begin its first trial – that of Thomas Lubanga Dyilo, a former Congolese warlord.
Neither event is earth shattering, but, taken together, these two steps mean that a new system of international justice is working.
Government and rebel leaders around the globe have been put on notice that criminal conduct will no longer be given a free pass.
Although the threatened indictment of al-Bashir has prompted protest in Khartoum, no one expects him to appear in court soon.
As for Lubanga, he is one of many in the Congo who has used civilians as pawns in a war that has cost more than five million lives in the past decade.
Though serious, the charges against him – recruiting child soldiers – do not pretend to encompass the range of abuses committed.
Five years after the world’s first permanent criminal tribunal commenced operations, it has made its mark.
The ICC has opened four active investigations, issued public charges against twelve people, and to date secured custody of four.
Nonetheless, the Court has come under fire for three alleged failings.
First, some argue that by interjecting itself into ongoing conflicts, the ICC has impeded efforts to secure peace.
But the facts on the ground do not bear this out.
In northern Uganda, ICC charges against leaders of the rebel Lord’s Resistance Army have helped bring an end to years of brutal fighting, and marginalized the LRA’s chief, Joseph Kony.
Similarly, in Sudan’s Darfur region, there has been no genuine peace process to disrupt, as attacks on civilians continue to be reported, even after the United Nations Security Council referred the matter to the ICC in 2005.
Even so, the ICC prosecutor’s application for an arrest warrant for al-Bashir – announced this past July – has yet to provoke a widely-feared crackdown on humanitarian organizations.
On the contrary, it may have prompted the arrest of a lower-level indictee for crimes in Darfur.
In short, more law, rather than less, is needed to help stem the violence.
Second, the ICC is said to have allowed itself to become a tool of national political leaders.
The fact that national governments referred three of the Court’s four active cases – and that in each of those cases only armed rebels or government opponents have been charged so far – has contributed to this perception.
Navigating this interplay between law and politics is perhaps the ICC’s greatest challenge.
On the one hand, the Court’s actions often have political consequences: however well-founded, accusing the leader of a rebel army may be seen as taking sides in a conflict.
On the other hand, the Court cannot charge – or refrain from charging – a senior political or military official responsible for grave crimes solely to avert negative political repercussions.
Nor would it be proper, where the gravity and scale of crimes materially differ, to charge all sides in a conflict in order to preserve a false sense of parity.
Both of these complaints reflect a third: unease with the Court’s overwhelming focus on Africa.
Some suggest that this is yet another example of Western institutions applying to Africa principles that they don’t apply to themselves.
The long history of Africa’s exploitation demands that this concern not be dismissed out of hand.
Nevertheless, Africa is the site of many of the world’s worst conflicts.
Nor is the ICC a foreign body.
Thirty African governments have ratified the ICC’s governing statute, and several of the Court’s 18 judges hail from Africa, as does a substantial portion of its staff.
Still, the Court should not hesitate to act outside Africa when mass atrocities demand redress.
The experience of other war crimes tribunals suggests that questions about political bias may take years to overcome.
Only by demonstrating professionalism in its work, and willingness to hold senior government figures accountable where appropriate, can the ICC engender broad and lasting support.
Over time, the ICC’s example should foster more effective national and regional prosecutions of serious crimes such as genocide, crimes against humanity, and war crimes.
For now, the Court can best address skeptics by more regularly and transparently explaining itself – its decisions, its mandate, and its constraints – to a global public to whom it ultimately must answer.
African Muslims In The Islamic World
Nigeria has been convulsed by religious violence triggered by the cartoons of the Prophet Muhammad published in a Danish newspaper months ago.
The violence began in the northern Nigerian city of Maiduguri during a protest by Muslims against the cartoons, with the Christian Association of Nigeria reporting at least 50 Christians killed.
Reprisals were swift, and at least 50 Muslims were killed in three days of violence in the southeastern (predominantly Christian) cities of Onitsha and Enugu.
The Nigerian protests against the cartoons (so far the most violent in Africa) raise the question: what is the role and position of African Muslims (or more accurately, sub-Saharan African Muslims) in the “Islamic World”?
When people in the rest of the world use the term “Islamic World” do they include in it sub-Saharan African Muslims, or do they have in mind only the Muslims of the Middle East and Asia?
Muslims in sub-Saharan Africa do not share many characteristics with Muslims in other parts of the world, especially those of the Arab world.
Sub-Saharan African Muslims are less assertive, and they face considerably more difficulties in their attempts to articulate their rights and establish their presence in their respective states and regions.
Part of the difficulty arises from the perpetual African dilemma of identity.
Africa has been described as a continent having a triple heritage, and the African Muslim, too, has a split personality.
He must decide whether he is a Muslim first, then a member of his tribe, say, Hausa, and then of his nation, say, Nigeria.
Even though Muslim practice is strong in Africa, there is widespread incorporation of traditional African rituals in ceremonies like weddings and funerals.
For example, among the Luhya in Western Kenya, it is not uncommon for Muslims to slaughter animals during funerals, even though, strictly speaking, there is no such provision in Islam.
Muslims in many sub-Saharan African states are also minorities.
They do not form a formidable presence, as in Nigeria, where some Muslim-majority states in the north have even implemented Sharia Law, making them more or less Islamic states.
Nevertheless, the Federal Republic of Nigeria is a secular state, as are almost all sub-Saharan African states.
The colonial legacy also helps account for the relatively docile nature of Muslims in sub-Saharan Africa.
The colonial powers’ arbitrary demarcation of borders lumped together in one state diverse ethnic groups which may have been historical antagonists.
Colonial political economy also concentrated “development” in resource-rich areas, while neglecting resource-poor regions and their populations, which in many cases were Muslim.
Thus we see a relatively poor Hausa-Muslim majority population in northern Nigeria and a relatively rich Ibo-Christian majority population in oil-rich southeastern Nigeria; a relatively rich Christian majority population in central Kenya and a relatively poor Muslim majority population on the coast and in the northeastern provinces; and so on.
As political power tends to polarize around economic power, sub-Saharan African Muslims have been under-represented in these mostly centralized political systems.
Moreover, Muslims in these mostly patron-client states have been forced to identify more with atomistic/parochial ethnic nationalism in order to enjoy the “fruits of independence” and thus acquire whatever political representation they have.
This has led to a related problem in countries bordering the Indian Ocean: disunity between coastal, more Arabized Muslims and the non-Arabized Muslims of the interior.
It is no exaggeration to argue that the more Arabized African Muslims along the Kenyan and Tanzanian coasts (including the island of Zanzibar) consider themselves “more Muslim” than the less Arabized Muslims inland.
Muslim political participation in sub-Saharan Africa has thus been extremely limited.
Political Islam is an almost unknown phenomenon in this region (the Islamic Party of Kenya was never registered, for example), and Muslim organizations have mostly focused on welfare and rights.
However, Muslims in sub-Saharan Africa, like most Muslims around the world, exhibit an “us versus them” mentality.
When Muslims form a minority, they have tended to co-exist peacefully with other religions, but where their populations are substantial (as in Nigeria), they tend to assert themselves.
Whatever ethnic and other divisions are at stake, the “us versus them” sentiment has played a large role in fomenting religious conflict in Nigeria.
Nevertheless, Nigeria (and perhaps Zanzibar) remains an exception in Islamic assertiveness in sub-Saharan Africa, which is why the idea of an active “Islamic World” includes only a relatively limited segment of sub-Saharan African Muslims.
Whether they like it or not, the majority of sub-Saharan African Muslims are represented by states that are mostly secular, Christian, or dominated by other religions.
After Arafat
Yasir Arafat appears, once again, to have held off challenges to his rule.
But his latest victory does not answer the question of what will happen when he finally does leave the political scene.
When Arafat was seriously ill in 2003, Palestinians were near panic.
Ahmad Dudin, former Fatah leader in Hebron, summed up the dilemma this way: "The Palestinian Authority has always been a one-man operation.
Arafat never really agreed to share power.
That is the problem."
Not only does Arafat have no designated successor, but he has crippled the creation of institutions that could provide for a smooth transition, develop new leaders, mediate disputes among competing candidates and factions, or check the power of a future dictator.
But at some point, Arafat will depart.
He is 74 years old, and cannot be described as healthy.
Arafat's ability to symbolize the Palestinian cause throughout the world has worn thin in recent years, but any successor would be more obscure.
So what will happen when a transition is forced on the Palestinian movement by his demise?
The best way to address that question is to focus not on who, but on what , would replace Arafat.
In a certain sense, Arafat is the Palestinian Authority (PA).
As a pro-reform Fatah official put it: "This is Arafat's narcissism. And we are all suffering from it.
I am afraid the Palestinian people will still be suffering from it even after his death."
Arafat's departure will leave a vacuum that no other institution or leader will be able to fill.
Indeed, Arafat has had several roles, and in each of them his stature has been unique.
While nominally the Palestinians have a collective leadership, the reality is that Arafat has overwhelming control.
He has been the Palestinian movement's sole leader almost from the day he founded it in 1959.
Other contenders, like Abu Jihad and Abu Iyad, were assassinated, and Faisal al-Husseini - the only major leader to rise to prominence within the West Bank and Gaza Strip - died young.
Arafat alone has the power to make everyone obey him, even if he often decides not to exercise this power.
Some argue that an obvious alternative to Arafat is democracy.
But the more likely outcome is an unstable and ineffective collective leadership, a division of power into fiefdoms, or a high degree of anarchy.
In a post-Arafat situation, it will be much harder for any successor or successors to impose discipline and hierarchy on the Palestine Liberation Organization (PLO), the PA, or Fatah.
Nor will Arafat's departure revive hopes for a political settlement with Israel.
True, Arafat's refusal to authorize crucial compromises on such matters as Israel's legitimacy and Palestine's borders has been a critical reason for the failure to resolve the Israel-Palestinian or Arab-Israeli conflicts.
Given Arafat's stature and control over the movement, he could have downsized the Palestinians' goals to acceptance of a state in only part of historic Palestine.
But he never took the leap, and the major issues remain unresolved.
The problem is that even if future Palestinian leaders want to resolve the issues that block peace with Israel, doing so will be far more difficult than it would have been for Arafat.
Under Arafat's long rule, whole generations of Palestinians have been indoctrinated with the belief that only total victory is acceptable.
Indeed, beyond day-to-day policies, Arafat has constructed the Palestinian movement's intellectual and psycho-political style, which is dogmatic and uncompromising.
Arafat's legacy is thus a political culture in which a leader may portray a disastrous tactic, strategy, or outcome as a victory and be believed.
So no political price is ever paid for continuing wars that cannot be won or making demands that will not be met.
The acceptance of violence without limit will be the most devastating feature of Arafat's legacy.
Many movements throughout history have used violence, but few have so thoroughly justified and romanticized it.
This problem will not go away when Arafat does.
How can someone with less legitimacy than Arafat escape this justification of violence?
Entire groups - Hamas, Islamic Jihad, Fatah's al-Aqsa Martyrs' Brigades - and their leaders owe their power to their willingness to kill Israelis, which has become the ultimate measure of political virtue.
Any effort by Palestinian security authorities to put down these groups by force would lead to far more violence.
Arafat's refusal to take sides ideologically helped create an illusion of Palestinian unity, with everyone dedicated to a single Palestinian struggle.
He achieved this consensus by devaluing statehood as an end in itself, building political unity on the myth of an idealized pre-1948 Palestinian society that could be "recreated" on the basis of the "right of return" and Israel's disappearance.
These aims are never going to be realized, but they also have never been subordinated to "ending the occupation," so they form the glue of Palestinian nationalism.
In today's divisive circumstances, the emergence of a new Palestinian leader will most likely take years.
During that interregnum, we should expect deadlock, anarchy, or civil war.
After Assimilation
Human migration is as old as history.
Even migration to distant places and remote cultures is nothing new.
In the nineteenth century, millions of Europeans sought liberty and prosperity in the Americas, notably in the United States.
What is new today is the scale of migration, often across huge cultural divides - and often without a definite aim.
The African boat people in the Mediterranean are often not even sure whether they want to be in Italy, Germany, or Britain.
Even those who are certain, like North Africans in Spain and France, or Turks in Germany, had as their priority escaping the hopelessness of their home countries, not arriving at a particular destination.
This modern form of migration raises massive problems for countries on the receiving end.
In Europe, it is probably the most serious social issue today, because no one has a clear idea about how to manage the resulting clash of cultures.
Once upon a time, North America, notably the US, seemed to provide the answer.
It was that of the "melting pot": different peoples made their own contribution to American culture, but, above all, they made every effort to accept what they found and integrate. "No," the Russian woman who came to the US in the early twentieth century replied to the grandchild who asked whether her ancestors arrived with the Pilgrims on the Mayflower.
"Our ship had a different name, but now we are all Americans."
More recently, this has changed, giving rise to a process described by Arthur Schlesinger, the historian and former aide to President John F. Kennedy, in his book
Even in Israel, the last true immigration country - at least for Jews - assimilation is no longer so easy.
Recent newcomers from Russia have their own political party, and old Europeans have become a distinct minority.
Israel and America continue to have mechanisms to integrate new migrants.
Language is an important underlying factor, and in Israel, there is the army, while in America, the values embodied in the Constitution still represent a shared secular faith.
But these mechanisms are weakening everywhere, and are virtually non-existent in European countries.
Modern societies are characterized by acute problems of belonging.
They don't offer the implicit, unconscious ties of community that citizens felt in the past.
As a result, people have begun to cling to other, more primordial group identities.
They resist assimilation, fearing that it will rob them of their identity without offering a new one.
What then is the alternative to assimilation?
The "salad bowl" of so-called multiculturalism is no real alternative, because it does not provide the necessary glue that binds communities together.
All the ingredients remain separate from the outset.
The only viable alternative for which there are examples is probably that of London or New York.
The main characteristic of this alternative is the coexistence of a common public sphere shared by all and a considerable degree of cultural separation in the "private" sphere, notably in residential areas.
The public space is multicultural in terms of people's backgrounds, but is governed by agreed values, even a common language, whereas the people's private lives are - to use an ugly word - ghettoized.
In theory, this is a distinctly second-best solution to the cultural consequences of migration; in practice it is the best answer we have.
But it cannot be had for nothing.
Even the necessary minimum of a common language requires a deliberate effort, to say nothing of certain rules of behavior.
Living in London, I marvel at the way in which we Londoners have come to terms with Indian family shops and West Indian-run public transport, while not asking many questions about whole districts that are Bangladeshi or Chinese.
No one has yet found a name for this new version of the "separate but equal" doctrine that some of us fought so hard against in the 1960's: separate private lives in a common public space that is equal for all.
This is clearly easier in London and New York than it is in smaller towns or even in the capitals of countries where the world language of English is not spoken.
Berlin's Turkish community and the North African communities around Paris seem increasingly separate, with their own public sphere and often language.
Where this happens, an explosive condition can arise, a kind of separatism within, not by historically separate groups but by newcomers against natives.
If we are forced to abandon the hope of assimilation, our efforts should concentrate on creating a public space to which all contribute and that all enjoy.
Ideally, this should be an expanding public space, for in the end, the element of unity in a modern society is the guarantee of its citizens' liberty.
After Austerity
NEW YORK – This year’s annual meeting of the International Monetary Fund made clear that Europe and the international community remain rudderless when it comes to economic policy.
Financial leaders, from finance ministers to leaders of private financial institutions, reiterated the current mantra: the crisis countries have to get their houses in order, reduce their deficits, bring down their national debts, undertake structural reforms, and promote growth.
Confidence, it was repeatedly said, needs to be restored.
It is a little precious to hear such pontifications from those who, at the helm of central banks, finance ministries, and private banks, steered the global financial system to the brink of ruin – and created the ongoing mess.
Worse, seldom is it explained how to square the circle.
How can confidence be restored as the crisis economies plunge into recession?
How can growth be revived when austerity will almost surely mean a further decrease in aggregate demand, sending output and employment even lower?
This we should know by now: markets on their own are not stable.
Not only do they repeatedly generate destabilizing asset bubbles, but, when demand weakens, forces that exacerbate the downturn come into play.
Unemployment, and fear that it will spread, drives down wages, incomes, and consumption – and thus total demand.
Decreased rates of household formation – young Americans, for example, are increasingly moving back in with their parents – depress housing prices, leading to still more foreclosures.
States with balanced-budget frameworks are forced to cut spending as tax revenues fall – an automatic destabilizer that Europe seems mindlessly bent on adopting.
There are alternative strategies.
Some countries, like Germany, have room for fiscal maneuver.
Using it for investment would enhance long-term growth, with positive spillovers to the rest of Europe.
A long-recognized principle is that balanced expansion of taxes and spending stimulates the economy; if the program is well designed (taxes at the top, combined with spending on education), the increase in GDP and employment can be significant.
Europe as a whole is not in bad fiscal shape; its debt-to-GDP ratio compares favorably with that of the United States.
If each US state were totally responsible for its own budget, including paying all unemployment benefits, America, too, would be in fiscal crisis.
The lesson is obvious:&#160; the whole is more than the sum of its parts.
If Europe – particularly the European Central Bank – were to borrow, and re-lend the proceeds, the costs of servicing Europe’s debt would fall, creating room for the kinds of expenditure that would promote growth and employment.
There are already institutions within Europe, such as the European Investment Bank, that could help finance needed investments in the cash-starved economies.
The EIB should expand its lending.
There need to be increased funds available to support small and medium-size enterprises – the main source of job creation in all economies – which is especially important, given that credit contraction by banks hits these enterprises especially hard.
Europe’s single-minded focus on austerity is a result of a misdiagnosis of its problems.
Greece overspent, but Spain and Ireland had fiscal surpluses and low debt-to-GDP ratios before the crisis.
Giving lectures about fiscal prudence is beside the point.
Taking the lectures seriously – &#160;even adopting tight budget frameworks –&#160;can be counterproductive.
Regardless of whether Europe’s problems are temporary or fundamental – the eurozone, for example, is far from an “optimal” currency area, and tax competition in a free-trade and free-migration area can erode a viable state – austerity will make matters worse.
The consequences of Europe’s rush to austerity will be long-lasting and possibly severe.
If the euro survives, it will come at the price of high unemployment and enormous suffering, especially in the crisis countries.
And the crisis itself almost surely will spread.
Firewalls won’t work, if kerosene is simultaneously thrown on the fire, as Europe seems committed to doing: there is no example of a large economy – and Europe is the world’s largest – recovering as a result of austerity.
As a result, society’s most valuable asset, its human capital, is being wasted and even destroyed.
Young people who are long deprived of a decent job – and youth unemployment in some countries is approaching or exceeding 50%, and has been unacceptably high since 2008 – become alienated.
When they eventually find work, it will be at a much lower wage.
Normally, youth is a time when skills get built up; now, it is a time when they atrophy.
So many economies are vulnerable to natural disasters – earthquakes, floods, typhoons, hurricanes, tsunamis –&#160;that adding a man-made disaster is all the more tragic.
But that is what Europe is doing.
Indeed, its leaders’ willful ignorance of the lessons of the past is criminal.
The pain that Europe, especially its poor and young, is suffering is unnecessary.
Fortunately, there is an alternative.
But delay in grasping it will be very costly, and Europe is running out of time.
After Chernobyl
KIEV: Chernobyl, the world’s most notorious nuclear power plant, will be shut down today, fourteen years after it spewed clouds of radioactive dust into the atmosphere.
Back then, Ukraine became the focus of global attention, but Ukrainians learned of the disaster much later than the rest of the world.
I recall that fateful Saturday afternoon with utter clarity, strolling through Kiev with my six-year-old daughter, oblivious to the danger.
Chernobyl changed Ukraine forever, and was a catalyst for the downfall of the Soviet Union.
Ultimately, Chernobyl changed the world.
Now that Chernobyl will be permanently shut down, world attention again turns to Ukraine, this time in hope, not fear.
Shutting down a vital source of electrical energy, which Chernobyl remains, is no an easy task, particularly with winter upon us.
Ukraine’s energy infrastructure is weak; losing 8-10% of our electricity production and $100 million in revenues will strain the system even more.
We also bear the responsibilities involved in laying off Chernobyl’s workers and depriving the adjacent city of Slavutych (population 28,000), of its main source of income.
Moreover, we must continue dealing with the technical and ecological issues surrounding the Chernobyl sarcophagus, as well as maintain nuclear safety in the remaining nuclear plants in operation in Ukraine, including what is left of Chernobyl.
In recent years, Ukraine alone financed all the costs of dealing with the Chernobyl disaster, consistently spending 5-10% of our state budget revenues to this end.
Dealing with the aftermath of the disaster will remain a financial burden even with the plant closed.
Yet closing the plant proves that we keep our promises.
We committed to shut down Chernobyl this year in an agreement with the G-7 countries and the European Commission.
Closure of the last remaining reactor at Chernobyl, I believe, must mark the beginning of a new phase of cooperation with the European Union and G-7 countries.
Closing Chernobyl may be the most dramatic, but it is only a single episode in our reform efforts.
When we gained independence, the world expected great things from Ukraine.
Over the course of a decade, the world appeared to forget about us.
Now we finally have a chance to break out of the downward spiral of economic decline.
At long last, our society and economy are set on the path to growth and development.
After pro-reform forces secured victory in last year’s presidential elections, all of the branches of government began cooperating to entrench democracy and the market economy in Ukraine.
For the first time since independence, Ukraine recorded growth in industrial output of 12.5% in the first 11 months of 2000, and GDP has grown by 5.4% this year.
Tough decisions were made, not least involving the energy sector.
Barter payments, -- which stifled the energy market -- were eliminated. So too tax breaks and other privileges that skewed the playing field in favor of a select few.
We are also doing everything possible to make Ukraine attractive to foreign investors.
In the energy sector, we are privatizing state-owned energy distributors.
To ensure complete transparency in the tender process, and eliminate problems experienced in the past, Western advisors are assisting us.
For our intention is to attract large Western energy companies with experience in this field as strategic investors.
Ten leading international energy companies, indeed, will participate in the privatization of the first group of companies.
By the end of next year, we will privatize 12 additional energy companies.
In its sheer scale, this may be the largest single energy privatization ever attempted in Europe.
Our energy companies service a territory the size of France; they will all be privatized.
Of course, these reforms are resisted, primarily by the oligarchs who thrive on a lack of transparency and use privileged access to state resources to enhance their business interests.
By eliminating barter and requiring monetary payment, we curtailed their opportunities to profit at the state’s expense.
Although we can and will continue the fight against corruption, we cannot do everything alone.
International support for our efforts is crucial.
A positive sign here is the recent decision by the European Bank for Reconstruction and Development (EBRD) to provide $215 million to complete the nuclear power stations at Rivne and Khmelnytsky, which will compensate for the loss of Chernobyl’s generating capacity.
Euroatom is also helping with a loan of $585 million to finance repairs at Ukraine’s functioning nuclear power plants.
In return, Ukraine will uphold its end of the deal: in addition to closing Chernobyl, we will introduce Western nuclear safety standards and renew cooperation with the IMF via the Extended Fund Facility program.
We still need other potential creditors to confirm their involvement.
Closing Chernobyl will not eliminate the Chernobyl threat.
Our people will be unable to live on thousands of acres of contaminated earth for hundreds of years.
The concrete sarcophagus built over the destroyed reactor must be renovated.
We are grateful to all the donor countries who, with contributions from our own state budget, helped raise the $760 million needed to make the sarcophagus safer.
In the end, Chernobyl’s legacy does not belong solely to Ukraine, for ours is a country located in the heart of Europe.
We are a European nation.
We realize that no one will implement reforms for us.
The West can, however, help speed up and facilitate our efforts, as it did with the closure of Chernobyl.
Cooperation between Ukraine and the West, without a disaster to concentrate everyone’s minds, will benefit all concerned.
After Dictatorship
The war in Iraq had barely begun when the minds of those who conceived the invasion turned to what should happen after the victory over Saddam Hussein's regime-a victory everyone assumed to be inevitable.
Politicians and experts have invariably sought to draw comparisons with recent examples like Afghanistan, Sierra Leone, East Timor, but also with more remote and fundamental cases.
After all, what is expected in Iraq is the fall of a highly ideological dictatorship.
Is there anything we can learn from the last examples of this kind, from the collapse of communism in Eastern Europe in 1989, or the end of the Third Reich in Germany in 1945 and the process of "de-Nazification" that followed?
The risks of such comparisons are almost too obvious for words.
Every case has its own defining features.
Regarding the demise of communism, the experiences of, say, Poland and Romania are profoundly different.
As we cross even more profound cultural boundaries, comparisons become still less relevant.
Yet there are a few issues that are common to the unraveling of most ideological dictatorships.
One such issue has to do with memory, and dealing with the past.
This is connected with a practical question: who is in a position to build a new country on the ruins of the old regime?
It is rare for a counter-elite to emerge quickly, much less for an elite-in-waiting to take over.
In Germany in the 1950s, there were many complaints about old Nazis reappearing in a democratic guise and occupying important positions.
Many of us fought against the apparent "restoration" of the old regime.
In Eastern Europe, the old leaders initially disappeared from the scene, but not for long.
Ex-communists were often reincarnated politically as social democrats.
Those who had been in the resistance-like Václav Havel in Prague-found this hard to take.
Yet these communist apparatchiks were not the same people they had been.
Circumstances had changed and the people with them.
This has a great deal to do with methods of dealing with the past.
Post-1945 Germany and post-1989 Poland are examples of countries that moved forward without much attention to the past.
Intellectuals complained that too little time and energy was spent on "mastering" the past.
They had good reasons to complain.
Indeed, after a decade or so, the need to draw clear lines and tell all, including the painful stories, became overwhelming.
But the most successful postcommunist countries moved forward first and coped with the past later.
Those who could not lift their eyes from the horrors of the past made less progress.
A second general issue has to do with priorities.
There is a natural tendency-especially in Anglo-Saxon countries-to regard elections as the most effective institutional remedy for countries emerging from ideological dictatorships.
To be sure, elections are important; but by themselves they do not solve problems.
Indeed, if they disappoint, the very principles on which they are based-democracy and civil liberties-will be discredited.
I am a strong, almost old-fashioned believer in parliamentary democracy, but when it comes to situations like postwar Iraq, two other imperatives are equally pressing.
One is the need to establish an effective administration to make certain that new policies of tolerance and market economics are actually implemented.
In East European countries, this was a major problem, and it is only through accession negotiations with the European Union that such administrative reform has been assured.
The second imperative is the rule of law.
The law has a different place in different cultures, and it presents special problems in Islamic countries.
However, it is crucial that every effort be made to educate and install an impartial and incorruptible judiciary.
Judges must not only be honest, but must be seen to be so and, as such, trusted.
The process of establishing the rule of law has been difficult and has remained incomplete in most post-dictatorial countries; yet it will be a key to successful recovery in Iraq as well.
One of the key points that I emphasized in my 1990 book

This is notably the case in economic terms.
Even post-War Germany's acclaimed, miracle-working Economics Minister, Ludwig Erhard, was deeply unpopular in the early 1950s, because it appeared as if only a few were getting rich, while most West Germans remained poor or got poorer.
For a certain period in the process of recovery, it is simply necessary for people to keep their nerve.
Poland is a recent example of a country that achieved this feat.
It requires plausible leadership and the reasonable hope that things will get better before long.
If this time of transition goes wrong, the alternative is renewed turmoil and the victory of fundamentalists of one kind or another.
The warning "Beware of the valley of tears!" is thus the one that must be taken most seriously by those responsible for rebuilding an Iraq scarred by the terrors of dictatorship and the ravages of war.
After Kyoto
The Kyoto Protocol treaty has now entered into force for the 126 nations who have joined it so far.
Now is the time to start thinking about how to engage all nations, including large emitters, in conversations about what to do after the treaty’s expiration in 2012.
This is exactly what the European Commission did recently by providing its first strategy for a post-Kyoto era, which will be discussed by the European Council next March.
While the Kyoto Protocol represents only a modest reduction of carbon emissions in industrialized countries – 5.2% between 2008-2012 relative to 1990 levels, with varying targets for individual countries – real progress can be made in sustaining development efforts and preserving our planet.
But first, all countries must integrate climate concerns into policy planning, and improve their governance in key sectors such as energy, infrastructure, and transport.
In other words, we must act in accordance with the recognition that climate change and its effects on people in both rich and poor countries remains a threat to global security.
At the end of the day, the long-term approach is likely to include a rules-based system, an incentives system, and investments in technology change.
Increasingly, adaptation at the national level will be recognized as a major issue that will require appropriate funding.
Dealing with the impacts of climate change and with emission reductions should not be mutually exclusive, but complementary.
Looking ahead to the post-Kyoto world offers us the chance to start a new dialogue and to look at new options on climate change.
Nations could set the more ambitious goal of limiting the long-term change in the earth’s temperature, and then assign emissions rights among countries in such a way that will eventually limit temperature increases to an acceptable level.
This would require increasing investments in energy research and development for new and improved technologies – a process that needs to be supported by stronger public-private partnerships.
Up to now, with only 15% of the world’s population, rich countries have been responsible for more than 75% of global carbon dioxide (CO2) emissions, and thus most of the environmental damage.
However, it is the developing countries – and thus the world’s poor – who are most vulnerable.
It is unrealistic to ask poor countries, where more than 1.6 billion people do not have access to clean energy and technologies, to bear the costs associated with the much needed technological change.
Working with partners, the World Bank is supporting financial strategies to assist developing countries in meeting the costs caused by climate change.
To date, over $1 billion dollars in Global Environment Facility (GEF) grants, together with about $8 billion in co-financing, have been committed to programs related to climate change.
While the regulatory mechanisms of both Kyoto and the European Trading Scheme have contributed to the establishment of an emerging market for carbon trading, interested parties are now concerned about the immediate future.
Without a regulatory framework beyond 2012, the window of opportunity for initiating project-based transactions will close by 2006/2007.
Given the long lead time between project preparation and the first benefits of emissions reductions, project developers have only a few years to act before carbon payments cease to make a meaningful contribution to project finance in the current context.
Developing infrastructure projects is a long process that requires 3-7 years from identification, through licensing, financing, and construction, and finally to the first certification of carbon emission reductions.
Therefore, projects need to be operational at the latest by 2007.
The World Bank has been instrumental in advancing carbon finance as a viable development tool, and in facilitating private-sector participation in the market.
The Bank is focused on representing the interests of its borrowing countries, helping them to develop assets for carbon trading according to their own priorities.
But, without a commitment by governments to limit greenhouse gas emissions beyond 2012, the carbon market will remain uncertain, and the private sector – vital to the market’s success – is unlikely to expand its participation in a meaningful and sustained way.
According to a recent World Bank-supported survey of companies interested in carbon finance, only one in five respondents declared that they were interested in buying post-2012 emissions reductions.
Now is the chance to look forward and enlist the global community – with no exclusions, although with differentiated responsibilities – in the pursuit of a more secure world, one that avoids the dire risks of environmental degradation and social conflict implied by inaction.
After Neoliberalism, What?
Two decades of applying neoliberal economic policies to the developing world have yielded disappointing results.
Latin America, the region that tried hardest to implement the "Washington Consensus" recipes--free trade, price deregulation, and privatization--has experienced low and volatile growth, with widening inequalities.
Among the former socialist economies of Eastern Europe and the Soviet Union, few have caught up with real output levels that prevailed before 1990.
In Sub-Saharan Africa, most economies failed to respond to the adjustment programs demanded by the IMF and World Bank.
The few instances of success occurred in countries that marched to their own drummers--and that are hardly poster children for neoliberalism.
China, Vietnam, India: all three violated virtually every rule in the neoliberal guidebook, even as they moved in a more market-oriented direction.
It is time to abandon neoliberalism and the Washington Consensus.
But the challenge is to provide an alternative set of policy guidelines for promoting development, without falling into the trap of promulgating yet another impractical blueprint, supposedly right for all countries at all times.
The record suggests that an adequate growth program needs to be anchored in two strategies: an investment strategy designed to kick-start growth in the short term, and an institution-building strategy designed to provide an economy with resilience in the face of adverse shocks.
The key to investment strategy is to get domestic entrepreneurs excited about the home economy.
Encouraging foreign investment or liberalizing everything and then waiting for things to improve does not work.
An effective strategy must accomplish two tasks: encourage investment in non-traditional areas, and weed out projects and investments that fail.
For this, governments must deploy both the carrot and the stick.
Learning what a country is (or can be) good at producing is a key challenge of economic development.
The carrot is needed because there is great social value in discovering, for example, that cut flowers, or soccer balls, or computer software can be produced at low cost, because this knowledge can orient the investments of other entrepreneurs.
The entrepreneur who makes the initial "discovery" can capture only a small part of the social value that this knowledge generates, as other entrepreneurs will quickly emulate him.
Consequently, entrepreneurship of this type--learning what can be produced--will typically be under-supplied in the absence of non-market incentives.
In turn, the stick is needed to ensure that these incentives do not lock in unproductive and wasteful investments.
Implementing such a strategy may differ from country to country, depending on administrative capacity, the prevailing incentive regime, the flexibility of the fiscal system, the degree of sophistication of the financial sector, and the underlying political economy.
Time-bound subsidy schemes, public venture funds, and export subsidization are some of the ways in which this approach can be implemented, but there are many others.
No single instrument will work everywhere.
Governments without adequate capacity to exercise leadership over their private sectors are likely to mess things up rather than improve allocation of resources.
The job can be done, but economic growth requires more than eliciting a temporary boost in investment and entrepreneurship.
It also requires effort to build four types of institutions required to maintain growth momentum and build resilience to shocks:
Market-creating institutions (for property rights and contract enforcement);
Market-regulating institutions (for externalities, economies of scale, and information about companies);
Market-stabilizing institutions (for monetary and fiscal management);
Market-legitimizing institutions (for social protection and insurance).
Building and solidifying these institutions, however, takes time.
Using an initial period of growth to experiment and innovate on these fronts can pay high dividends later on.
A key point here is that institutional arrangements are, by necessity, country-specific.
Discovering what works in any one country requires experimentation.
After all, institutions are not hot-house plants capable of being planted in any soil and climate.
Reforms that succeed in one setting may perform poorly or fail completely in others.
Such specificity helps explain why successful countries--China, India, South Korea, and Taiwan, among others--usually combined unorthodox elements with orthodox policies.
It also accounts for why important institutional differences persist among the advanced countries of North America, Western Europe, and Japan in areas such as the role of the public sector, the legal system, corporate governance, financial markets, labor markets, and social insurance.
While economic analysis can help in making institutional choices, there is also a large role for public deliberation and collective choice.
In fact, we can think of participatory democracy as a meta-institution that helps select among the "menu" of possible institutional arrangements in each area.
Designing such a growth strategy is both harder and easier than implementing standard neoliberal policies.
It is harder because the binding constraints on growth are usually country-specific and do not respond well to standardized recipes.
But it is easier because once those constraints are appropriately targeted, relatively simple policy changes can yield enormous economic payoffs and start a virtuous cycle of growth and institutional reform.
Adopting this approach does not mean abandoning mainstream economics--far from it.
Neoliberalism is to neoclassical economics as astrology is to astronomy.
In both cases, it takes a lot of blind faith to go from one to the other.
Critics of neoliberalism should not oppose mainstream economics--only its misuse.
Selepas Serangan di Paris
NEW YORK – Sejumlah serangan di Paris yang dilakukan oleh beberapa pelaku yang terkait dengan Islamic State atau ISIS (Negara Islam Irak dan Suriah), yang terjadi setelah dua bom meledak di Beirut dan jatuhnya pesawat Rusia di Semenanjung Sinai, kembali menegaskan fakta bahwa ancaman teroris telah memasuki fase baru dan semakin berbahaya.
Alasan mengapa ISIS memutuskan untuk melakukan serangannya sekarang masih berupa dugaan; barangkali ISIS memperluas terornya ke tingkat global untuk mengimbangi kehilangan wilayah mereka di Irak.
Tetapi, apapun alasannya, wajib bagi kita untuk memberi respon jelas.
Sebetulnya, tantangan yang ditimbulkan ISIS justru menjadi seruan bagi kita untuk menerapkan beberapa tindakan, sebab tidak ada kebijakan tunggal yang memadai.
Berbagai upaya perlu diterapkan di berbagai bidang.
Pertama, militer. Serangan udara yang lebih kuat untuk melemahkan aset-aset militer, fasilitas minyak dan gas, dan para pemimpin ISIS sangat penting.
Meskipun demikian, serangan udara saja tidak akan cukup.
Unsur militer yang kuat di darat juga diperlukan jika ingin mengambil alih dan mempertahankan wilayah darat.
Sayangnya, kita tidak punya cukup waktu untuk membangun kekuatan gabungan di darat dari nol.
Upaya ini sudah dicoba namun gagal, dan negara-negara Arab tidak mampu atau tidak mau membentuk armadanya.
Tentara Irak pun gagal memenuhi harapan.
Kelompok milisi yang didukung Iran justru memperburuk keadaan.
Pilihan terbaik adalah bekerja sama lebih erat dengan tentara Kurdi dan memilih warga dari suku Sunni di Irak dan Suriah.
Hal ini berarti menyediakan intelijen, senjata, dan kemauan untuk menugaskan lebih banyak tentara – lebih dari 3.500 tentara Amerika sudah bertugas disana dan kemungkinan akan ada penambahan 10.000 tentara – untuk melatih, memberi masukan, dan membantu mengarahkan tindakan militer.
Upaya seperti ini harus bersifat kolektif.
Bentuknya bisa informal – “koalisi negara-negara yang bersedia” (coalition of the willing) yang meliputi Amerika Serikat, Perancis, Britania Raya, negara-negara Arab, dan bahkan Rusia dalam kondisi yang tepat – atau dilaksanakan di bawah naungan NATO atau PBB.
Tidak masalah apapun bentuk atau kemasannya, yang penting adalah hasilnya.
Tetapi, deklarasi perang secara simbolis haruslah dipertimbangkan dengan penuh kehati-hatian, jangan sampai ISIS terkesan memenangi perang ini.
Komponen diplomasi pun tidak kalah penting dibanding tindakan lainnya.
Presiden Suriah, Bashar al-Assad, adalah sosok yang layaknya menjadi recruiting tools bagi ISIS dan harus dicopot dari jabatannya.
Siapapun yang meneruskan jabatan beliau harus mampu menjaga ketertiban dan melarang ISIS memanfaatkan kekosongan kekuasaan, seperti yang terjadi di Libya.
Selain itu, perubahan politik yang teratur hanya akan dimungkinkan melalui dukungan Rusia dan Iran.
Satu opsi jangka pendek yang patut dicoba adalah pembentukan pemerintah koalisi yang masih dipimpin oleh perwakilan dari kelompok minoritas Alawite, suatu persetujuan yang sepadan dengan mencopot jabatan Assad.
Secara prinsip, dan seiring berjalannya waktu, pemerintah pusat yang lebih representatif akan terbentuk, walaupun pembahasan mengenai rencana pemilihan umum dalam waktu 18 bulan ke depan nampaknya terlalu bermuluk-muluk.
Mencapai suatu kompromi dalam pembentukan pemerintah hampir mustahil.
Itulah sebabnya penguatan upaya militer diperlukan untuk menjadikan daerah-daerah kantong lebih aman dan luas sehingga mampu melindungi penduduk sipil dan memerangi ISIS.
Suriah bukanlah negara yang hidup dalam kondisi layak dan normal, bahkan mungkin tidak akan bisa kembali ke kondisi normal.
Negara Suriah yang terdiri dari daerah-daerah kantong dan canton-canton adalah bentuk yang lebih realistis di masa mendatang.
Elemen lainnya yang sangat diperlukan dalam strategi efektif apapun harus meliputi bantuan tambahan atau memberi tekanan kepada Turki untuk mengambil lebih banyak tindakan demi membendung rekrutmen ISIS.
Turki, bersama dengan Yordania dan Lebanon, memerlukan bantuan finansial lebih besar sebab mereka memikul sebagian besar beban karena persoalan  pengungsi.
Pemimpin Arab dan Muslim dapat berperan dengan menyampaikan pesan untuk menantang visi ISIS dan mendelegitimasi perilaku mereka.
Aspek domestik dalam kebijakan juga diperlukan.
Keamanan dalam negeri dan penegakan hukum – meningkatkan perlindungan baik di wilayah perbatasan dan dalam negeri – harus disesuaikan dengan kenaikan tingkat ancaman.
Retail terrorists – individu atau kelompok melakukan serangan bersenjata ke kelompok rentan di masyarakat – sangat sulit diatasi.
Ancaman dan terjadinya beberapa serangan mengharuskan ketahanan sosial yang lebih kuat dan sedapat mungkin menyeimbangkan ulang antara privasi individu dan keamanan kolektif.
Kita juga perlu menerima fakta dan kenyataan yang ada.
Perjuangan melawan Negara Islam bukanlah perang konvensional.
Kita tidak bisa memberantas atau menghancurkannya dalam waktu dekat, sebab ISIS merupakan jaringan dan gagasan, serta suatu organisasi dan negara de facto yang mengendalikan wilayah dan sumber daya.
Memang, terorisme masih dan terus menjadi salah satu momok di era ini.
Meskipun demikian, kabar baiknya adalah ancaman yang diakibatkan ISIS ke Timur Tengah dan seluruh dunia dapat dikurangi secara signifikan melalui tindakan terpadu dan berkelanjutan.
Pembelajaran utama dari serangan di Paris adalah kita harus siap bertindak kapan pun dan dimanapun.
After Pax Americana?
NEW YORK – It has become popular to suggest that when the dust settles from the global financial crisis, it may become clear that the United States-led post-war world has come to an end.
If so, the global system that has secured peace, security, openness, and economic growth over the past six decades could be in grave danger.
Inspired by American leadership since World War II’s end, Europe, then Japan, then much of Asia and the world rose to new levels of prosperity; the world economy globalized upon the foundation of international institutions, norms, and standards; and foreign students educated in American universities returned home with new ideas about free markets, entrepreneurship, and democracy.
The US military’s protective umbrella gave large swaths of the world a vacation from war, making it easier for them to focus on economic growth and regional integration.
America not only took the lead role in building the institutions of a globalizing world – the United Nations, World Bank, IMF, NATO– it also became the model that many other countries looked to for inspiration.
After eight years of compromised American leadership, a botched war of choice in Iraq, failure to take the lead in global efforts to address climate change, Abu Ghraib, Guantánamo Bay, running up a $10 trillion debt, and igniting a global financial crisis – America’s once-glittering model has lost a good deal of its luster and America’s leadership has been questioned by many.
The point was driven home at the 7th Asia-Europe Meeting (ASEM) in Beijing this autumn, where European and Asian leaders began exploring ideas for a new global financial structure.
For much of the past 60 years, it would have been impossible to hold such a fundamental dialogue without US participation.
Today, it is almost becoming a new global norm that neither the international committee nor the US is prepared for.
Despite talk about American decline, the world is not prepared for a post-American era.
As irksome as some of America’s actions have been, particularly over the past eight years, America remains the world’s most critical champion of the progressive values that have lifted hundreds of millions of people out of abject poverty and political repression.
If the US were to play a relatively smaller role in world affairs, and no other system was created to pick up the slack, these values could be at risk.
Although many states now hide behind an alleged universal principle of inviolable state sovereignty, for example, would the international community really want to go back to the old model where states did whatever they wanted to their citizens within the confines of their own borders?
Do countries around the world believe that they will be better off if the global trade system breaks down or international shipping lanes become less secure?
Are countries like China willing to step up and pay their fair share of dues to keep the UN running (China currently pays 2.1% of UN dues, compared to more than 25% for the US), or to capitalize revised international financial institutions or the Global Fund to Fight AIDS, Tuberculosis, and Malaria in a meaningful way?
Unless other countries become more willing to step forward for the common good, a post-American world could quickly become a far more frightening environment than what it would replace.
To make its case for a continued global leadership role, America must, however, step up to the plate.  While the go-it-alone impulse of the Bush administration has been discredited by its consequences, the inverse lessons regarding how important collaborative action is in today’s interconnected world are still being learned.

Even at the apex of American power, America’s greatness was always based on inspiring others, and the opportunities for building market share in that particular category remain unlimited.
It is impossible to overestimate how significant a step Barack Obama’s election is in this direction, but America’s actions over the coming years will be the ultimate determinant of whether the power of America’s model can be restored.
America can and should, for example, become the global leader combating climate change through major investments in alternative energy, conservation, and energy efficiency, and by taking strong actions at home to reduce America’s greenhouse gas emissions.  It should transform its immigration policy to recruit the best and brightest people from around the world to move to the US and become citizens, and remain the world’s leading champion of open markets, especially during the current financial crisis.
Closing the prison at Guantánamo and reaffirming America’s commitment to international law and human rights will also be an important step in this direction.
The world wants to believe in an America that lives up to its own best values.
The prospect of a truly global community of nations working together to achieve the greater good for all is indeed exciting.
But, although America has been far from perfect over the last six decades, the end of the
If the world is going to shift in the direction of a new and more globally democratic system, other nations will need to meaningfully step forward to assume new responsibilities.
It is in America’s and the world’s interest that they do so.  The evidence of this will be seen not only in global institutions but also in places like Darfur, Zimbabwe, and Burma.
Until this happens, let us all hope that America can get back on track as the global champion of collaborative action to address the world’s greatest challenges and work with as many other countries  as possible to move collectively in the right direction.
After the Dear Leader
SEOUL – Korea is a unique country.
The Cold War ended when the Soviet Union collapsed in 1991, and is now remembered only as history to most people around the world.
The Korean Peninsula, however, remains divided along ideological lines, and the two Koreas co-exist as living remnants of the Cold War.
A total of almost 1.5 million young soldiers from both North and South Korea face off against each across the heavily armed Demilitarized Zone.
Events and structural forces, however, have affected and changed the nature of the North Korean system since 1991.
The sudden discontinuation of the supply of petroleum and natural resources from Russia in the early 1990’s, the failure of the centrally-planned economy, and the subsequent massive famine in the mid-1990’s left North Korea’s leaders no alternative but to tolerate informal market activities.
Nowadays, every North Korean seems to like money and know its value.
The engagement policy pursued by the South Korean government in recent years also contributed to changes in North Koreans’ perceptions of the outside world and of their own abject economic situation.
In these desperate circumstances, North Korea’s leaders clung to their strategy of developing nuclear weapons as a last resort to defend the security of their regime.
But, regardless of whether the nuclear issue is resolved, the spread of market forces in North Korea will continue to change every aspect of life there in the coming years.
Nobody yet knows what the political consequences of these economic changes will be.
So, even before the recent reports of North Korean leader Kim Jong-il’s health problems, North Korea was already a country marked by growing uncertainty.
Nevertheless, the reports about Kim’s ill health hit like a rude wake-up call about the precarious nature of conditions in North Korea.
The “Dear Leader’s” declining health will add even more uncertainty and may cause international instability across East Asia.
For example, Kim Il Sung groomed his son Kim Jong-il as his successor for about two decades before the younger Kim took power in 1994.
Kim Jong-il, however, has not yet even chosen his successor.
Indeed, it is widely expected that a new system of collective leadership will emerge if Kim Jong-il is incapacitated.
The problem is that North Korea has no experience with collective leadership.
In the last six decades, all power has been concentrated in the hands of one person.
One-man rule has been so completely embedded in North Korea’s political culture and system that it is difficult to expect collective leadership to succeed.
Thus, some type of power struggle, rather like what occurred in the Soviet Union following Stalin’s death, may invariably follow a brief period of collective rule.
The dilemma for the world is that major decisions will have to be made during this period of instability.
Will the North’s would-be leaders be able to manage the country’s stock of nuclear weapons responsibly and safely without transferring a few of them abroad, much less respond to international pressure to dismantle them in a reasonable and flexible manner?
Over the past decade, Kim Jong-il has emphasized the importance of his military-first policy.
As a result, the army will be dominant in decision making on important issues like nuclear negotiations.
But the problem with military rule is a tendency toward incomplete understanding of the implications of political decisions – a problem that will be aggravated further by collective leadership.
Under a North Korean collective leadership dominated by the military, the power of the country’s economic bureaucrats will be marginal, at best.
This may lead to suppression of pressure from below for economic reform and opening.
As a result, the economy may worsen and political instability increase.
In such a difficult domestic situation, North Korea’s leaders may adopt more hostile policies to obtain economic aid from both South Korea and the United States.
Recently, South Korea’s government made it clear that it would continue to engage North Korea, but in a more principled way than had previous administrations.
This reflects a hardening of South Korean public opinion, following North Korea’s nuclear test in 2006.
As a result of this policy shift – and the recent killing of a South Korean tourist in North Korea’s Kumgang Mountains – official dialogue between North and South has stopped for the past several months.
International relations in northeast Asia will become more subtle and unstable as the Kim Jong-il era ends.
After six turbulent but somewhat predictable decades, we may be entering into a new era of greater turbulence but less predictability on the Korean Peninsula.
After the Death of Utopia
A decade ago, people spoke of the end of history, meaning the ultimate triumph of a liberal capitalist political order.
Nowadays, many scoff at that notion as too simplistic.
Nonetheless, we are at both the end and beginning of something remarkable.
In the wake of the death of the utopian - and often bloody - certainties of the 19th and 20th centuries (Communism's collapse was but the latest spectacular example), and with fading belief in the liberal welfare state, traditional views about work, retirement, education, the Church, solidarity, and other social institutions are changing rapidly.
The central driver of all this is today's enormous acceleration in the underlying pace of technological and economic change.
Call it fast-forward modernization .
Of course, the worldwide crash of high-tech stocks in 2000 chilled the hype about a "new economy" that seemed to be emerging at the "end of history."
But falling share prices should not blind us to the fact that on top of the ongoing information revolution, three fresh waves of revolutionary technology are poised to hit: bio-technology (including new medical technologies and genetic engineering, such as the creation of human embryos through cloning), nanotechnology, and robotics.
Each is its own industrial revolution, and will profoundly alter our lives and ways of thinking.
Indeed, the revolution is already upon us.
For the first time in history, a global techno-market order is transforming the world of finance, business, politics and, indeed, physiology, beyond recognition.
This new techno-market system is shaped and characterized by a belief in the increasing importance of knowledge, new ideas, innovations and new technologies, and a higher pace of what the economist Joseph Schumpeter famously called "creative destruction."
As a result, corporate capitalism is rapidly becoming obsolete, replaced by a creative capitalism in which entrepreneurship, combined with a greater willingness to adopt innovations, transforms the business landscape.
Innovative start-up firms become huge companies faster than ever before.
But these infant giants are quickly threatened with eclipse by even newer enterprises.
Take the example of computers.
It took 15 years for other countries to compete successfully with America's Silicon Valley in semi-conductors, but less than five years in Internet technology.
This system provides unprecedented financial incentives to scientists and entrepreneurs to aggressively develop new technologies and thus become rich.
But the revolution is not only for the elite; it also offers a realistic (non-utopian) promise of dramatically improved lives for many people around the entire globe - not in 100 years, but in the foreseeable future.
We are not
These changes are also triggering changes in human consciousness.
The result is a litany of "post-utopian" values that include a stronger emphasis on individual freedom and personal responsibility.
In this world without utopia, individual freedom is the supreme value.
But, as with any change of such magnitude, there are holdouts.
Indeed, politics everywhere now seems dominated by the "war of lifestyles" that has emerged from today's emphasis on individual autonomy.
Not so long ago, issues such as the environment, the balance of work versus leisure in daily life, and the role of marriage, abortion, and other family concerns were secondary political disputes, as politicians fought over who would receive what share of a nation's wealth.
Now these issues define domestic political agendas.
Much of the new battle over lifestyles is undoubtedly misunderstood, perhaps because debates about them are conducted in a simplistic way: anti-global movements versus multi-nationals, environmentalists versus corporate polluters, small farmers versus agro-business, and so on. But, beyond slogans, there is an underlying fault line between those who have the cultural capacity to embrace change and those who resist it by adhering to traditional ideas about how one's life and, by extension, society, should be organized.
This conflict exists globally.
In societies that have been preparing themselves by opening their markets and embracing universal education, the disruptions of this revolution can probably be absorbed and handled.
Conflict is most acute in closed societies characterized by a politically repressive climate and culturally induced obstacles to growth.
Such obstacles include the absence of an informed and capable workforce, instinctive mistrust and rejection of new ideas and technologies just because they come from the West, lack of respect for those who acquire new knowledge, and endemic discrimination against women.
The new battle of lifestyles has given rise to new enemies of open societies, such as the Taliban and Al Qaeda.
It is no coincidence that terrorism thrives in societies that are intrinsically hostile to today's modernizing values and belief in individual autonomy.
So long as these ideas clash, violence will lurk.
To defend post-utopian values in the longer-term, politicians (and generals and spymasters) cannot seek security by drastically curtailing fundamental freedoms, because to do so risks forfeiting public support and a weakening of the pillars of the post-utopian market order.
In challenging groups like Al Qaeda, they must understand that they are engaged in a war of ideas; winning the hearts - and the lifestyles - of societies is the only way to win that battle.
After the God that Failed
Discussing morality and politics, it is said, is like discussing vegetarianism with cannibals.
Much of the public, it seems, thinks this way--for good reason.
Moral principles and moral obligations in today's political/economical realm have undoubtedly gone astray, unashamedly displaced by the interests of profit and power.
The implications are clear.
In today's world, "order" reflects a balance of interests that is maintained mainly by force--be it military or financial.
But something other than this "order of the barracks" is possible.
Think of it as the order of the church choir, where individual members cooperate on the basis of a shared culture and values.
One reason for the complete absence of values in "power politics" nowadays is that leaders get away with speaking words whose true meaning they leave out of their policies.
Moreover, secular spiritual leaders like Gandhi, Schweitzer, and King have vanished, annihilated it seems by our new fetishes--success, expediency, gain, and special interests.
Of course, the so-called civilized world never managed to create a living utopia.
Efforts to do so usually ended disastrously.
As someone born in the old Soviet Union, I know firsthand the despair and brutality of such attempts.
This does not mean that all efforts to build a more moral world are doomed.
Having survived an age of extreme ideologies, such as capitalism, communism, and, recently, market fundamentalism, most people no longer seek answers in ideological clichés and the driving, purifying force of political certainty.
Forced harmonization is dead, which opens the door to making new voluntary forms of social harmony possible.
Let me suggest five steps necessary to begin to bring this about.
The first calls for the world to deem as utterly unacceptable state violence that seeks to impose conformity and discipline.
Such violence is unjust on its face because it suppresses and intimidates both body and spirit.
To achieve this end, however, the state must be stripped of its ability to impose divisive dogmas, traditions, and stereotypes.
That can happen only if a carefully crafted system of "checks and balances" is established, in which powerful organized interests--states, above all--are restrained in their efforts at dominance.
Not only political forces need to be checked.
In the rush to concentrate wealth, for example, majority owners of corporations must not be allowed to harm the interests of minority shareholders, who as a rule are honest common citizens.
This leads me to my second reform priority: a systemic separation of state power and capital.
Even in long-established democracies, candidates for elective office are not evaluated according to their wisdom and leadership, but according to the size of their campaign war chests.
Acquiring government power through possession of capital--and converting it into unjust economic rents--must be curtailed.
Can such a boundary between power and capital be set?
Yes.
I say this as someone who created significant capital and, having done so, changed her occupation in order to cultivate morality in politics.
Laws can make the separation of capital and political power transparent for all to see.
But the will to achieve this division is, sadly, absent among most people in power.
Related to this, it is necessary to separate mass media from both power and the interests of capital.
In Europe, America, and Japan, media ownership is increasingly concentrated, which rightly worries citizens in these countries, particularly when media owners move from forming opinion to forming governments.
How much more worrying, then, is such concentrated ownership in new democracies and developing countries, where the check of civil society is mostly absent?
All these reforms presuppose an independent judiciary.
The autonomy of judges must be assured, which requires that their character is such that people trust their decisions.
In states where justice is bought by the rich and powerful, this may impossible, but individual judges can and do stand up to power.
All citizens should support such judicial stalwarts, for their example can refashion an entire justice system.
In essence, creating a more moral political order requires the removal of money as
Of course my proposed "transformations" may sound too good to be possible.
In defense I wish to cite the great Harvard philosopher John Rawls, who died recently.
Theory, no matter how elegant, Rawls argued, should be dismissed or revised when proven untrue.
The corollary to this is that institutions and even world orders--no matter how efficient and successful--must be reformed if they are unjust.
Perhaps only someone who comes from one of the post-Soviet states can conceive of politics in terms of original principles in the manner of Rawls.
Our newborn states were given the opportunity to create moral political systems on the ruins of a "god that failed."
The struggle to do so has been mighty, but despite our travails--and our poverty--I believe that my country has the strength to contribute creatively to this endeavour.
After the Golden Age of Finance
LONDON – Even after the passage of new financial regulations in the United States, the Dodd-Frank Act, and the publication of the Basel Committee’s new capital requirements, the financial sector’s prospects over the next few years remain highly uncertain.
There has been some recovery in prices for bank shares from the lows of 2008, of course, but that rally faltered recently.
Quite apart from their concerns about the robustness of the rebound in the economy, investors are uncertain about many financial firms’ business models, and about the future size, shape, and profitability of the financial sector in general.
After all, banks remain deeply unpopular in all developed countries.
Bankers are still social pariahs, as lowly valued by the public as drug dealers or journalists.
They are reviled if they lose money, and assailed if they make it.
For banks and their shareholders, it looks a case of heads they win, tails we lose. Thus, as banks return to profitability, politicians in North America and Europe have begun to talk again about new taxes that would skim those profits off to the benefit of taxpayers, whose support kept banks in business at the height of the crisis.
This is a huge contrast with the financial sector’s position in the previous three decades.
From the late 1970’s until 2007, the financial sector grew far more rapidly than the real economy.
In 1980, financial assets – stocks, bonds, and bank deposits – totaled around 100% of GDP in the advanced economies.
By 2007, the figure was over 400% in the US, the United Kingdom, and Japan.
During this period, credit expanded rapidly as a share of GDP, reaching more than 300% at the peak.
In the UK, the profits of financial intermediaries, which had averaged around 1.5% of the whole economy’s profits in the 1970’s, reached 15% in 2008.
In the US, bank profits were an even larger share of the total.
This was the golden age of finance.
Bankers’ pay soared alongside profits – indeed, it grew even faster.
To paraphrase William Wordsworth, bliss was it in that dawn to be alive, and to be a derivatives trader was very heaven.
But the expansion came to a shuddering halt in 2008, the first year in decades in which aggregate financial assets fell, and there is still little sign of a sustained recovery.
Is this a short-term phenomenon?
Will the financial sector return to pre-crisis growth rates when the economic situation has been fully stabilized?
Will financial “deepening” continue?
Will bank stocks once again outperform the market?
A recent study by Andy Haldane and others at the Bank of England casts doubt on the prospect of a return to the status quo ante.
They note that the Golden Age was in fact an unusual period, if you look at the last two centuries of economic history.
Haldane bases his analysis on the trend in the Gross Value Added (GVA) of the financial sector.
Over the last 160 years, the GVA of finance has grown by two percentage points a year faster than that of the economy as a whole.
But this excess growth has not been evenly spread.
During the two decades leading up to World War I, the financial sector grew almost four times faster than the economy, in the first wave of financial deepening and globalization, but from 1918 until the 1970’s, finance expanded less rapidly than average economic growth.
Only when markets were deregulated and liberalized from the early 1970’s onwards did finance once again leap ahead.
In the US, financial sector GVA was only 2% of the total in the 1950s, but stands at 8% today.
Haldane believes that this growth spurt is well and truly over.
He argues that much of the apparent growth in value added has in fact been illusory, based on increased leverage, excess trading, and banks writing deep out-of-the-money options – for example, credit-default swaps (a $60 trillion market in 2007). “What all these strategies had in common,” writes Haldane, “was that they involved banks assuming risk in the hunt for yield – risk that was often disguised because it was parked in the tail of the return distribution.”
From a regulator’s perspective, this is a powerful argument for requiring higher capital to constrain the risk that banks can take on.
Indeed, the Basel Committee plans to require more capital in the future, though the new requirements will be delayed, owing to concerns about the cost and availability of credit to sustain the recovery.
Against that background, it is hard to believe that we will quickly return to the heady growth in financial assets, credit, and risk we saw from the 1970’s to 2007.
Financial-sector returns are likely to be lower.
Returns of 20% on equity targets are a thing of the past.
And lower profitability will reduce pay more effectively than any direct regulatory controls.
For most of us, unless we remain seriously overweight in financial stocks, this may not be a bad prospect.
We do not want to inflate another asset-price bubble on the scale of the one that burst in 2007-2008.
But there is a risk for regulators and central banks.
If they over-constrain the financial sector, risk may migrate outside the regulatory frontier, where it will be harder to measure and monitor.
That is why it is important to maintain some flexibility, to allow currently unregulated institutions like hedge funds and private-equity funds to be swept into the regulatory net if they become large and systemically important.
The tighter their controls on risk in banks, the more frontier police the regulators will need.
After the Greek Default
CAMBRIDGE – The Greek government, the European Commission, and the International Monetary Fund are all denying what markets perceive clearly: Greece will eventually default on its debts to its private and public creditors.
The politicians prefer to postpone the inevitable by putting public money where private money will no longer go, because doing so allows creditors to maintain the fiction that the accounting value of the Greek bonds that they hold need not be reduced.
That, in turn, avoids triggering requirements of more bank capital.
But, even though the additional loans that Greece will soon receive from the European Union and the IMF carry low interest rates, the level of Greek debt will rise rapidly to unsustainable levels.
That’s why market interest rates on privately held Greek bonds and prices for credit-default swaps indicate that a massive default is coming.
And a massive default, together with a very large sustained cut in the annual budget deficit, is, in fact, needed to restore Greek fiscal sustainability.
More specifically, even if a default brings the country’s debt down to 60% of GDP, Greece would still have to reduce its annual budget deficit from the current 10% of GDP to about 3% if it is to prevent the debt ratio from rising again.
In that case, Greece should be able to finance its future annual government deficits from domestic sources alone.
But fiscal sustainability is no cure for Greece’s chronically large trade deficit.
Greece’s imports now exceed its exports by more than 4% of its GDP, the largest trade deficit among eurozone member countries.
If that trade gap persists, Greece will have to borrow the full amount from foreign lenders every year in the future, even if the post-default budget deficits could be financed by borrowing at home.
Eliminating or reducing this trade gap without depressing economic activity and employment in Greece requires that the country export more and import less.
That, in turn, requires making Greek goods and services more competitive relative to those of the country’s trading partners.
A country with a flexible currency can achieve that by allowing the exchange rate to depreciate.
But Greece’s membership in the eurozone makes that impossible.
So Greece faces the difficult task of lowering the prices of its goods and services relative to those in other countries by other means, namely a large cut in the wages and salaries of Greek private-sector employees.
But, even if that could be achieved, it would close the trade gap only for as long as Greek prices remained competitive.
To maintain price competitiveness, the gap between Greek wage growth and the rise in Greek productivity – i.e., output per employee hour – must not be greater than the gap in other eurozone countries.
That will not be easy.
Greece’s trade deficit developed over the past decade because Greek prices have been rising faster than those of its trading partners.
And that has happened precisely because wages have been rising faster in Greece, relative to productivity growth, than in other eurozone countries.
To see why it will be difficult for Greece to remain competitive, assume that the rest of the eurozone experiences annual productivity gains of 2%, while monetary policy limits annual price inflation to 2%.
In that case, wages in the rest of the eurozone can rise by 4% a year.
But if productivity in Greece rises at just 1%, Greek wages can increase at only 3%.
Any higher rate would cause Greek prices to rise more rapidly than those of its eurozone trading partners.
So Greece faces a triple challenge: the fiscal challenge of cutting its government debt and future deficits; the price-level challenge of reducing its prices enough to wipe out the current trade gap; and the wage-productivity challenge of keeping future wage growth below the eurozone average or raising its productivity growth rate.
Ever since the Greek crisis began, the country has shown that it cannot solve its problems as the IMF and the European Commission had hoped.
The countries that faced similar problems in other parts of the world always combined fiscal contractions with currency devaluations, which membership in a monetary union rules out.
A temporary leave of absence from the eurozone would allow Greece to achieve a price-level decline relative to other eurozone countries, and would make it easier to adjust the relative price level if Greek wages cannot be limited.
The Maastricht treaty explicitly prohibits a eurozone country from leaving the euro, but says nothing about a temporary leave of absence (and therefore doesn’t prohibit one).
It is time for Greece, other eurozone members, and the European Commission to start thinking seriously about that option.
After the Guns of August
The Middle East is a place where the dust hardly ever settles.
When it occasionally does, even for a short interval – as UN Resolution 1701 for cessation of hostilities in Lebanon seems to be holding – it is time to take stock of events in the hopes that a responsible debate may influence those in power.
Let’s start with the United States.
President George W. Bush has been short on neither initiatives nor catchy slogans and acronyms.
Recent years are littered with them: “Global War on Terror” (GWOT), “Road Map,” “Middle East Partnership Initiative “ (MEPI), “Broader Middle East and North Africa” (BMENA) – originally “Greater Middle East Initiative (GMEI) – Democracy Assisted Dialogue (DAD), and so on.
His latest reverie, envisioned in the thick of the recent fighting between Israel and Hezbollah, was the New Middle East (NME), with US clients Israel, Egypt, Jordan, and Saudi Arabia serving as the pillars of regional order.
But like all his previous initiatives since the terrorist attacks on New York and Washington almost five years ago now, the NME ran into trouble from the outset.
Secretary of State Condoleezza Rice announced its birth while rejecting an immediate ceasefire in Lebanon.
Her poor timing made the initiative appear heartless, as thousands of civilians were being uprooted, killed, or maimed by Israel’s efficient but ruthless artillery and air force.
This so embarrassed the three Arab NME partners that each raced to distance itself from the US-sponsored initiative.
Saudi Arabia, which had remained silent for nearly two weeks, did so with a $500 million contribution to rebuilding devastated areas of Lebanon and another billion to support Lebanon’s threatened currency.
Egypt’s heir apparent Gamal Mubarak followed suit in the fourth week of the fighting by heading a 70-member delegation on a solidarity visit to Beirut.
But, rather than earning him the respect of an outraged Egyptian public, revelations in the opposition press that his plane had to obtain a safe passage and authority to land from the Israelis garnered only howls of derision.
As for America, anything it touches in the Middle East has become radioactive, even for longstanding clients and friends.
In the course of maneuvering to delay the UN ceasefire, Bush and Rice continually reiterated the need for a Security Council resolution that deals forcefully with “the roots of the problem.”
Of course, for them and for Israel, this was Hezbollah and the need to eradicate or at a minimum disarm it and force its fighters to a safe distance from settlements and towns in northern Israel.
While this is a reasonable demand, the rest of the Middle East – and, indeed, much of the world, including Europe – regard the root cause of the conflict as Israeli intransigence and arrogance, together with America’s blind support for it.
Both America and Israel have cited foot-dragging in implementing UN Resolution 1559, which calls for disarming all non-state actors in Lebanon and the deployment of government forces all the way to the southern border.
But for years the US and Israel have not uttered a word about the dozens of UN resolutions, going back as far as Resolution 49 on partition in 1947, which called for the establishment of distinct Arab and Jewish states on roughly half of Mandated Palestine.
This and numerous other resolutions seeking redress for injustices toward Palestinians have been ignored by the US.
Thus, for 300 million Arabs and more than one billion Muslims the “root cause” of the Middle East conflict is not Hezbollah.
As its leader, Hassan Nasrallah aptly put it, “We are just a reaction to chronic injustice.”
It may well be that there is more than one root cause – every party to the conflict has a favorite one.
There is no point in belaboring whose pain is greater or whose root cause is deeper.
In fact, arguing over grievances merely drives the sides further apart.
The long overdue UN Resolution 1701 may, with its adoption, indicate that all parties are fatigued, or could no longer withstand international pressure.
This is good news for all concerned and provides an opportunity to tackle each party’s “root cause.”
Seizing the opportunity requires that humility rather than moral supremacy prevails.
Empathy, not ethnocentrism, should be the order of the day now that the guns are falling silent and we have rediscovered the limits of military force.
But if we have learned anything at all from the tragic assassinations of the region’s greatest peacemakers, Anwar Sadat and Yitzhak Rabin, it is that the guns do not remain silent for long.
During any lull, a fanatic from either side could jump to center stage and, through an act of utter madness, kick up the settling dust and dash the hopes of the many on both sides who still long for a lasting peace.
After the Millennium Development Goals
CAMBRIDGE – In 2000, 189 countries collectively adopted the United Nations Millennium Declaration, which evolved into a set of concrete targets called the Millennium Development Goals (MDGs).
These ambitious targets – ranging from halving extreme poverty and reducing maternal mortality by three-quarters to achieving universal primary schooling and halting (and beginning to reverse) the spread of HIV/AIDS – are supposed to be met by the end of 2015.
As the deadline approaches, development experts are debating a new question: What comes next?
It is virtually certain that many of the MDGs will not have been met by the end of 2015, but there have been striking successes in some areas.
For example, the goal of halving extreme poverty (measured by the number of people living on less than $1.25 a day) will likely be achieved ahead of time, largely thanks to China’s phenomenal growth.
At the same time, there is little evidence to suggest that those successes were the result of the MDGs themselves.
China implemented the policies that engineered history’s greatest poverty eradication program prior to, and independently from, the Millennium Declaration and the MDGs.
Clearly, however, the MDGs were a public-relations triumph, which is not to belittle their contribution.
Like all worthwhile PR efforts, the MDGs served to raise awareness, galvanize attention, and mobilize action – all for a good cause.
They amplified the global conversation about development and defined its terms.
And there is evidence that they got advanced countries to pay more attention to poor nations.
Indeed, the MDGs possibly had their clearest impact on aid flows from rich to poor countries.
A study by Charles Kenny and Andy Sumner for the Center for Global Development in Washington, DC, suggests that the MDGs not only boosted aid flows, but also redirected them toward smaller, poorer countries, and toward targeted areas like education and public health.
However, aid was not directly linked to performance and results, and it is much more difficult to know whether it had the desired impact overall.
The MDGs encompass eight goals, 21 targets, and 60 indicators.
Much criticism has focused on the use of these numerical targets and indicators, which, skeptics argue, are misspecified, mismeasured, and divert attention from equally important areas.
But these complaints miss the point.
Any effort that is concrete and implementable needs to monitor the results, and setting clear numerical targets is the best way to do so.
Still, a central paradox plagues the MDGs.
The Millennium Declaration was meant to be a compact between the world’s rich and poor countries.
Poor countries promised to refocus their development efforts while rich countries pledged to support them with finance, technology, and access to their markets.
But, oddly, of the eight goals, only the last one deals with “global partnership,” or what rich countries can and should do.
Even here, the MDGs contain no numerical target for financial aid or any other aspect of rich countries’ assistance, in contrast to the highly specific poverty-related targets set for developing countries.
It is perhaps telling that the “progress charts” prepared by the United Nations Development Program, the agency charged with reporting on progress toward achieving the MDGs, track only Internet usage under that goal.
Why we need a global effort to convince developing countries to do what is good for them is not clear.
Poverty reduction and human development should be the first order of business for governments in these countries, with or without the MDGs.
It is true, of course, that these governments often pursue different goals, for political, military, and other reasons.
But it is wishful thinking to believe that they can be persuaded to act otherwise by international declarations that lack enforcement mechanisms.
If we have learned one thing in the development business, it is that real reform cannot be bought with donors’ money, let alone with vague promises of money.
Equally problematic, the MDGs implicitly assume that we know how to achieve development targets, and that only resources and political will are missing.
But it is doubtful that even well-intentioned policymakers have a good handle on, say, how to raise secondary-school completion rates sustainably or reduce maternal mortality.
Many development economists would argue that significant improvements in governance and political institutions are required before such goals can be achieved.
The most that rich countries can do is to provide an enabling environment for the benefit of developing countries that are willing and able to take advantage of it.
These considerations suggest an obvious direction for the next iteration of the MDGs.
First, a new global compact should focus more directly on rich countries’ responsibilities.
Second, it should emphasize policies beyond aid and trade that have an equal, if not greater, impact on poor countries’ development prospects.
A short list of such policies would include: carbon taxes and other measures to ameliorate climate change; more work visas to allow larger temporary migration flows from poor countries; strict controls on arms sales to developing nations; reduced support for repressive regimes; and improved sharing of financial information to reduce money laundering and tax avoidance.
Notice that most of these measures are actually aimed at reducing damage – for example, climate change, military conflict, and financial crime – that otherwise results from rich countries’ conduct.
“Do no harm” is as good a principle here as it is in medicine.
This kind of reorientation will not be easy.
Advanced countries are certain to resist any new commitments.
But most of these measures do not cost money, and, as the MDGs have shown, setting targets can be used to mobilize action from rich-country governments.
If the international community is going to invest in a bold new public-relations initiative, it might as well focus on areas where the potential payoffs are the greatest.
After the Promised Land
LONDON – At the height of the Arab uprisings last spring, many Europeans were gripped by nightmare visions of a tsunami of migrants crashing against the continent’s shores.
The wave never hit, but its specter fed a tenacious anti-immigrant populism that has concealed an important new trend: migration to Europe – and to the United States – has largely stalled.
In many countries, more immigrants are leaving than are arriving, owing mainly to the economic crisis that has drained jobs in the West.
That reversal is one of the great under-reported stories of 2011 (and of the preceding two years), and the numbers are startling. Consider Spain, which is on track to lose more than a half-million residents by 2020.
By contrast, between 2002 and 2008, Spain’s population grew by 700,000 a year, driven largely by immigration.
The trends are similar elsewhere in Europe.
While this fact alone will not quiet opponents of immigration, it does give countries more breathing room to repair and strengthen badly broken systems for receiving and integrating newcomers.
Although rapidly aging Western countries are unable to attract the immigrants they need, they allow millions who are already there to suffer discrimination and abuse.
Detentions and deportations take place under sometimes terrible conditions.
Meanwhile, the international community collectively fails to protect vast populations of vulnerable migrants, such as the millions stranded by the recent conflicts in North Africa.
Undoubtedly, rising anti-immigrant populism must be confronted.
While polling suggests that attitudes are influenced more by ethnicity than religion, both help to define identities and mindsets.
Political parties in France, Switzerland, and the Netherlands (to name a few) have run successful campaigns that scapegoat immigrants.
Moreover, governments from Alabama to Hungary are passing laws that undermine what should be migrants’ rights.
Italy recently adopted harsh “emergency” decrees that target migrants by making undocumented entry and residence a criminal offense.
Anti-immigrant rhetoric from the political extremes has fed into mainstream political discourse.
European leaders trip over themselves to declare, one more forcefully than the next, that multiculturalism is dead.
Dutch politician Geert Wilders, whose Freedom Party is informally part of the governing coalition, did them one better by being charged with incitement to anti-Muslim hatred.
In the US, alligator-filled moats and electrified border fences have featured in the current presidential campaign.
Such attacks on immigration might offer some instant political gratification, but their net result is to cleave societies whose cohesion is already seriously challenged by the economic crisis.
Growing discrimination in employment, housing, and education affects not just immigrants and their children; it harms our societies as a whole.
With the lull in net immigration, we now have a window of opportunity to address these shortcomings.
Debunking the myths about migration – that most immigrants enter unlawfully, for example, or that immigration displaces existing workers – would be a good place to start.
It would also be useful to explain that immigration is necessary for prosperity and growth in almost all OECD countries.
If aging societies in the West and elsewhere (like Japan) fail to get immigration right, they will be woefully unprepared when they confront the real tidal wave: the retirement of baby boomers in the coming two decades.
The gaps in these countries’ labor markets – from software specialists to physicians to home health aides – will be immense.
The European Union’s labor force will decline by almost 70 million workers in the next 40 years; in the absence of significant net immigration (combined with a much higher retirement age), European economies and social safety nets will shrivel.
The priorities are clear.
We need to understand better how our economies will evolve in the coming decades, and to redesign our educational systems to produce workers with usable skills.
And, where it is clear that immigrants will be needed, we must be able to identify, welcome, integrate, and protect them.
Meanwhile, our most fundamental institutions – schools, police, and the courts – must be re-engineered to reflect and respond to the diversity of our communities, which is now a fact of life.
Countries must learn to work together to achieve these goals, few of which can be reached by going it alone.
If our toolbox were empty, our inaction might be understandable.
But examples of smart migration practices abound.
Canada and the Philippines, for instance, have a well-functioning accord that protects the rights of temporary workers.
Sweden has developed legislation that minimizes bureaucracy for companies that need foreign workers.
And important advances have been made in ensuring that immigrant children receive the education that they need to become full members of society.
Progress is being made on the global level as well, despite the economic crisis and populist headwinds.
In June, the International Labor Organization’s member states overwhelmingly approved the Domestic Workers Convention, which will significantly increase protections for a vulnerable group of workers –&nbsp;the majority of whom are migrants.
Meanwhile, the Global Forum on Migration and Development, established in 2007, has quickly become an important means of fostering knowledge and partnerships.
The reason for growing international cooperation is simple: countries everywhere are affected by migration, and, increasingly, they are experiencing immigration and emigration simultaneously.
Indeed, roughly one-third of migrants nowadays move between developed countries; one-third move between developing countries; and only one-third move from the developing to the developed world.
Highly skilled workers, such as bankers and engineers, are flocking to China.
Mexico, known primarily as a country of emigration, is home to millions of migrants from Central America.
Millions of people in Southeast Asia venture to the Middle East to work, but millions more cross borders within the region.
The list goes on.
When it comes to migration, we are all in the same boat – and that boat is leaking.
Starting in 2012, countries should redouble their efforts to fix it.
After the Taliban
``Failed state'' is a term applied frequently to Afghanistan and is often deemed the cause for why terrorists gained such influence there.
But a country does not fail of its own volition, nor is it weakened by unknown causes.
A country fails, when it fails, for definite, identifiable reasons.
These must be addressed if Afghanistan is to be revived.
Twenty years of invasion, civil war, and drought have left Afghanistan's institutions in ruin.
Millions of Afghans huddle in refugee camps or are displaced from their homes.
Land-mines defile the countryside.
Millions are sick and poor; many live at starvation levels.
For these and many other reasons, rebuilding Afghanistan's economy will require not only economic reconstruction but an effort to reinvent the country's political and cultural institutions.
Such a massive effort will be doomed to failure, however, if Afghanistan's neighbors intervene in ways that promote economic upheaval all over again.
Afghanistan is no place for quick fixes.
Rebuilding the country cannot be done cheaply.
Any thought that the anti-terror coalition will be able to bail out fast (as the West did when it abandoned Afghanistan to its fate after the Soviet withdrawal ten years ago) should be forgotten.
The West must stick with Afghanistan until its reconstruction is established.
Otherwise, it runs the risk of renewed chaos and violence, but this time in a more destabilized region, as Pakistan's current problems demonstrate.
Three problems are of immediate concern, the most important being feeding the Afghan people - both within the country and in refugee camps outside Afghanistan.
Humanitarian aid is being delivered, but a distribution system safe from the predations of Afghanistan's warlords needs to be built.
Indeed, the warlords have been given too big a say in distributing aid already, and it may be hard to strip them of this power.
But stripped they must be.
The second problem involves relocating Afghan refugees now living in Pakistan and Iran, as well as those displaced within Afghanistan.
To achieve this goal, the agricultural economy must be revived in order to revive this industry, providing jobs and food for people.
A massive impediment here is the millions of mines left over from the Soviet invasion that must be removed.
The West has a big incentive to be generous to Afghanistan's rural poor.
Starving farmers, if unassisted, may return to cultivating a very reliable cash crop: the opium poppy, long a staple of the warlord economy.
Eliminating it will not only help farmers and the West as it tries to curtail heroin use, but also Afghanistan's infant government as it struggles to assert its national authority against the warlords.
A bankrupt warlord, after all, cannot buy weapons or bribe people to maintain their loyalty.
Major infrastructure investments will also be needed if the economy is to be revived.
Housing, particularly for returning refugees, will need to be constructed fast.
Cities such as Kabul, Mazar, Herat and others will need to be rebuilt as centers of economic and cultural life.
Village housing must be provided at a massive level.
Roads, airports and communications systems must also be revitalized if trade is to be restored.
The educational system needs to be rebuilt almost from scratch, and with so many women anxious to return to teaching, a revived educational system will also help Afghanistan's democratic politicians gain a powerful lobby of workers.
Particular attention should be given to elementary schools and libraries outside of cities.
Afghanistan poses particular difficulties in reconstruction, as it is not a society with a strong political center.
Planners should take advantage of the country's decentralized nature and emphasize private sector participation in reconstruction.
A decentralized system will respond better to local needs and avoid an over-bureaucratic public sector.
But autonomous economic regions should be avoided as a threat to Afghan national unity because they would play into the hands of the warlords.
Moreover, poorer regions would do badly in such a system.
In the long run, Afghanistan has resources that can be exploited.
There is the potential for oil and gas exploration, and of mining iron ore and precious metals.
These activities should be explored in a framework of economic development across Central Asia.
Afghanistan, indeed, must be integrated into the regional pipeline and other development schemes.
Afghans can contribute in a tangible way here by reopening the North-South route connecting the resource rich economies of Central Asia to densely populated India and Pakistan.
None of this will be possible unless Afghanistan's young males are disarmed and given productive work.
Essential here, is to attract expatriate Afghans with skills and professional achievements to help in rebuilding the country by establishing small firms that will suck up the unemployed.
Expatriate involvement will also likely support the rights of women to participate fully and legally in economic and political life, as was the case before 1978.
Finally, donor countries must apply the lessons learned in restoring the war-ravaged states of the former Yugoslavia.
Grants and planning must be coordinated, and the consent of Afghanistan's neighboring countries assured.
If the latter are ignored, regional interests can incite chaos once again.
An international conference on Afghanistan should be called by the US and held under UN auspices.
It must affirm not only Afghanistan's territorial integrity, but insure that donor commitments meet the scale of the job and that these promises are kept.
A decade ago, the West turned its back on Afghanistan and chaos ensued.
To abandon the country again would be criminal folly.
After the War on Terror
NEW YORK – The basic outlines of Barack Obama’s approach to foreign policy became clear in 2009.
His administration believes that the United States should talk with other governments even if it disagrees profoundly with their character.
He prefers acting with other countries to going it alone.
And he has shifted the focus of US foreign policy from what countries do within their borders to how they act beyond them.
All of this differentiates Obama from his immediate predecessor, George W. Bush, whose administration branded selected countries as evil and mostly refused to deal with them; often rejected cooperation with other governments, lest the US find itself constrained; and sought to transform other countries, rather than to influence their actions.
Any parallels between Obama’s foreign policy and that of Bush are more with the father, America’s 41st president, George H.W. Bush.
Diplomacy, of course, should not be viewed as a favor or concession that signals “softness.”
Obama rightly recognizes that it is an important tool of foreign policy, to be employed when it promises results that are more favorable than the alternatives.
Obama is also correct that acting in concert with others is almost always desirable.
The challenges that most define this era – nuclear proliferation, terrorism, global climate change, and pandemic disease – can be managed only collectively.
Moreover, the reality is that the US is now too stretched economically and militarily to succeed by relying solely on its own resources.
Finally, Obama is right to focus more on the behavior of countries than on their nature.
It is not just that the assistance of odious governments is sometimes essential; it is also that there is nothing more difficult than remaking the internal workings of other societies.
Yet, even with these adjustments, and despite Obama’s communication skills and his personal popularity (reflected in his Nobel Peace Prize and in declining anti-Americanism around the world), 2009 was a difficult first year for his administration’s foreign policy.
To begin with, a willingness to talk to governments does not always translate into an ability to work with them.
The US has shown new flexibility with Iran and North Korea, but neither has reciprocated so far.
Reasonableness does not always yield results.
Similarly, countries often choose not to cooperate.
China, for example, resists using its influence with North Korea, fearing that instability on the Korean peninsula could lead to large refugee flows into China or a united Korea allied with the US.
China prefers an imperfect status quo to such alternatives.
For its part, Russia appears reluctant to pressure Iran to reining in its nuclear ambitions.
Obama has worked hard to improve US-Russian ties: bilateral arms control is again a priority, and the president agreed to alter plans for anti-missile deployments in Poland and the Czech Republic.
But Russian leaders resist tough sanctions on Iran, lest they jeopardize financial dealings there and lead to increased Iranian support for Muslim minorities inside Russia.
As a result, it will be difficult in 2010 and beyond to develop an effective package of sanctions and incentives that enjoys broad international backing.
Similarly, when it came to negotiating a new pact to mitigate global climate change, diplomatic efforts had to contend with international discord in 2009, forcing leaders to postpone their efforts yet again.
With developing countries fearing the impact of binding limits on carbon emissions on their economic growth, and with insufficient support in the US Congress, a global treaty appears unlikely in 2010 as well.
In the Middle East, despite Obama’s vow to re-start the Israeli-Palestinian peace process, progress remained stymied by differences over the scope and content of an agreement and how to bring it about.
The lack of forward movement stems not just from disagreement over goals, but also from the inability of a divided Palestinian leadership to compromise.
Weak protagonists make poor participants in peace negotiations.
Moreover, other countries’ domestic politics do matter for what Obama can accomplish.
Afghanistan’s often corrupt and incompetent government undermines counter-insurgency efforts there.
Pakistan’s government does not share the same threat assessments or priorities as its American benefactor.
No amount of increased American military effort is likely to offset the resulting lack of progress in weakening extremists in both countries.
Domestic politics in the US is also getting in the way of Obama’s foreign-policy goals.
The absence of consensus on climate change, or resistance in Congress to new trade agreements, weakens the president’s hand.
And America’s unwillingness to take meaningful steps to reduce its ballooning fiscal deficit further undermines the effectiveness of its foreign policy.
Finally, some of what the Obama administration chose to do (singling out Israeli settlements, for example) and how it did it (Guantánamo comes to mind) made tough situations tougher.
But the larger truth is that the world has become a more difficult place to manage, much less to lead.
The post-Cold War era of US unipolarity has ended, owing to America’s economic mismanagement, the war in Iraq, the continuing rise of other countries, and globalization.
There are bright spots, of course.
America’s relations with other major powers – China, India, Japan, Russia, Europe, and Brazil – are at least as cooperative as they are competitive; conflict between such major powers, the dominant characteristic of the international order in the twentieth century, is unlikely.
It is also possible that some of today’s most closed countries, including Iran, North Korea, Burma, Cuba, and Venezuela, will become more open and less threatening with time.
On balance, though, early evidence suggests that this will not be an era of smooth multilateralism or American leadership.
Something more complicated, and unwieldy, appears increasingly likely.
Barack Obama’s next three years in office (or seven, if he is re-elected in 2012) will likely be characterized as much by frustration as by accomplishment.
Aftershocks from Japan
NEW HAVEN – The devastation – both human and physical – from the earthquake and tsunami in Japan is unfathomable.
It is impossible at this point to gauge the full extent of the damage with any degree of precision.
But we can nonetheless begin to assess its potential spillover effects on the rest of Asia and other major economies around the world.
The narrow view of the catastrophe’s economic impact is that Japan doesn’t really matter anymore.
After all, more than 20 years of unusually sluggish trend growth in Japanese output has sharply reduced its incremental impact on the broader global economy.
The disaster may produce some disproportionate supply-chain effects in autos and information-technology product lines such as flash drives, but any such disruptions would tend to be transitory.
On the surface, the world’s two largest economies have little to fear.
Japan accounts for only 5% of America’s exports and 8% of China’s.
Under the worst-case outcome of a complete disruption to the Japanese economy, the direct repercussions on the United States and Chinese economies would be small – shaving no more than a few tenths of a percentage point off their annual growth rates.
Within the so-called G-10 developed economies, Australia has the largest direct exposure to Japan – the destination of about 19% of its total exports.
The eurozone is at the opposite end of the spectrum, with Japan accounting for less than 2% of its exports.
Among emerging-markets, the Philippines and Indonesia are the most exposed to Japan, which absorbs about 16% of their total exports.
South Korea, the third-largest economy in East Asia, is at the other end of the scale, relying on Japanese demand for only about 6% of its exports.
But the narrow view misses the most critical consideration: this “Japan shock” has not occurred at a time of great economic strength.
That is true not only of Japan itself, where two lost decades have left a once-vigorous economy on a less-than-1% growth trajectory since the early 1990’s.
But it is also true of the broader global economy, which was only just beginning to recover from the worst financial crisis and recession since the 1930’s.
Moreover, the Japan shock is not the only negative factor at work today.
The impacts of sharply rising oil prices and ongoing sovereign debt problems in Europe are also very worrisome.
While each of these shocks may not qualify as the proverbial tipping point, the combination and the context are disconcerting, to say the least.
Context is vital.
Notwithstanding the euphoric resurgence of global equity markets over the past two years, the world economy remains fragile.
What markets seem to have forgotten is that post-bubble, post-financial-crisis recoveries tend to be anemic.
Economies grow at something much closer to their stall speeds, thus lacking the cyclical “escape velocity” required for a self-sustaining recovery.
As a result, post-crisis economies are far more vulnerable to shocks and prone to relapses than might otherwise be the case.
Alas, there is an added complication that makes today’s shocks all the more vexing: governments and central banks have exhausted the traditional ammunition upon which they have long relied during times of economic duress.
That is true of both monetary and fiscal policy – the two mainstays of modern countercyclical stabilization.
Policy interest rates are close to zero in the major economies in the developed world, and outsize budget deficits are the norm.
As a result, unconventional – and untested – policies, such as so-called “quantitative easing,” have become the rage among central bankers.
All along, such unconventional policies were viewed as a temporary fix.
The hope was that policy settings soon would return to pre-crisis norms.
But, with one shock following another, the “exit strategy” keeps being deferred.
Just as it is next to impossible to take a critically ill patient off life-support treatment, it is equally difficult to wean post-bubble economies from their now steady dose of liquidity injections and deficit spending.
In an era of extraordinarily high unemployment, political pressures only compound the problem.
This raises perhaps the most troublesome concern of all: with a post-crisis world getting hit by one shock after another, and with central banks having no latitude to cut interest rates, it is not hard to envision a scenario of open-ended monetary expansion that ends in tears.
The dreaded inflationary endgame suddenly looms as a very real possibility.
None of this detracts from the resilience factor.
Yes, Japan will rebuild, which will undoubtedly spur some type of recovery in its disaster-battered economy.
That happened in the aftermath of the Hanshin (Kobe) earthquake in 1995, and it will happen this time as well.
But, just as the post-Kobe rebuilding did little to end the first of Japan’s lost decades, a similar outcome can be expected this time.
The upside of rebuilding – beyond the urgent restoration of normal life for thousands of people – is only a temporary palliative for an impaired economy.
That’s only one of the lessons that Japan offers the rest of us.
The Japanese economy has, in fact, been on the leading edge of many of the more serious problems that have afflicted the global economy in recent years.
From asset bubbles and a dysfunctional financial system to currency suppression and monetary-policy blunders, Japan has been in many respects the laboratory of our future.
Unfortunately, the world has failed to learn the lessons of Japan.
And now it risks missing another important clue.
The significance of the earthquake and tsunami of 2011 is not the relatively low magnitude of Japan’s direct impact on the broader global economy.
The more meaningful message is how these shocks box the rest of us into an even tighter corner.
Against Simplification
NEW YORK – It is said that Americans have a genius for simplification.
Gradually, however, the quest for it has become a global trend, one that continues to conquer new territories, just as blue jeans once did.
The speed of our daily life is visibly increased – and not for the better – by this unstoppable evolution.
The tyranny of pragmatism seems to mark all of the complex dilemmas of our time.
Too many valid choices are ignored or skirted through the routine of short-cuts.
Nowhere is this trend more damaging than in today’s mercantile approach to art.
Even the much-praised notion of competition seems fake and cynically manipulated by the “corporate” mentality that now pervades the world of culture – by the financial pre-selection that determines what publishers, producers, and other impresarios will support.
Just imagine what might have happened with the works of, say, Proust, Kafka, Musil, Faulkner, or Borges had they been subjected to mass-market competition like shoes or cosmetics.
Culture is a necessary pause from the daily rat race, from our chaotic and often vulgar political surroundings, and it is a chance to recover our spiritual energy.
Great books, music, and paintings are not only an extraordinary school of beauty, truth, and good, but also a way of discovering our own beauty, truth, and good – the potential for change, of bettering ourselves and even some of our interlocutors.
If this respite and refuge is gradually narrowed and invaded by the same kind of “products” as those that dominate the mass market, we are condemned to be perpetual captives of the same stunted universe of “practicalities,” the ordinary agglomeration of clichés packaged in advertisements.
I was thinking again about these old and seemingly unsolvable questions during my re-reading of a quite challenging novel by a close friend and a great writer, not very present in the vivid landscape of American letters of today.
The theme, style, and echo of his work says a lot, I think, about our simplified world.
The novel is Blinding, by Claudio Magris.
Hailed in Europe as one of the great novels of the twentieth century, Blinding arrived in America only after a great delay, and never received the attention that it deserved.
Unfortunately, that is no surprise.
The number of literary translations done nowadays in the United States is, according to a United Nations report, equal to that of Greece, a country one-tenth the size.
Imported books are thought to be too “complicated,” which is another way of saying that literature should deal with simple issues in a simple way, obeying the rules of the mass market, with its tricks of packaging, accessibility, advertisement, and comfort.
At the core of Magris’ book is the destiny of a group of Italian communists who travel to Yugoslavia after the Second World War to contribute to the construction of a socialist society, only to be caught in the conflict between Stalin and Tito.
They are imprisoned for their Stalinist allegiance; when they are finally allowed to return to Italy, their old comrades refuse to accept them.
The book’s plot spans two centuries of revolution.
Then, suddenly,
“the party vanished, overnight, as if all of a sudden a giant sponge had drained the entire sea, Adriatic and Austral, leaving litter and clots of mud, and all the boats stranded.
How can you go home again if the sea has been sucked down a vast drain that opened up beneath it, emptying it who knows where, into a void?
The earth is arid and dead, but there won’t be another one, nor another heaven.”
The solitude of the individual facing his faith alone, without collective illusions, and forced to do something with himself in the arid, noisy world tells us something important about the exiled world of modernity and its complex and contradictory problems.
Magris’s novel is not only an important literary achievement; it also has a deep connection to the dangers that we face now, particularly the wave of fanaticism, from Mumbai to Oslo, in the name of a holy war against the “other.”
Are all the extremists searching for a new coherence, for a lost illusion of togetherness and a new hope of resurrection?
Can we ever forget September 11, 2001, the start of a bloody century in which the mystical force of hatred and destruction has recovered its strength?
Are Osama Bin Laden’s minions, the bloody Hamas-Hezbollah battalions, or troubled loners like Timothy McVeigh, Theodore Kaczynski, and now Norway’s Anders Behring Breivik, the “heroes” of our contemporary nightmare?
Is this the “rebel” response to an overly globalized, incoherent, and ultimately disturbing reality?
If so, their barbarism demands scrutiny – in relation to both historical precedent and to our modernity – rather than merely being labeled “monstrous” (though it certainly is that).
The new religious militants, fighting in the name of their particular and peculiar God, seem as fanaticized as the Fascists, Nazis, and Communists of earlier decades.
Magris’s main character is a rebel in more than one embodiment: as Salvatore Cipico, one of the inmates in the communist concentration camp in Yugoslavia; as Jurgen Jurgensen,  ephemeral king of Iceland and a convict forced to build his own jail; and as Jason, the mythic adventurer searching for the volatile truth.
A multilayered and complex chronicle of the devastating tragedies of the twentieth century, Blinding is an insistent, informed, and irreplaceable incursion into the moving landscape of the human soul, its wounds and voids, its vitality and versatility, its deep distortions and its unpredictable dynamics.
It is a fascinating story about the conflict between ideals and reality, or Utopia and humanness; about being faithful to a cause and betraying it; and about sacrifice and solidarity.
It is also a rich and original literary achievement that challenges today’s consumerist ethic.
By renouncing simplicity, it also repudiates today’s prevailing confusion of information with literature, of facts with creativity, and best-selling products with true works of art.
Ethics and Agriculture
MELBOURNE – Should rich countries – or investors based there – be buying agricultural land in developing countries?
That question is raised in Transnational Land Deals for Agriculture in the Global South, a report issued last year by the Land Matrix Partnership, a consortium of European research institutes and nongovernmental organizations.
The report shows that since 2000, investors or state bodies in rich or emerging countries have bought more than 83 million hectares (more than 200 million acres) of agricultural land in poorer developing countries.
This amounts to 1.7% of the world’s agricultural land.
Most of these purchases have been made in Africa, with two-thirds taking place in countries where hunger is widespread and institutions for establishing formal land ownership are often weak.
The purchases in Africa alone amount to an area of agricultural land the size of Kenya.
It has been claimed that foreign investors are purchasing land that has been left idle; thus, by bringing it into production, the purchases are increasing the availability of food overall.
But the Land Matrix Partnership report found that this is not the case: roughly 45% of the purchases involved existing croplands, and almost a third of the purchased land was forested, indicating that its development may pose risks for biodiversity.
The investments are both private and public (for example, by state-owned entities) and come from three different groups of countries: emerging economies like China, India, Brazil, South Africa, Malaysia, and South Korea; oil-rich Gulf states; and wealthy developed economies like the United States and several European countries.
On average, per capita income in the countries that are the source of these investments is four times higher than in the target countries.
Most of the investments are aimed at producing food or other crops for export from the countries in which the land is acquired, for the obvious reason that richer countries can pay more for the output.
More than 40% of such projects aim to export food to the source country – suggesting that food security is a major reason for buying the land.
Oxfam International calls some of these deals “land grabs.”
Its own report, Our Land, Our Lives,indicates that, since 2008, communities affected by World Bank projects have brought 21 formal complaints alleging violations of their land rights.
Oxfam, drawing attention to large-scale land acquisitions that have entailed direct rights violations, has called on the Bank to freeze investments in land purchases until it can set standards ensuring that local communities are informed of them in advance, with the option of refusing them.
Oxfam also wants the Bank to ensure that these land deals do not undermine either local or national food security.
In response, the World Bank agreed that there are instances of abuse in land acquisition, particularly in developing countries in which governance is weak, and said that it supported more transparent and inclusive participation.
At the same time, it pointed to the need to increase food production to feed the extra two billion people expected to be alive in 2050, and suggested that more investment in agriculture in developing countries is required to improve productivity.
The Bank rejected the idea of a moratorium on its own work with investors in agriculture, arguing that this would target precisely those who are most likely to do the right thing.
One may ask whether transparency and the requirement that local landholders consent to a sale is enough to protect people living in poverty.
Supporters of free markets will argue that if local landowners wish to sell their land, that is their choice to make.&#160;
But, given the pressures of poverty and the lure of cash, what does it take for people to be able to make a genuinely free and informed choice about selling something as significant as a right to land?
After all, we do not allow poor people to sell their kidneys to the highest bidder.
Of course, hardline supporters of free markets will say that we should.
But, at the very least, it needs to be explained why people should be prohibited from selling kidneys, but not from selling the land that grows their food.
Most people can live without one kidney.
No one can live without food.
Why does the purchase of body parts give rise to international condemnation, while the purchase of agricultural land does not – even when it involves evicting local landholders and producing food for export to rich countries instead of for local consumption?
The World Bank may indeed be more concerned about local landholders’ rights than other foreign investors are.
If so, the 21 complaints made against Bank projects are most likely the visible tip of a vast iceberg of violations of land rights by foreign investors in agricultural projects in developing countries – with the others remaining invisible because victims have no access to any complaint procedure.
One such case belatedly came to the attention of the United Nations Human Rights Committee.
In November, the Committee concluded that Germany had failed to police the Neumann Kaffee Gruppe regarding its complicity in the forced eviction of several villages in Uganda to make way for a large coffee plantation.
But the evictions took place in 2001, and the villagers are still living in extreme poverty.
They found no remedy, in either Uganda or Germany, for the violation of rights that, according to the Committee, they possess under the International Covenant on Civil and Political Rights, to which Germany is a signatory.
Are we to believe that landholders fare better with Chinese or Saudi investors?
Aid Works
NEW YORK – The critics of foreign aid are wrong.
A growing flood of data shows that death rates in many poor countries are falling sharply, and that aid-supported programs for health-care delivery have played a key role.
Aid works; it saves lives.
One of the newest studies, by Gabriel Demombynes and Sofia Trommlerova, shows that Kenya’s infant mortality (deaths under the age of one year) has plummeted in recent years, and attributes a significant part of the gain to the massive uptake of anti-malaria bed nets.
These findings are consistent with an important study of malaria death rates by Chris Murray and others, which similarly found a significant and rapid decline in malaria-caused deaths after 2004 in sub-Saharan Africa resulting from aid-supported malaria-control measures.
Let’s turn back the clock a dozen years.
In 2000, Africa was struggling with three major epidemics.
AIDS was killing more than two million people each year, and spreading rapidly.
Malaria was surging, owing to the parasite’s growing resistance to the standard medicine at the time.
Tuberculosis was also soaring, partly as a result of the AIDS epidemic and partly because of the emergence of drug-resistant TB.
In addition, hundreds of thousands of women were dying in childbirth each year, because they had no access to safe deliveries in a clinic or hospital, or to emergency help when needed.
These interconnected crises prompted action.
The United Nations’ member states adopted the Millennium Development Goals in September 2000.
Three of the eight MDGs – reductions in children’s deaths, maternal deaths, and epidemic diseases – focus directly on health.
Likewise, the World Health Organization issued a major call to scale up development assistance for health.
And African leaders, led by Nigeria’s president at the time, Olusegun Obasanjo, took on the challenge of battling the continent’s epidemics.
Nigeria hosted two landmark summits, on malaria in 2000 and on AIDS in 2001, which were a crucial spur to action.
At the second of these summits, then-UN Secretary-General Kofi Annan called for the creation of the Global Fund to Fight AIDS, TB, and Malaria.
The Global Fund began operations in 2002, financing prevention, treatment, and care programs for the three diseases.
High-income countries also finally agreed to reduce the debt owed by heavily indebted poor countries, allowing them to spend more on health care and less on crippling payments to creditors.
The United States also took action, adopting two major programs, one to fight AIDS and the other to fight malaria.
In 2005, the UN Millennium Project recommended specific ways to scale up primary health care in the poorest countries, with the high-income countries helping to cover the costs that the poorest could not pay by themselves.
The UN General Assembly backed many of the project’s recommendations, which were then implemented in numerous low-income countries.
Donor aid did start to rise sharply as a result of all of these efforts.
In 1995, total aid for health care was around $7.9 billion.
This inadequate level then crept up slowly, to $10.5 billion by 2000.
By 2005, however, annual aid for health had jumped another $5.9 billion, and by 2010, the total had grown by another $10.5 billion, to reach $26.9 billion for the year.
The expanded funding allowed major campaigns against AIDS, TB, and malaria; a major scaling up of safe childbirth; and increased vaccine coverage, including the near-eradication of polio.
Many innovative public-health techniques were developed and adopted.
With one billion people living in high-income countries, total aid in 2010 amounted to around $27 per person in the donor countries – a modest sum for them, but a life-saving one for the world’s poorest people.
The public-health successes can now be seen on many fronts.
Around 12 million children under five years old died in 1990.
By 2010, this number had declined to around 7.6 million – still far too high, but definitely an historic improvement.
Malaria deaths in children in Africa were cut from a peak of around one million in 2004 to around 700,000 by 2010, and, worldwide, deaths of pregnant women declined by almost half between 1990 and 2010, from an estimated 543,000 to 287,000.
Another $10-15 billion in annual aid (that is, roughly $10-15 more per person in the high-income world), bringing total aid to around $40 billion per year, would enable still greater progress to be made in the coming years.
The MDGs for health could be achieved even in many of the world’s poorest countries.
Unfortunately, at every step during the past decade – and still today – a chorus of aid skeptics has argued against the needed help.
They have repeatedly claimed that aid does not work; that the funds will simply be wasted; that anti-malaria bed nets cannot be given to the poor, since the poor won’t use them; that the poor will not take anti-AIDS medicines properly; and so on and so forth.
Their attacks have been relentless (I’ve faced my share).
The opponents of aid are not merely wrong.
Their vocal antagonism still threatens the funding that is needed to get the job done, to cut child and maternal deaths by enough to meet the MDGs by 2015 in the poorest countries, and to continue after that to ensure that all people everywhere finally have access to basic health services.
A decade of significant progress in health outcomes has proved the skeptics wrong.
Aid for health care works – and works magnificently – to save and improve lives.
Let us continue to support these life-saving programs, which uphold the dignity and well-being of all people on the planet.
Airpocalypse di Eropa
SINGAPURA – Para pembuat kebijakan di Eropa memang senang menceramahi negara lain tentang polusi udara.
Negara-negara di Asia, khususnya Tiongkok, menjadi target favorit penerima kritik mereka.
Bahkan rasa-rasanya konferensi berskala besar apapun yang membahas tentang lingkungan hidup belum lengkap jika tidak disertai presentasi oleh pembuat kebijakan dari Eropa tentang “praktik-praktik terbaik” di Eropa yang harus ditiru negara-negara lain.
Akan tetapi, bicara mengenai polusi udara, mungkin ada baiknya negara-negara Eropa lebih sedikit bicara dan lebih banyak mendengar.
Polusi udara menjadi semakin parah di kawasan Eropa.
Organisasi Kesehatan Dunia (WHO) menyebutnya “satu-satunya risiko kesehatan lingkungan hidup yang terbesar” dan memperkirakan 90% penduduk Eropa terpapar dengan polusi outdoor yang melebihi batas kualitas utama berdasarkan pedoman WHO.
Pada tahun 2010, sekitar 600.000 penduduk Eropa meninggal di usia muda karena polusi dalam ruangan (indoor) dan di luar ruangan (outdoor) dan kerugian ekonomi ditaksir mencapai $1.6 triliun, yakni sekitar 9% PDB Uni Eropa.
London dan Paris mengalami masalah kualitas udara buruk.
Tingkat nitrogen dioksida di sejumlah wilayah di London seringkali mencapai 2-3 kali di atas batas wajar yang disarankan.
Di UK saja, polusi udara membunuh sekitar 29,000 orang dalam setahun dan menempati peringkat kedua penyebab kematian di usia muda.
Paris bahkan mungkin lebih menderita; pada bulan Maret, setelah tingkat polusi udaranya melebihi Shanghai, pemerintah setempat lalu menetapkan larangan mengemudi secara parsial dan menggratiskan layanan transportasi publik.
Sayangnya, pembuat kebijakan di Eropa tidak terdorong untuk melakukan gerakan yang sama.
George Osborne, selaku Chancellor of the Exchequer pemerintah UK, menolak kepemimpinan Inggris dalam perjuangan melawan perubahan iklim. “Kita tidak akan menyelamatkan bumi dengan menutup pabrik baja, fasilitas smelter aluminium, atau pabrik kertas,” jelasnya dalam pernyataan yang dibuat di tahun 2011.
George Osborne tidak sendirian.
Di saat para politisi Eropa berpendapat perlindungan lingkungan hidup akan melemahkan perekonomian Eropa yang sudah memburuk, tentu tidak mengejutkan bila upaya mengurangi polusi udara tidak berhasil.
Standar-standar yang mengatur emisi beracun dari pabrik batubara yang diperkenalkan Uni Eropa bahkan kalah ketat bila dibandingkan Tiongkok, menurut laporan Greenpeace.
Namun sejumlah politisi Eropa menyerukan kebijakan yang lebih longgar, sementara Hongaria menyarankan agar standar tersebut dihapuskan seluruhnya.
Memang benar bahwa tingkat polusi udara di Asia sangat mengkhawatirkan.
Di Benua Asia terdapat sembilan dari sepuluh negara dengan polusi terburuk, berdasarkan Peringkat Kualitas Udara yang diterbitkan Yale University pada tahun 2014 lalu.
New Delhi menempati posisi pertama kota dengan tingkat polusi tertinggi di dunia, dimana polusi udara mencapai 60 kali di atas batas aman.
Berkat udara yang tidak sehat di Beijing, perusahaan-perusahaan asing membayarkan “hardship bonus” kepada 30% karyawan yang bekerja di kota tersebut.
Namun setidaknya pembuat kebijakan di Asia sudah mengakuinya sebagai masalah dan mengambil tindakan untuk mengatasinya.
Tiongkok, misalnya menyerukan “perang melawan polusi.”
Hingga tahun  2017, Beijing – pernah disebut “Greyjing” oleh media internasional – akan mengeluarkan sekitar CN¥760 miliar ($121 miliar) guna melawan polusi udara.
Inti dari upaya Tiongkok adalah meningkatnya penggunaan transportasi publik, perdagangan hijau (green trade), dan penyempurnaan campuran energi (energy mix).
Pemerintah sudah menetapkan pendirian halte bus setiap 500 meter di pusat-pusat kota, menurunkan tarif hingga 5% atau lebih rendah bagi 54 produk ramah lingkungan, dan menonaktifkan pabrik-pabrik batubara yang sudah tua dan tidak efisien.
Porsi bahan bakar non-fosil dalam pemakaian energi primer diperkirakan bertambah menjadi 20% pada tahun 2030.
Besar kemungkinan semua target ini diterapkan dengan ketat, mengingat kuatnya dukungan politik dari pimpinan tertinggi.
Sementara itu di India, pemerintah negara bagian di Gujarat, Maharashtra, danTamil Nadu mencanangkan peresmian skema cap-and-trade untuk materi partikulat yang pertama di dunia.
Mahkamah Agung India bahkan mengusulkan pembebanan biaya ekstra terhadap kendaraan pribadi bermesin diesel di New Delhi.
Negara-negara lain di kawasan Asia juga mengambil tindakan untuk meningkatkan kualitas udara.
Vietnam berencana membangun delapan jalur kereta api perkotaan dalam beberapa tahun ke depan.
Kota Bangkok, yang sudah memerangi polusi udara sejak 1990an, menanam 400,000 pohon.
Jepang memberikan subsidi bagi mobil hidrogen dan membangun  beberapa kawasan baru yang khusus diperuntukkan bagi pejalan kaki.
Eropa, sebagai salah satu wilayah terkaya di dunia, selayaknya berada di baris terdepan dalam upaya menggalakkan keberlanjutan lingkungan hidup.
Namun begitu persoalannya menyangkut polusi udara, para pembuat kebijakan di Eropa harus berhenti menguliahi negara lain dan fokus menyelesaikan permasalahan mereka sendiri.
Alan Greenspan on Trial
The release of Alan Greenspan’s ghostwritten memoirs The Age of Turbulence has elicited charges that he was not such a great central banker after all.
Stan Collender of National Journal sees the fingerprints of the White House on these attacks: Greenspan is harshly critical of George W. Bush’s administration, after all, and to attack the credibility of Republican ex-policymakers who are critical of Bush is standard counterpunching for it.
But what is one to make of the criticisms of Greenspan’s tenure at the Federal Reserve?
The indictment contains four counts: that Greenspan wrongly cheered the growth of non-standard adjustable-rate mortgages, which fueled the housing bubble; that he wrongly endorsed Bush’s tax cuts; that he should have reined in the stock market bubble of the 1990’s; and that he should have done the same with the real estate bubble of the 2000’s.
To the first two counts, Greenspan now pleads guilty.
He says that he did not understand how the growth of non-standard mortgages had lured borrowers and investors into bearing dangerous risks.
He was, he now says, focusing on how fixed-rate mortgages are relatively bad deals for borrowers in times of low inflation, which was a mistake.
Greenspan also pleads guilty to a mistake in early 2001.
He thought that he was giving balanced testimony to Congress on government budget issues.
He testified that it is important to run surpluses to pay down the debt, but that surpluses must not be so large that the government winds up owning American industry.
He also testified that tax cuts are better than spending increases to keep surpluses from growing too large, but that uncertainty is enormous, so that any tax cuts should be canceled if they threatened to bring us back to an age of deficits.
Robert Rubin and Kent Conrad warned him that the press would not interpret his testimony as being balanced, and that Congress would interpret it as an excuse to abandon fiscal discipline.
They were right.
Greenspan also pleads guilty to misunderstanding the character of the Bush administration.
He thought that his old reality-based friends from the Ford administration were back in power. He thought that he – and Treasury Secretary Paul O’Neill – could win the quiet “inside game” for sensible policy without resorting to an “outside game” that would make his reappointment in 2004 unlikely.
He was wrong.
But how serious are these policy-political crimes to which Greenspan now pleads guilty?
In my view, they are misdemeanors.
Against them you have to set what former Treasury Secretary Larry Summers calls Greenspan’s “golden glove” performance at avoiding and minimizing recessions during his years at the Fed.
The “felonies” of which Greenspan stands accused are the other two charges: that he should have done more to stop the stock market bubble of the late 1990’s, and that he should have done more to stop the housing bubble of the early 2000’s.
Here, Greenspan holds his ground, and pleads not guilty.
The only way, he says, for the Fed to have kept stock prices in reasonable equilibrium ranges in the late 1990’s would have been to raise interest rates so high that they hit the real economy on the head with a brick.
Interest rates high enough to curb stock market speculation would also have curbed construction and other forms of investment, raised unemployment, and sent the economy into recession.
To cause a significant current evil in order to avoid a possible future danger when our knowledge is limited and our judgments uncertain is, Greenspan believes, unwise.
In this, he is following a tradition of caution that extends from Edmund Burke to John Maynard Keynes.
Greenspan mounts a similar defense concerning the housing bubble.
High construction employment has been good for American workers in the past half-decade – a period that has not produced much good for them.
Higher interest rates to reduce the housing boom seem, even in retrospect, ill advised if the cost is mass unemployment.
And Greenspan eschews paternalism: he would not assume the role of a regulator telling people that they cannot buy a house even though a lender is willing to finance it.
But Greenspan would have served the country and the world better if he had been somewhat more paternalist in slowing the growth of non-standard adjustable-rate mortgages.
He would have served the country and the world better had he been less of a loyal Republican working the inside game of trying to convince Bush’s political advisors that good policy was important, and more of a nonpartisan steward of America’s long-term fiscal stability.
Of course, such a Greenspan would never have been re-appointed.
All in all, Greenspan served the United States and the world well through his stewardship of monetary policy, especially by what he did not do: trying to stop stock and housing speculation by halting the economy in its tracks.
Assured Mutual Dependence
LONDON – During the Cold War, the certainty of “mutually assured destruction” steered the nuclear arms race away from catastrophe: a would-be attacker would face immediate retaliation, inevitably ending in both sides’ annihilation.
Today, a very different race is taking place – a race for the earth’s vital resources, and it threatens to undermine stability in key regions of the world.
The growing dependence of countries on one another’s food, water, and energy requires that the global response to sustainability is taken to the highest political level.
Unlike the nuclear arms race of the twentieth century, the resource-security agenda is not linear.
Mutually assured destruction was explicitly acknowledged during the Cold War in statements from both sides.
In the race for resources that defines the twenty-first century, no actor is directly or indirectly threatening other players to curtail food or energy exports, but all bear the systemic risks.
Countries have become unavoidably interdependent, and climate change, water stress, and the loss of ecological resilience all increase the volatility of this mutual dependence.
In a world of limited and scarce resources, countries and companies will be forced to make decisions that affect one another’s security.
In order to navigate this interdependence, the Earth Security Index 2014, produced by the Earth Security Initiative, shows countries’ combined vulnerabilities that might increase the risk exposure of governments and companies, unless more strategic approaches and sustainable investments are put in place.
The ESI identifies four areas of mutual dependence that will likely shape global security in the coming decades:
·         Choke points.
Countries’ growing demand for energy, water, food, and land cannot be satisfied without incurring tradeoffs among limited available resources.
Choke points are reached when the available resources are insufficient to satisfy demand.
In China and India, for example, this means that in certain regions there may not be enough water in the short term to run coal-fired thermal power stations and irrigate large fields to grow crops.
In China, 60% of planned coal-fired power plants will be built in water-stressed regions.
·         Food.
The growing dependence of many countries on food, water, and energy imports creates new opportunities for trade and investment, but it also exposes countries to critical vulnerabilities.
Australia, for example, is a large coal exporter but imports most of its refined fuels and holds just three days of fuel stockpiles. The challenges of mutual dependence are particularly acute with respect to food.
As the ESI shows, some countries – including Egypt, Peru, and the United Arab Emirates – are heavily dependent on cereal imports from a small number of suppliers.
Moreover, grain suppliers’ exposure to extreme weather may compromise their ability to sustain supplies, with knock-on effects for import-dependent countries. In 2010, for example, Russia imposed an export ban on wheat, following a severe drought.
The resulting food-price increases are believed to have played a role in Egypt’s revolution.
·         Teleconnections.
Anticipating systemic ecological risks will be increasingly important for sectors such as reinsurance and infrastructure investments.
“Teleconnections” refer to weather events that are related to one another over large geographic distances.
They are well known to science but not properly discussed by the industries, investors, and governments whose security depends on environmental stability.
For example, tropical rainforests play a crucial function in maintaining stable weather and rainfall, acting as a “pump” that helps moisture travel between different regions.
Deforestation can thus have a destabilizing effect on weather patterns, amplifying the frequency and severity of extreme events such as floods and droughts.
The resulting liabilities to key industries and the financial sector are clear.
In Brazil, for example, deforestation in Amazonia has slowed significantly over the last five years, but Brazil has already lost more than 11 million hectares of rainforest; its exposure to extreme weather has also steadily risen, with floods causing $4.7 billion in losses in 2011 alone.
·         Land productivity bottlenecks: Agriculture systems are reaching resource limits, and persistent governance gaps compromise their ability to ensure food security, dignified livelihoods, and ecological stewardship.
Companies, investors, governments, and communities confront a series of critical barriers to increasing the food availability that the world needs: Local populations’ insecure land ownership; receding water tables, owing to unsustainable extraction rates; inefficient use of pollution-causing inputs like fertilizers and pesticides; the loss of vital ecosystems, affecting the resilience of food production; and certain areas’ inability to cope with extreme weather.
In some regions of India, for example, these issues are playing out in tandem. Insecure land tenure acts as a disincentive for smallholder farmers to commit to productivity-enhancing investments; water extraction rates are depleting aquifers as a result of permissive policies; and food security remains out of reach for millions of people, despite rapid economic growth in urban areas.
Countries and companies will increasingly need to invest in sustainable land in order to hedge their resource risks.
In 2015, global frameworks are due to be agreed to address climate change, coordinate responses to natural disasters, and guide the world’s development agenda.
Some of these multilateral processes – in particular, those seeking an ambitious global climate agreement – appear to be moving in slow motion and against the grain of geopolitical interests.
In the past, the case for high-level nuclear governance was urgent and clear, but required processes for creating a common understanding of risks and opportunities across national borders.
Successful multilateral responses, like the Nuclear Non-Proliferation Treaty, continue to be supported by more flexible global platforms, such as the Nuclear Threat Initiative, based on relationships and trust established outside the box of formal multilateralism.
This year, as world leaders discuss the next generation of sustainability, development, and climate frameworks, they will need to put their security and mutual dependence at the heart of the responses.
Here, too, the world will need to create informal platforms that supplement traditional multilateralism.
In particular, the outdated divisions between rich and poor countries and their responsibilities must be revised.
As new powers like China, Brazil, India, and other G-20 economies bid to reform global governance systems, their vulnerability to resource security must invigorate these processes.
Only then will the world be on track to improve the security of all.
Alexander Hamilton’s Eurozone Tour
PRINCETON – Europe’s debt crisis has piqued Europeans’ interest in American precedents for federal finance.
For many, Alexander Hamilton has become a contemporary hero.
Perhaps one day his face should appear on the €10 banknote.
Specifically, for European states groaning under unbearable debt burdens, Hamilton’s negotiation in 1790 of the new federal government’s assumption of the states’ large debts looks like a tempting model.
Indeed, after Thomas Sargent won the Nobel Prize in Economics last year, he cited it as a precedent in his acceptance speech.
Hamilton argued – against James Madison and Thomas Jefferson – that the debts accumulated by the states during the War of Independence should be assumed by the federation.
There were two sides to his case, one practical, the other philosophical.
Initially, the most appealing argument for his plan was that it would provide greater security to creditors, and thus reduce interest rates, from the 6% at which the states financed their debt to 4%.
Hamilton emphasized the importance of a commitment to sound finance as a prerequisite to public economy. “When the credit of a country is in any degree questionable,” he argued, “it never fails to give an extravagant premium upon all the loans it has occasion to make.”
While that logic certainly appeals to Europeans today, Hamilton insisted on a stronger reason for pursuing sound finance than merely the pursuit of expediency.
There is, he maintained, “an intimate connection between public virtue and public happiness.”
That virtue consisted in honoring commitments, and it would build solidarity in the new political community of the United States.
Indeed, public virtue made federal finance what he called “the powerful cement of our union.”
The condition for success in the American case was that the US raised its own revenue, with federally administered customs houses initially providing the bulk of its receipts.
The logic of a need for specific revenue applies also in modern Europe, where a reformed fiscal system might include common administration of value-added tax (with the additional benefit of eliminating a considerable amount of cross-border fraud).
In the American case, however, unity carried a price: a ceiling was imposed on Virginia’s exposure to the common debt.
Only this inducement to the most powerful state in the union persuaded Madison to drop his opposition to the proposal.
That compromise (which also led to the US capital’s relocation to the District of Columbia, on the border of Virginia and Maryland) may serve as a precedent for limiting Germany’s liabilities if Eurobonds, or some other debt-mutualization scheme, are introduced.
The US experiment in federalized finance was not immediately successful.
Two important components of Hamilton’s financial architecture were not realized, or were realized imperfectly.
He proposed a model of joint-stock banking on a national scale, which ran into immediate opposition (curiously, his proposal was much more influential in Canada).
Second, opponents eventually blocked his proposal for a national central bank.
The charter of the First Bank of the United States was allowed to lapse in 1811; a generation later, in 1836, President Andrew Jackson successfully opposed the charter of the Second Bank of the United States.
Nor did the Hamiltonian scheme of federal finance guarantee a peaceful commonwealth.
In fact, the fiscal union proved to be explosive rather than adhesive.
As international capital markets developed in the early nineteenth century, state governments borrowed on a large scale, quickly turning them from creditors into debtors.
A wave of state defaults followed in the late 1830’s.
A generation later, in the 1860’s, the Civil War between northern and southern states resulted in large part from a dispute about the character of financial burdens –amp at least from the South’s perspective.
Abraham Lincoln’s original proposal to end the immoral practice of slavery by compensating slave owners for manumission was unacceptably expensive, so the Union, according to the slave-holding Confederacy, was determined to expropriate the South.
The federal assumption of states’ debts by itself could not guarantee political order.
The Civil War revealed the centrality of a common foundation of morality to Hamilton’s approach to debt and public finance.
As a result, his approach foundered on the differences between the different states’ conception of morality.
Europeans today have latched onto the practical side of Hamilton’s argument – that is, the idea that debt mutualization might be a means to cheaper credit; but they have worked out neither the political institutions, nor the shared public virtue, that Hamilton deemed crucial.
The extended and politicized debate about debt restructuring has made a Hamiltonian solution more difficult, because the credit of the countries that would be party to it has become questionable.
An obvious starting point for a Hamiltonian Europe would be to set some standard limit for federalized national debt – perhaps the tarnished threshold of 60% of GDP that was mandated (without adequate enforcement) by the Maastricht convergence criteria, or perhaps a lower limit.
Debt exceeding that amount would be left to the responsibility of the member states.
Collective burden-sharing is in the long run the only non-catastrophic way out of Europe’s current crisis, but that requires a substantially greater degree of political accountability and control on a European level.
The lesson to be learned from Hamilton and the US is that the necessary institutions will not function without a greater degree of moral consensus as well.
Alexievich’s Achievement
NEW YORK – It was 1985, and change was in the air in the Soviet Union.
Aging general secretaries were dropping like flies.
Elem Klimov’s cinematic magnum opus “Come and See” depicted World War II without the heroics on which we were reared, highlighting the tremendous human suffering instead.
Klimov’s approach echoed that of Svetlana Alexievich – this year’s Nobel laureate in literature – in her first book, War’s Unwomanly Face, published the year before.
But, whereas many rushed to see Klimov’s film, Alexievich’s book did not seem to excite readers.
The Soviet Union, supposedly progressive, remained rooted in patriarchy.
Women had jobs, but rarely careers.
Women writers wrote exquisite poetry and prose, and they were officially recognized as the equals (well, almost) of their male peers; but they tended to avoid certain topics – and war was a man’s business.
And thus Alexievich begins War’s Unwomanly Face, “There had been more than 3,000 wars in the world, and even more books.
But all we know about war is what men told us.”
And men told us a lot. “We always remembered the war,” Alexievich recalled, “at school, at home, at weddings and christenings, during holidays and funerals.
War and post-war lived in the home of our soul.” Indeed, I had heard so much about the war by the time War’s Unwomanly Face came out, I had little interest in hearing more about it – whether the suffering and sacrifice or the heroism and triumph – from any perspective.
Fast-forward almost a decade.
America was big on gender politics, and, as a graduate student there, I was embarrassed to be behind.
So I finally read War’s Unwomanly Face.
To my surprise, it was not WWII that I learned about; rather, I got my first glimpse into the emotions that my own relatives experienced, as they fought and survived the war.
People like my grandmother had recounted only the oft-repeated male story, completely denying her own experience.
But her experience mattered, and Alexievich recognized that.
I was so inspired by War’s Unwomanly Face that a few years ago I wrote my own book detailing the endurance of women in my family in the war-ravaged Soviet Union.
Other books by Alexievich were similarly inspiring.
Zinky Boys: Soviet Voices from the Afghanistan War (1991) spoke of a distant fight – the nine-year Soviet war in Afghanistan – that eroded Russian culture and humanity, while Voices from Chernobyl: The Oral History of a Nuclear Disaster (1997) meditated on the global significance of the nuclear disaster.
Public reaction to both was mixed.
Neither the state nor the people quite knew how they felt about Afghanistan or Chernobyl – one a lost war, the other an incomprehensible catastrophe.
Alexievich has described herself as “an ear, not a pen.”
She listens and builds a story, before writing it down.
Her talent is to make the private public, to expose the thoughts that people are afraid to think.
Alexievich does not shy away from the horrific aspects of her subject matter, exemplified in a passage from War’s Unwomanly Face: “We didn’t just shoot [prisoners]… we pinned them up, like pigs, with ramrods, cut into pieces.
I went to observe… I waited for that moment when their eyes would start bursting from pain.” While this brutally matter-of-fact tone can make readers uneasy (indeed, it was one reason why I took so long to read the book), we cannot afford to be ignorant of the truth, even – or perhaps especially – if it makes us squirm.
Honest, daring, and sad, Alexievich’s books – containing stories in which life, broken and stolen, is worse than death – show how a woman’s perspective can humanize world problems and make them understandable to all.
In some ways, Alexievich’s literary contribution, which the Nobel committee called “a monument to suffering and courage in our time,” is equal to that of the Austrian novelist and playwright Elfriede Jelinek, whom the committee recognized in 2004 for her work’s feminist critique of Austria’s Nazi past and patriarchal present.
Now, like Jelinek, whose work was largely unknown to non-German readers until she won the Nobel, Alexievich is finally being recognized for her profound impact.
Her award sends a powerful message – not only about her talent, but also about the importance of the female perspective in the public sphere.
To be sure, Alexievich was far from invisible before.
Her books have been translated into 20 languages, with millions in circulation.
And, like many other Nobel laureates, including Jelinek, she has played an active role in civil society, most recently taking a stand against Russia’s annexation of Crimea.
Interestingly, the frequency with which Nobel Prizes have been awarded to women has been increasing.
In 1991, Nadine Gordimer was the first woman in more than a quarter-century to receive the literature prize; now, women receive it every 2-3 years.
Moreover, this summer, the writer and literary critic Sarah Danius became the first woman in 200 years to serve as the permanent secretary of the Swedish Academy, which chooses the Nobel laureate in literature.
But the patriarchal culture from which Alexievich emerged is far from dead.
Recognizing the ways in which she has enriched people’s thinking about difficult – and historically masculine – subjects can only be good, not only for the women she inspires, but also for the men she influences.
I have just finished Alexievich’s latest dreadful masterpiece, Secondhand Time, a brutal account of the chaotic Russian capitalism of the 1990s.
In recent interviews, Alexievich has said she is working on two more books – one about love, the other about aging.
I don’t want to read either of them, but I will.
A Sunni-Shia Bridge Too Far
BAGHDAD – Iraq’s recent parliamentary election, the first since United States troops left the country in 2011, was held amid a rising tide of violence that is fast approaching the levels experienced during the 2005-2007 insurgency.
Can the new government restore order and address the many immense challenges that Iraq faces?
The challenges are indeed daunting.
The authorities must resolve fundamental constitutional questions (such as whether Iraq should be a federal state or a confederation), rebuild civil society, reform state institutions, reconstruct the economy, and end the waste and corruption in the oil sector.
But perhaps the most intractable challenge of all is bridging the sectarian rift between the country’s Shia and Sunni citizens.
These fissures are mirrored in other Arab countries (such as Syria, Lebanon, the Gulf countries, and Yemen) and, increasingly, in the wider Muslim world (including Pakistan, Malaysia, and Indonesia).
Is this a historical aberration, or are Islam’s two largest sects condemned to perpetual mutual hostility?
Certainly, there have been periods when the two communities have coexisted peacefully.
But what matters today is that Shia and Sunni relate to their past differently, and that this historical memory can be distorted – and even invented – to create mistrust and hate.
The overthrow of the first Muslim dynasty, the staunchly anti-Shia Umayyads, in the year 750, by the Abbasids, who traced their lineage to the Prophet Muhammad’s uncle, raised hopes, albeit short-lived, of a Sunni-Shia rapprochement.
The 500 years of Abbasid reign that followed provide many valuable illustrations of how these two communities subsequently related to each other.
In particular, there is much to be learned from the different legacies of the caliph al-Nasir (1180-1225) and the last Abbasid caliph, al-Musta’sim (1242-1258).
The rule of al-Nasir – who viewed the Shia as an intrinsic part of the Islamic community and sought to treat all of his subjects equally – was characterized by a marked decrease in sectarian tensions.
By contrast, Sunni-Shia clashes – including killings, arson, and other violence – were common during al-Musta’sim’s rule.
These examples demonstrate the importance of good leadership when communities that uphold different claims to the truth are subject to the same political authority – especially when these communities seek assurance that their survival is not threatened.
Iraq’s current political leaders need to learn from this past and ensure that none of the country’s communities face marginalization or discrimination – lessons that apply throughout the Muslim world.
In Pakistan, for example, there are sectarian killings almost daily; in Malaysia, the tiny Shia population is viewed as an existential threat; and incendiary language dominates discourse about rival sects in Wahhabi circles in Saudi Arabia and far beyond.
Politics and power struggles explain much of the violence and mistrust.
Fear of Iranian-led hegemony, for example, has focused Gulf leaders’ minds on their Shia population’s loyalty.
Malaysia’s political parties use anti-Shia animus to spread fear, helping to attract votes and consolidate power.
Syria and its regional allies are determined to protect a new regional balance of power that shifted in their favor following the US-led invasion of Iraq.
But political calculation cannot explain everything.
The fall of Saddam Hussein in 2003 provides a good example of how a political event, viewed through a sectarian lens, can be interpreted differently.
The US destruction of the Iraqi state brought about a precarious new order that sought to redress years of Sunni dominance by favoring the Shia.
However, the shock of sudden Sunni disempowerment generated a discourse, widely shared in the Muslim world, in which the Shia are guilty of collusion in the US occupation of the country – a view reinforced by events in Syria.
According to this thesis, the Shia simply reverted to their “historic” role as wreckers and fifth columnists.
Was it not the case, it is claimed, that the Shia also colluded with the Mongols in the fall of Baghdad in 1258, culminating in the death of the last Abbasid caliph and the destruction of the Abbasid Empire, the “universal state” of Muslims?
Several medieval Muslim historians pointed to the role of the Shia vizier Ibn al-‘Alqami, arguing that he plotted with the Mongols to bring down the caliphate.
Once the preserve of a handful of scholars, the Ibn al-‘Alqami story now plays a prominent part in today’s Sunni- Shia disputes.
Indeed, “‘Alaqima,” the plural form of the Arabic name “‘Alqami,” is now applied to the Shia as short-hand for treachery.
Social media forums are replete with polemics about the Shia role in assisting both Mongol and US invaders.
Many even claim that Iraq’s Shia are al-‘Alqami’s descendants, and that Nouri al-Maliki, Iraq’s Prime Minister, is his modern incarnation.
These diatribes reflect Iraqi’s polarized historical memory.
Despite ample historical evidence of peaceful inter-communal relations, many people – whether through simple ignorance of history or the need to assert the supremacy of one version of the truth – prefer to consecrate narratives of treachery and betrayal that perpetuate hatred.
More important, the current situation reflects a lack of wisdom, responsibility, and basic decency on the part of political and religious leaders, who prefer to fuel, rather than dampen, inter-communal strife.
Sadly, intolerance has now become a generalized condition.
There is too little knowledge about other communities’ beliefs and history, and what little exists has been overwhelmed by sectarian anger and its poisonous rhetoric.
As long as Sunnis and Shia refuse to think about their past together, it is difficult to foresee a tranquil future together.
And if political and religious leaders are unable or unwilling to seek accommodation, it will be up to like-minded individuals, groups, and civil-society institutions to rebuild mutual respect and find ways to cooperate.
Doing so will require knowledge, patience, and, above all, open minds and hearts.
All for One Tax and One Tax for All?
CAMBRIDGE – When the next full-scale global financial crisis hits, let it not be said that the International Monetary Fund never took a stab at forestalling it.
Recently, the IMF proposed a new global tax on financial institutions loosely in proportion to their size, as well as a tax on banks’ profits and bonuses.
The Fund’s proposal has been greeted with predictable disdain and derision by the financial industry.
More interesting and significant are the mixed reviews from G-20 presidents and finance ministers.
Governments at the epicenter of the recent financial crisis, especially the United States and the United Kingdom, are downright enthusiastic, particularly about the tax on size.
After all, they want to do that anyway.
Countries that did not experience recent bank meltdowns, such as Canada, Australia, China, Brazil, and India, are unenthusiastic.
Why should they change systems that proved so resilient?
It is all too easy to criticize the specifics of the IMF plan.
But the IMF’s big-picture diagnosis of the problem gets a lot right.
Financial systems are bloated by implicit taxpayer guarantees, which allow banks, particularly large ones, to borrow money at interest rates that do not fully reflect the risks they take in search of outsized profits.
Since that risk is then passed on to taxpayers, imposing taxes on financial firms in proportion to their borrowing is a simple way to ensure fairness.
“What risks?” the financial firms demand to know.
The average cost of the bailouts was “only” a few percentage points of GNP.
And the crisis was a once-in-a-half-century event.
The IMF rightly points out that these claims are nonsense.
During the crisis, taxpayers were on the hook for almost a quarter of national income.
Perhaps the next crisis will not turn out so “well,” and the losses borne by the public will be staggering.
Even with the “success” of the bailouts, countries suffered massive output losses due to recessions and sustained subpar growth.
But, while regulation must address the oversized bank balance sheets that were at the root of the crisis, the IMF is right not to focus excessively on fixing the “too big to fail” problem.
A surprising number of pundits seem to think that if one could only break up the big banks, governments would be far more resilient to bailouts, and the whole “moral hazard” problem would be muted.
That logic is dubious, given how many similar crises have hit widely differing systems over the centuries.
A systemic crisis that simultaneously hits a large number of medium-sized banks would put just as much pressure on governments to bail out the system as would a crisis that hits a couple of large banks.
There are altogether too many complex ideas floating around that look good on paper, but might well prove deeply flawed in a big-time crisis.
Any robust solution must be reasonably simple to understand and implement.
The IMF proposal seems to pass these tests.
By contrast, some finance specialists favor forcing banks to rely much more on “contingent” debt that can be forcibly converted to (possibly worthless) stock in the event of a system-wide meltdown.
But how this form of “pre-packaged bankruptcy” could be implemented in a world of widely different legal, political, and banking systems is unclear.
Financial history is littered with untested safety-net devices that failed in a crisis.
Better to rein in the growth of the system.
The IMF is on much weaker ground, however, in thinking that its one-size-fits-all global tax system will somehow level the playing field internationally.
It won’t.
Countries that now have solid financial regulatory systems in place are already effectively “taxing” their financial firms more than, say, the US and the UK, where financial regulation is more minimal.
The US and the UK don’t want to weaken their competitive advantage by taxing banks while some other countries do not.
But it is their systems that are in the greatest and most urgent need of stronger checks and balances.
Let’s not go too far in defending the “holdout” countries that are resisting the IMF proposal.
These countries need to recognize that if the US and the UK do implement even modest reforms, a lot of capital will flow elsewhere, potentially overwhelming regulatory systems that seemed to work well until now.
And what about the second tax proposed by the IMF, on banks’ profits and bonuses?
Such a tax is politically appealing, but ultimately it makes little sense – except, perhaps, in a crisis year when bank subsidies are glaringly transparent.
It would be better to improve financial-market regulation directly and let national tax systems handle banks’ income like that of any other industry.
The IMF’s first effort at prescribing a cure may be flawed, but its diagnosis of a financial sector bloated by moral hazard is manifestly correct.
Let’s hope that when the G-20 leaders meet later this year, they decide to take the problem seriously instead of tabling discussion for a decade or two until the next crisis is upon us.
All in the Family
MUNICH – Big economic crises often cause iconic companies to falter.
Rupert Murdoch’s media empire is a model of the modern global enterprise.
A particularly dynamic and innovative business model came from outside and took over central aspects of British and then American public life.
That model is now threatened by the fallout from the scandal that started with phone hacking in Murdoch’s British press operations.
The Murdoch experience is a microcosm of how modern globalization works.
Murdoch always looked like a foreign intrusion into British life.
It was not just that he was Australian; he also brought new ideas.
In particular, the application of digital technology, introduced after a ferocious struggle with the powerful print unions, brought substantial cost savings and allowed a new era of journalism.
Even more importantly, Murdoch represented a concept of family business that is common in many parts of the world, but relatively rare in Britain and the United States.
Family capitalism in the continental European model uses relatively little capital to achieve maximum control.
It frequently depends on very complex corporate structures, with multiple layers of holding companies, as well as privileged shares that can guarantee the continuation of control.
This sort of firm is also very common in the most dynamic emerging-market economies in Asia and Latin America.
The Murdoch family holds only 12% of the shares of News Corporation, the top-level holding company, but it wields about two-fifths of the voting rights; other votes are held by a loyal Saudi prince.
For decades, academic analysts have been fighting over whether such large-scale family businesses should be considered beneficial.
Their defenders point out that such companies often have a much longer-term vision than is true of managerial capitalism, which enables them to establish strong and enduring relationships with their customers and suppliers.
At least in the case of the Murdoch empire, it now appears that they pursue long and binding relationships with politicians and the police as well.
Indeed, political entanglements are one of two sources of weakness in European-style family capitalism, as owners seek political advantages and preferred access as much as they strive for technical innovation.
Murdoch’s empire depended on its closeness to politicians.
In retrospect, three successive British prime ministers – Tony Blair, Gordon Brown, and David Cameron – were on overly familiar terms with a manipulative business leader.
Cameron now talks about the need for “a healthier relationship between politicians and media owners.”
And Murdoch apparently is now saying that he wishes that all these prime ministers would “leave me alone.”
The second notorious weakness of family businesses is the problem of succession.
When he appeared before the British parliament in July, Rupert Murdoch looked like an old man, remote and out of control.
In old-style family firms, there is a clear rule of succession that the oldest son takes over.
But that rule is rightly recognized as being potentially dysfunctional.
There is obviously no guarantee that the oldest son is the best businessman, and the result could be bitter and ferocious sibling rivalry.
